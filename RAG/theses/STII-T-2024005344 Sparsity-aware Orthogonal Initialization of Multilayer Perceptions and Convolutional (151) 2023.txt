Sparsity-Aware Orthogonal Initialization of Multilayer Perceptrons and Convolutional Neural Networks

KIARA SANDEL ESGUERRA

UNIVERSITI TEKNOLOGI PETRONAS

MASTER OF SCIENCE IN ELECTRICAL & ELECTRONICS ENGINEERING

DECEMBER 2023

ABSTRACT

Deep neural networks have achieved impressive pattern recognition and generative
abilities on complex tasks by developing larger and deeper models, which are
increasingly costly to train and implement. There is in tandem interest to develop
sparse versions of these powerful models by post-processing with weight pruning or
dynamic sparse training. However, these processes require expensive
train-prune-finetune cycles and compromise the trainability of very deep network
configurations. We introduce sparsity-aware orthogonal initialization (SAO), a method
to initialize sparse but maximally connected neural networks with orthogonal weights.
SAO constructs a sparse network topology leveraging Ramanujan expander graphs to
assure connectivity and assigns orthogonal weights to attain approximate dynamical
isometry. We implemented SAO to multilayer perceptrons and demonstrated that SAO
networks outperform magnitude pruning in very deep and sparse networks up to a
thousand layers with fewer computations and training iterations. Meanwhile, in
convolutional neural networks within the SAO framework, kernels may be removed
prior to model training based on a desired compression factor rather than post-training
based on parameter-dependent heuristics. SAO is parameter-efficient, exhibiting the
potential for applications with tight energy and computation budgets such as edge
computing tasks, because it achieves sparse, trainable neural network models with
fewer learnable parameters without requiring special layers, additional training,
scaling, or regularization. The advantages of SAO networks are attributed to both its sparse but maximally connected topology and orthogonal weight initialization.

CHAPTER 1

INTRODUCTION

This chapter provides an overview of deep neural networks, including their
applications, advancements, challenges, and proposed methodologies for addressing
these challenges. The problem statement, objectives, scope, and the contributions of

the research are also introduced.

1.1 Introduction

Deep neural networks (DNN) have demonstrated state-of-the-art performance in
learning complex patterns in a variety of creative and choice-based tasks. A popularly
accepted maxim is that task complexity can be managed by increasing the number of
perceptrons (in width and depth) and the size of the training dataset. This idea emerged
from the success of the deep CNN AlexNet in the 2012 ImageNet Large Scale Visual
Recognition Challenge (ILSVRC), which achieved significantly lower prediction error
than the winners of the previous years, enabled by advancements in hardware that
allowed efficient training of deep neural networks [1]. Due to this, the development of
DNN design leans towards the construction of larger and deeper networks [2; 3; 4].
However, the continuously increasing size of DNNs to achieve higher accuracy drives

up the computational and energy costs of model training and deployment [5; 6].

Model compression techniques are methods to reduce these costs without degrading
the model performance [7]. For example, pruning removes insignificant weights from
densely-connected neural network based on a significance criterion such as magnitude

[8; 9] and dynamic sparse training seeks to simultaneously learn sparse connections

and weights values together [10]. State-of-the-art model compression techniques can
be computationally expensive, requiring a series of iterated train-prune cycles. Model
compression may result in the loss of global connectivity because some neurons lose
a connective path to other neurons [11]. Taken to the extreme, sparsification methods

induce layer-collapse where entire layers are disconnected [12].

Another risk of removing connections is that it may destroy the dynamical isometry
property of the network [13; 14]. This characteristic is essential to maintain stable
training dynamics for very deep neural networks and confers benefits such as depth-
independent learning rates [15] and successful training of networks with thousands of
layers without specialized architectures like residual connections or techniques such as
batch normalization [16]. Several theoretical studies support the importance of depth
in neural network performance, attributing to it the increase in representation power
[17; 18]. Early studies of deep convolutional neural networks show that deep networks
outperformed shallow networks by a large margin [1] which subsequently spurred the
success of deep convolutional neural networks in the ILSVRC challenge [19; 20; 21;
22; 23].

The performance of neural networks is influenced more by the graph expander-like
properties of the topology rather than on the density of network connections and neurons
[24; 25]. The benefits are attributed to the preservation of both local connectivity and
global connectivity, where each layer is capable of sensing all of its inputs, and yet all
information from the input reaches the output [11]. This is supported by studies which
have incorporated expander properties in the formation of sparse neural networks such

as X-Net [11] and RadiX-Net [26].

1.2 Problem Statement

The development of deep neural network architectures is driven by depth, which
improves the network’s capability to capture more complex or descriptive features,
leading to increased predictive power. However, the increasing depth and complexity

of these networks have led to higher computational and memory requirements,

resulting in elevated training costs, increased carbon emissions, and reduced
accessibility on resource-constrained devices. To counter this, model pruning was
introduced to reduce the size of the model without diminishing its expressivity via

removal of parameters. However, this technique has several drawbacks:

1. Standard pruning methods are typically performed on a pretrained dense model
to be effective. As a consequence, pruning can only save up on inference cost and

not training costs.

2. Standard pruning methods are agnostic to the connectivity of the network,
resulting to loss of local (per layer) and global connectivity. At high levels of
sparsity, the networks can become disconnected making them untrainable as

there are no more paths for information to flow.

3. Pruning leads to higher risk of vanishing and exploding gradients that disrupts

training, posited to be due to the loss of dynamical isometry.

Currently, the established methods to compress neural networks leveraging sparse

structures incurs additional training costs to maintain practical accuracy.

1.3. Research Objectives

The objectives of this study are:

1. To develop a sparse neural network structure that can preserve neural network
connectivity, enabling the explicit construction of deep and parameter-efficient

neural networks while minimizing information loss.

2. To develop a method to obtain the optimal weights for these sparse structures in

a compute-efficient manner that will promote network convergence.

3. To benchmark the performance of the identified sparse architecture with its
corresponding weights to the equivalent dense network baseline and sparse
variants generated with conventional pruning methods as a function of network

density and depth.

1.4 Scope of Research

1. Models: SAO was implemented only on fully-connected and convolutional

neural network architectures for image classification.

2. Datasets: The models were evaluated, in terms of accuracy, only on a selection
of image classification datasets consisted of MNIST, CIFAR-10, CIFAR-100, and
CINIC-10 datasets.

3. Implementation: SAO was implemented using only (c, d)-regular bipartite
Ramanujan graphs to form the sparse structure which was translated only to
weight-level sparsity in MLP and kernel-level sparsity in CNNs. For CNNs,
SAO was implemented through only two different orthogonal convolution
initialization methods: Delta-Orthogonal Initialization (SAO-Delta) and

Explicitly-Constructed Orthogonal Initialization (SAO-ECO).

4. Benchmarks: SAO was compared, in terms of accuracy, only to standard pruning
methods, i.e. local and global magnitude pruning and random pruning, and to
Ramanujan graph constructions without orthogonality, i.e. same structure as SAO
with different weight distributions, with and without prior training of the dense

model at different levels of sparsity.

5. Software: SAO and other sparse networks were only evaluated through the

PyTorch framework which is not readily optimized for sparsity.

6. Hardware: Several GPUs were used for training the models, depending on the
availability, including: NVIDIA Tesla T4, NVIDIA P100, NVIDIA V100, and
NVIDIA A100 available through cloud computing services. These hardware are
not optimized for all types of sparse matrix multiplications and could therefore

not readily exploit the sparsity of SAO and other sparse networks during training.

1.5 Contribution of Thesis

1. A proposed principled approach for increasing sparsity in deep neural networks
while ensuring that no layer or neuron will be entirely disconnected based on the

Ramanujan construction.

2. The introduction of sparsity-aware orthogonal initialization (SAO), an

initialization method which grants the following features:

¢ Sparse yet maximally-connected architecture without reference to a dense

model

¢ Inherent dynamical isometry, eliminating the need for rescaling or

orthogonality regularization to achieve well-conditioned gradients

¢ Trainability in very deep sparse neural networks without the use of
specialized architectural features, regularization, dense model pretraining,

or post-hoc adaptation

1.6 Organization of the Thesis

The rest of the thesis is organized as follows:

Chapter 2 presents the background and a comprehensive review of related
literature. In Chapter 3, we delve into the methodology used. Chapter 4 unveils the
experimental results and accompanying discussions, while Chapter 5 offers a summary
of our research findings and a discussion of their implications for future work in this

field.

CHAPTER 2

REVIEW OF RELATED LITERATURE

This chapter provides a comprehensive review of literature on deep neural networks,
including their types, the motivation behind designing these networks to be deeper, the
commonly-used techniques to assist the training of these networks to achieve
outstanding performance, and the drawbacks of depth in terms of computational costs.
This chapter also discusses the compression method called model pruning, which can
be used to reduce computational costs in such networks, its benefits, and drawbacks.
Finally, this chapter introduces existing methods to mitigate the drawbacks of model
pruning and the limitations of these methods, and the studies upon which this study is

built to provide a solution to these limitations.

2.1 Deep Neural Networks

Deep neural networks (DNNs) is a type of artificial neural network (ANN) known
for their depth, as determined by the number of hidden layers between its input and
output layers. The depth allows the network to learn hierarchical representations which
enables feature extraction. These networks serve as the foundation of deep learning,
and have shown success in various tasks such as computer vision, natural language

processing, speech recognition, and many others [27].

The architectural designs of deep neural networks play a pivotal role in their ability
to model complex data. Multilayer perceptrons (MLPs), are a fundamental architecture
where each neuron in a layer is connected to every neuron in the subsequent layer [28].

While MLPs have been instrumental in the development of deep learning, such

architectures face challenges such as overfitting and computational complexity which
makes them suboptimal, especially with the emergence of more challenging tasks [29].
These challenges have brought advancement in architectural designs, such as the
convolutional neural network (CNN) which have gained significant attention and

success in computer vision tasks [30].

In the following sections, we give a more thorough description of MLPs and CNNs,

which are the two networks used in this study.

2.1.1 Multilayer Perceptrons

Multilayer Perceptron (MLP) is a type of artificial neural network in which all nodes
(or neurons) in one layer are connected to all nodes in the next layer. Figure 2.1 shows
an example of a two-layer MLP with four inputs, five hidden units, and three outputs
[28]. As the network is “fully-connected”, every input affects the outputs of the hidden
layer, which in turn affects the final outputs. The “strength” of the connections between
these nodes is characterized by their weights, which are consolidated in a weight matrix

W ¢€ R™*", where m and n are the number of output and input nodes, respectively.

Output layer

Hidden layer

Input layer

Figure 2.1: Two-layer MLP with four inputs and three outputs.

These networks can be used for various machine learning tasks such as image

classification, natural language processing, recommendation systems, and time series


analysis. As such, these networks have been implemented in various fields. In finance,
DNNs have been employed for stock market prediction [31], credit scoring [32], and
fraud detection [33]. In the healthcare sector, DNNs have demonstrated practical use
for medical diagnosis, disease prognosis [34], and drug discovery [35]. In law
enforcement, DNNs have been utilized for crime forecasting [36], metal object
detection [37], and face recognition [38]. These diverse applications highlight the

versatility and impact of DNNs in addressing challenges in different domains.

While MLPs have demonstrated their effectiveness in various applications, they
exhibit limitations when it comes to handling spatial data [30]. MLPs treat each input
feature independently, i.e. flattening the image and using pixel values as the features,
disregarding the spatial relationships and local patterns that are often crucial in
computer vision tasks. To address these limitations and leverage the spatial nature of
the data, specialized architectures called Convolutional Neural Networks (CNNs) have
emerged as a powerful solution. In the following section, we discuss the architecture,

mechanisms, and applications of CNNs.

2.1.2 Convolutional Neural Networks

Convolutional Neural Network (CNN) is a type of deep neural network typically
used for computer vision tasks such as image classification, segmentation, and object
detection. The concept of CNNs was first introduced by Yann LeCun and colleagues
in the 1980s through LeNet-5 which they used for handwritten digit recognition [30].
This type of network is inspired by the structure of the visual cortex in the human brain
which is composed of many small, interconnected regions called “receptive fields” that
are sensitive to specific patterns or features in visual stimuli [39]. These receptive fields
are arranged in a hierarchical manner, where the higher levels represent more complex
and abstract features. In a CNN, the convolutional layers simulate the receptive fields
of the visual cortex [40]. Each convolutional neuron receives inputs from a small, local
region of the input data, and applies a set of weights to these inputs in order to detect a
specific pattern or feature [41]. The output of each convolutional neuron is then passed

to the next layer of the network, where more complex features are detected. Figure 2.2

shows a convolution operation performed by a weight tensor W € R!*?*2x2 with stride

1 on an input tensor Y € R?*3*3, resulting in an output tensor Y € R!*?*?(28]

Input Kernel Input Kernel Output

Figure 2.2: Convolution operation performed on a two-channel input resulting in a one-
channel output.

CNNs have several advantages over MLPs. First, CNNs are more parameter
efficient than MLPs because they use a technique called parameter sharing, where the
same set of parameters is used for multiple regions of the input [42]. This is
implemented using filters, which are small weight matrices that are convolved with the
input data to produce a feature map. The same set of weights is then used to produce
multiple feature maps, reducing the number of required parameters. Furthermore,
CNNs can automatically learn hierarchical feature representations from raw data. For
example, in image recognition tasks, CNNs are able to learn high-level features such
as edges and shapes from the raw pixel data and use these features to accurately
classify objects in an image. Another advantage of CNNs is that they have shift
invariance and locality, which are not present in MLPs [43]. Shift invariance is a
property of CNNs that allows them to recognize patterns in data regardless of the
position of the pattern within the data [44]. MLPs do not have this property, as the
output of each node is dependent on the specific arrangement of the inputs as an image
is flattened. This means that a shift in the input data will produce a different output. As
a result, multilayer perceptrons are not as well-suited to tasks that require shift
invariance, such as image recognition. Locality, on the other hand, means that only

nearby inputs are relevant to each other. When applying a convolutional filter to an

image, the network considers only a small, local region of the image, rather than
looking at the entire image at once. This allows the network to focus on local features
and patterns in the data, rather than trying to process the entire image at once. Through
the assumption of locality, for a CNN each node in a layer is only connected to a small
local region of the previous layer, rather than all the nodes in the previous layer as in
an MLP, which also improves parameter efficiency. Furthermore, convolutions are not
confined to a fixed input size, only constraints set by the parameters of the convolution,

whereas MLPs have a fixed number of neurons.

CNNs have revolutionized computer vision tasks, demonstrating remarkable
performance in image recognition, object detection, and image segmentation. In
autonomous driving, CNNs have been instrumental in developing vision-based
systems for lane detection, pedestrian detection, and traffic sign recognition [45].
Medical imaging has also benefited greatly from CNNs, with applications in tumor
detection, retinal image analysis, and skin cancer classification [46]. The ability of
CNNs to capture spatial relationships and hierarchical features has positioned them as

a cornerstone in many computer vision tasks.

2.1.3 Mixers

Inspired by Vision Transformers, [47] proposed the MLP-Mixer, which is an
MLP-based architecture that can achieve comparable performance to convolutional
neural networks on image classification tasks. These networks do not use convolutions
nor self-attention, but rely on MLPs which are repeatedly applied across spatial
locations or feature channels. This architecture is simple both in terms of its concept
and implementation, relying only on basic matrix multiplication, data layout changes
through reshaping and transpositions, and the use of scalar nonlinearities. Two types of
MLPs are used in the architecture, namely the channel-mixing MLPs, which allows
communication between different channels, and the token-mixing MLPs, which allows
communication between different spatial locations. Figure 2.3 shows the structure of

the MLP-Mixer acquired from [47].

fae

yyy
(AAAAA!

Figure 2.3: MLP-Mixer architecture.

This model can achieve 87.94% top 1 accuracy on the ImageNet classification
dataset with a throughput of 40 imgs/sec/core, whereas the ViT-H/14 that can achieve
88.55% accuracy with a throughput of 15 imgs/sec/core. Although the vision
transformer achieved higher accuracy, the MLP-Mixer compensates for this through

efficiency.

There are other variations of the MLP-Mixer, such as the ResMLP [48], the gMLP
[49], and the HyperMixer [50]. The ResMLP also takes inspiration from ViT, only that
a linear layer is used in place of the self-attention sublayer. These are MLPs which use
residual connections, such as in ResNets [21]. Although the performance falls short
against transformers, the advantage of this architecture is it does not require training
techniques such as BatchNorm, GroupNorm, or LayerNorm, in contrast to ViTs. The
gMLP, an MLP architecture with gating, was able to achieve comparable performance
to ViT with improved regularization, and achieved 3% higher accuracy than
MLP-Mixer for 66% less parameters on the ImageNet classificatioin dataset. In the
NLP domain, gMLP can minimize perplexity during pretraining as well as
Transformers. The HyperMixer improved upon the MLP-Mixer by allowing variable
length of inputs and position invariance, similar to the transformers, but with linear

instead of quadratic complexity. These features allow this architecture to be used for


Natural Language Process (NLP).

2.2 Importance of Depth in Deep Neural Networks
2.2.1 Historical Overview

The AI Renaissance is said to have been spurred in 2012 [23], when the deep CNN
AlexNet won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC),
where it outperformed competing shallow networks by a large margin [1]. The success
of the deep architecture was enabled by two factors: the availability of a large training
dataset, i.e., ImageNet, and software which allowed graphics processing units (GPUs)
parallelization to accelerate training, which is especially beneficial to AlexNet due to
the additional parameters brought by depth. It was emphasized that depth is
instrumental to the network’s success: the removal of any middle convolutional layer

resulted in 2% Top-1 accuracy loss on ImageNet.

Since then, the ILSVRC winners have all been deep CNNs [19; 20; 21; 22; 23]. In
2014, ILSVRC was won by GoogLeNet, a 22-layer CNN architecture designed
following the Hebbian principle: “neurons that fire together, wire together’. The
network utilized “inception modules” consisted of multiple convolutional layers of
different sizes that are executed in parallel. This design allowed the increase of depth
and width without increasing the computational costs [19]. In 2015, [21] proposed the
ResNet (residual nets), as an answer to the difficulties encountered when training
deeper networks, i.e. saturation with depth. The networks utilised residual connections
which enabled direct flow of information from earlier to deeper layers. The 152-layer
ResNet won the 2015 ILSVRC in the ImageNet classification, detection, and

localization tasks and in the COCO detection and segmentation.

The advantages of deeper networks is supported by theoretical studies. [51]
discussed how deeper networks require less computational elements than shallower
networks which allows for better generalization. [52] discussed the advantages of
deeper architectures such as feature reuse and the potential for higher abstraction. [53]

found that the deeper networks can learn functions of higher complexity compared to


182 layers| |152 layers| [152 layers}

li {19 layers] [22 layers! s|

ee _ E 5.4
shallow at 8 re } | fe z 3.6 3 23 spied
= se

a

Lin etal anchez & Krizhevskyet al Zeiler& =— Sir

Perronnin (AlexNet) Fergus Zisse

n&  Sregedy et al He etal Shao et al Huetal — Russakovsky et al
GG) (GoogleNet) — (ResNet) (SENet)

Figure 2.4: Winners of the ImageNet Large Scale Visual Recognition Challenge
throughout the years.

shallow networks for the same resources. [17] and [18] also presented theoretical

evidence on the benefits of depth in neural network performance.

Some studies have also suggested that increasing neural network depth has larger
payoffs than increasing the width. [3] proved that when approximating a polynomial
function with n variables, a neural network with only a single hidden layer requires a
number of neurons which grow exponentially with n, while the requirement for deep
neural networks grow only linearly with n. An implication of this lower requirement
for achieving practical expressibility is that that the power of neural networks
improves more rapidly with depth than with width. [54] showed that narrow networks
can approximate wide and shallow networks as long as its size exceeds a polynomial
bound, which, along with their other results, implies that depth contributes more than
width to the expressiveness of ReLU networks. [2] showed that, for ReLU feedforward
neural networks, shallow networks require exponentially larger number of neurons

Q(poly(+)) to approximate the same function with error ¢ as deep networks
O(polylog(z)) .

2.2.2 Techniques for Training Deeper Networks

One of the main challenges of training very deep neural networks is the vanishing

gradient problem, which occurs when the gradients of the parameters in the network


become very small and the network is unable to leam effectively. This can happen
when the network has a large number of layers, and the gradients become diluted as
they are propagated through the network, making it difficult for the network to learn
[55]. Furthermore, as the number of layers continues to increase, a saturation point
is reached. At this stage, even though the network may not experience issues such as
overfitting or ill-conditioned gradients, the accuracy of the model begins to decline [21].
In this section, we will discuss two techniques commonly used in deep neural networks

to circumvent this problem, specifically batch normalization and residual connections.

2.2.2.1 Normalization

Batch normalization (BN) is a technique used to improve the performance and
stability of deep learning model introduced by [56]. The use of BN reduced the Top-5
error on the ImageNet dataset of the GoogLeNet esemble from 6.67% to 4.9%. This
technique is said to improve the training dynamics of a deep neural network by
reducing the internal covariate shift, which is the change in the distribution of the
inputs to each layer of the network as is is trained. BN addresses this issue by
normalizing the activations of each layer in the network, i.e. for each layer, the mean
and variance of the activations are computed over a mini-batch of data, and the
activations are then scaled and shifted using these statistics, such that the activations of
each layer have a stable distribution. This technique allows the use of a higher learning
rate which leads to faster learning. BN is applied to the activations through the

following equations:

1 <3
= (é)
=— r
UB >
2 _ (i) 2
03 >= — rT’ — pp
B ma 2! Hp)

a) = a — pp
Vox +€

y) = 78 48

(2.1)


where 2“ is the ith input in the batch, jzg is the mean of the batch, o?, is the variance
of the batch, © is the normalized input, y“ is the output of the batch normalization, 7

is the scaling factor, and 7 is the shifting factor.

2.2.2.2 Residual Connections

[21] introduced the residual networks (ResNets) which are CNNs that utilize “skip
connections”. In this study, skip connections were used to perform residual mapping by
providing a path from the input directly to the output without passing through the layers.
These networks were able to push the saturation point of accuracy and depth, being able
to benefit from additional layers further than other networks. The ResNet was able to
win various visual recognition competitions, such as the 2015 ILSVRC and the COCO
competitions with relatively fewer parameters and less computational complexity than
other networks. This has been attributed to the depth of the representations in the model.

Figure 2.5 shows how skip connections are implemented.

weight layer

x
identity

Figure 2.5: Example of a residual block.

The use of skip connections have several benefits which contributed to the
outstanding performance of ResNets. First, these skip connections can alleviate the
vanishing gradient problem in very deep neural networks by providing an additional
path in which information can flow. Second, the architecture of ResNets allows for
faster convergence than other neural networks, even those which use specialized
initialization and normalization layers [57]. It has been conjectured that the saturation
problem in very deep neural networks is not because of the vanishing gradients, but

because of their low convergence rates. Thus, the benefits of ResNets lie beyond just


maintaining the gradient flow in the network [21]. The benefits of the addition of
residual connections is apparent in deeper layers. [21] compared the Top-1 error of
plain networks and ResNets with the same dimensions on ImageNet, where the
18-layer ResNet achieved 27.88% error while the plain network achieved 27.94%,
while the 34-layer ResNet achieved 25.03% error while the plain network achieved
28.54%.

2.2.3. Dynamical isometry

While empirical evidence demonstrates the power and effectiveness of deep
networks in solving complex tasks, the underlying mechanisms governing their
learning dynamics are not fully comprehended. [15] first sought to shed light in these
theoretical underpinnings by analyzing deep linear neural networks. They introduced
the concept of dynamical isometry, a mathematical condition which allows for faithful
backpropagation. A model achieves dynamical isometry when it is operating in a
regime called the edge of chaos where the gradients are guaranteed to never vanish nor
explode. Dynamical isometry is defined as the condition where the singular value
distribution of the Jacobian of a model (the matrix of all first-order partial derivatives
of a vector-valued function) is centered at an O(/) constant. When the Jacobian of a
model has very few small and large singular values, it is expected that the gradient will
not decay nor explode during training. Achieving dynamical isometry is especially
beneficial for very deep networks which are prone to suffering from vanishing
gradients. They showed that Gaussian initialization does not result in dynamical
isometry, but orthogonal initialization does. Thus, the use of orthogonal initialization
allows trainability of very deep vanilla neural networks with training time independent
of depth. Furthermore, the presence of dynamical isometry allows training of very
deep neural networks even without the use of batch normalization or residual

connections.

Pennington and Schoenholz extended dynamical isometry analysis to nonlinear
models through free probability theory, which allowed the analytic computation of the

entire singular value distribution of a network’s Jacobian. In their work, it was found


that ReLU networks are not capable of dynamical isometry, while Tanh (and other
sigmoidal activation functions) with orthogonal initialization, and not Gaussian, allows
singular values which are arbitrarily close to 1. They also posed that having all the
singular values be equal to one rather than having a mean of one is a much stronger
condition [58]. They showed how orthogonal sigmoidal networks can learn faster than
ReLU networks due to this property, regardless of the optimizer used. Similar to linear
networks from [15] although on a lesser degree, these orthogonal sigmoidal networks
benefit from sublinear training time with depth. Furthermore, they have shown that
these sigmoidal networks can achieve better performance than ReLU, contrary to

conventional knowledge regarding activation functions.

The concept of dynamical isometry may explain the behaviour of certain
architectures and initializations. For instance, batch normalization (BN) pushes the
singular values away from 1. Without skip connections, networks with BN are
untrainable for common initialization schemes and suffer from gradients exploding
with depth [59]. Meanwhile, residual networks are said to always be on the edge of
chaos, hovering over the boundary between stability and chaos, for different nonlinear
activations. At the edge of chaos, the network has approximate dynamical isometry,
i.e., the singular value distribution is concentrated in a range O(1) [15]. This condition
allows the networks to preserve the gradient flow without sacrificing the expressivity
of the network. Although BN has shown to improve accuracy in some CNN
architectures with no skip connections, such architectures have not been successful in
maintaining trainability at the same depth as residual connections which can achieve

practical accuracy up to 1000 layers [21].

2.2.4 Deeper networks result in higher computational costs

The computational complexity of training a DNN is determined by the operations
involved in the forward and backward propagation. During forward propagation, the
DNN is presented with data which it processes through a series of operations, as
determined by the network’s layers, consisted of matrix multiplications, element-wise

operations, and non-linear activation functions to make a prediction. In backward


propagation, the gradient of the loss function is calculated to be utilized by an
optimization algorithm to adjust the weights in the direction of minimizing the loss

[60]. This procedure is approximately twice as expensive as the forward pass [61].

From the described procedures, it can be inferred that the computational
complexity of a DNN increases with the number of layers, neurons, and connections in
the network. This is demonstrated in Figure 2.6 which shows the giga floating-point
operations (G-FLOPs) of different convolutional neural networks. Per specific
architecture, the network with more layers (as specified by the number in the label)
had higher G-FLOPs and higher accuracy [62]. Thus, it follows that designing
networks to be deeper and deeper to achieve better accuracy would also continuously

increase the computational complexity which presents practical challenges.

Net-A-Large

Top-1 accuracy [%]

Operations [(G-FLOPs]

Figure 2.6: Accuracy vs. G-FLOPs of different convolutional neural networks.

The computational complexity requires expensive hardware infrastructure that is
not accessible to a lot of users, thus creating a landscape where only large companies
can leverage the full potential of deep networks for their research or applications.

Beyond the training phase, the necessity of powerful hardware for deployment and


inference limits implementation in real-world scenarios, where cost-effective and
energy-efficient hardware is important [63]. Applicability on edge devices is crucial
due to the rising interest in the utilization of such technology. Edge devices eliminate
the need for data transmission to remote servers for analysis, reducing the time delay
and enhancing data security [64]. Applications such as autonomous vehicles,
surveillance systems, or healthcare monitoring greatly benefit from low-latency
processing [65]. Furthermore, the computational complexity results in higher energy
consumption during training, contributing to increased carbon emissions which are
harmful to the environment [6]. Figure 2.7 shows the carbon emission equivalent of
training modern deep learning models [66].

CO2 Equivalent Emissions (Tonnes) by Selected Machine Learning Models and Real Life Examples, 2022

cceloni et al, 2022: Strubell ot al. 20% | Chart: 2023 Al index Report

GPT-3 (1758)

OPT (1788)|

Car, Avg. Incl. Fuel
Lifetime

sLoom (1768) |

American |
Avg., 1 Yoar
Human Life, |
Avg., 1 Year

Alt Travel,
1 Passenger, NY-SF

200 250 300 350 400 450 500
CO? Equivalent Emissions (Tonnes)

°
5|

Figure 2.7: CO2 equivalent emissions of modern deep learning models.

The computational complexity of training deep neural networks poses many
challenges. Addressing these challenges is crucial for enabling widespread adoption of
deep learning while promoting efficiency and sustainability. Thus, there is interest in

the development of different model compression techniques.

2.3. Model Compression to Alleviate Resource Demands of Deep Neural
Networks

The continuously increasing depth of CNNs to achieve higher accuracy drives up
the computational and energy costs of model training and deployment [5], which
serves as motivation for the development of model compression techniques [7], an
example of which is model pruning. Model pruning optimizes the neural networks by
training a densely-connected neural network then removing the insignificant
parameters in the networks based on a criterion, such as magnitude [8; 67]. This
technique not only reduces the size of a network, it also improves performance by
making the model less prone to overfitting as it removes redundant parameters [68].
This technique has been shown to reduce the parameter count of trained networks by
up to 90% without diminishing the accuracy [69]. [70] found that large, pruned models
consistently out-performed their smaller, dense counterparts with identical memory

footprint .

One of the earliest studies about pruning was done by LeCun in the paper Optimal
Brain Damage (ODB). The motivation behind ODB is that majority of the connections
in a neural network can be removed without repercussions as only a small number of
them are critical for the performance of the model [8]. This is inspired by synaptic
pruning in the human brain. Babies are born with roughly the same number of neurons
as adults, but they have more connections between these neurons. During brain
development, excess connections are pruned, while the remaining connections are
strengthened [71]. To perform ODB, a network is first trained, the connections are
ranked by “importance” based on their magnitude and effect on the error of the model,
and finally the least important connections are removed. This process is repeated until
the target sparsity is reached. Another early form of pruning is the Optimal Brain
Surgeon (OBS), which uses second-order information, or information coming from
calculating the second derivative with respect to the weights, to determine the
importance of each connection [67]. Although effective, the drawback is that the

second derivative is costly to calculate.


2.3.1 Types of Pruning by Schedule

Model pruning techniques can be divided based on the pruning schedule: pruning
at initialization and pruning after training. Most pruning methods are implemented
after training, where the model is trained to convergence, pruned, and then finetuned
to recover the accuracy loss. These techniques do not reduce the training costs as they
require training a dense model, and thus have potential benefits only during inference
time. These drawbacks led to the development of pruning at initialization methods.
State-of-the-art methods include SNIP [72], GraSP [73], and SynFlow [12]. However,
subnetworks acquired through pruning at initialization don’t perform as well as the

subnetworks acquired from pruning after training [74].

2.3.2 Types of Pruning by Granularity

Pruning methods can be divided in terms of the granularity at which pruning is
performed. In MLPs, pruning can be done either on individual connections or weights
between neurons or on entire neurons [75; 76; 77]. In CNNs, pruning can be done
on four levels: on weights (also called fine-grained), vector-level, kernel-level, and
filter or channel-level as illustrated in Figure 2.8. Fine-grained and vector-level are
both intrakernel pruning methods which prune the parameters inside the keel, only
that vector-level pruning has a higher level of structure in which rows of the kernels are
removed. Meanwhile, kernel-level pruning zeroes out entire kernels while channel-level

pruning removes all the kernels of a specific input or output channel [78].


Finc-grained Vector-level Kernel-level Filter-level
Sparsity(0-D) Sparsity(1-D) Sparsity(2-D) Sparsity(3-D)

Figure 2.8: Different pruning granularity with increasing regularity.

Fine-grained pruning generally achieves better accuracy than coarse-grained for
the same sparsity level. The advantage of coarse-grained pruning such as filter and
channel-level, on the other hand, is that models pruned with such methods can enjoy
acceleration even on general purpose CPU or GPU [79; 80; 81]. Networks pruned at
the kernel-level can be accelerated using recent CUDA Deep Neural Network
(cuDNN) library from NVIDIA through Winograd decomposition [82], while those
pruned at the vector-level requires custom hardware [78]. Meanwhile, fine-grained
pruning requires hardware specialized for sparse matrix multiplications to result in
model acceleration (83; 84]. Coarse-grained pruning can also result in smaller models
compared to fine-grained pruning for the same sparsity level due to how sparse neural
networks are stored, where alongside the weights, a set of indices describing the
location of nonzero values are also stored, similar to the Compressed Row Storage
(CRS). Due to the regularity of the sparsity in coarse-grained pruning, the sparsity in
the model can be represented using a fewer number of indices. Meanwhile, for

fine-grained pruning, there is a unique set of indices for all values [78].

2.4 Graph Theory Perspective of Deep Neural Networks

Techniques such as network pruning have been widely investigated to create sparse
networks. The performance of these sparse networks depend not only the evolved

weights, but also on the sparse architecture [84]. Graph theory provides a powerful


framework for representing and analyzing connectivity patterns in such networks.
Neural layers can be represented as bipartite graphs which can be analysed in terms of
graph properties, i.e., connectivity. Consequently, concepts in graph theory can be
leveraged to construct efficient connectivity patterns that promote information flow
and enhance the expressive power of neural networks. This sections discusses the
graph representation of neural layers and the type of graphs which can be leveraged in

the construction of sparse neural layers with advantageous qualities.

2.4.1 Bipartite graphs

In graph theory, a graph G is a pair comprised of a set of vertices (or nodes) V and
edges E that connect these nodes. Figure 2.9 shows an example of an undirected graph

with six nodes and seven edges [85].

Figure 2.9: Example of an undirected graph with six nodes and seven edges.

One type of graph is the bipartite graph G(U, V, E), characterized as a graph with
nodes that can be divided into two disjoint sets of nodes U and V with edges E,, where
the edges of each set are all directed to the other set. Figure 2.10 shows an example of

a bipartite graph [86].


Figure 2.10: Example of a bipartite graph.

A bipartite graph can be represented as a matrix, where the rows represent the
vertices in one set and the columns represent the vertices in the other set, referred to as
the biadjacency matrix. The element at the i** row and j** column is 1 if there is an
edge between the i*” vertex in the first set and the j** vertex in the second set, and 0

otherwise. The bipartite graph shown in Figure 2.10 can be represented as the matrix:

Fil 0011

In this matrix, the nodes in the first set are represented by the rows, while the nodes
in the second set are represented by the columns. From the biadjacency matrix B, one

can derive the adjacency matrix A [87]:

Ou. BT

A= (2.2)
B 0nv


2.4.2 Ramanujan bipartite graphs

Expander graphs are a type of sparse graph with strong connectivity properties that
make them useful in a variety of applications, such as error-correcting codes,
cryptography, and network routing [88]. Suchs graphs can be characterized through
vertex, edge, or, if the graph is d-regular, i.e., all the nodes have the same number of

edges given by d, spectral expansion.

For d-regular bipartite graphs to be expanders, the eigenvalues of the adjacency
matrix satisfy |}; — A2g| < 1 — A1/d, where \, and 2g are the largest eigenvalues [11].
Meanwhile, it is said that the optimal type of expanders are Ramanujan graphs. A
graph is Ramanujan if the eigenvalues of its adjacency matrix satisfy the condition
dz < 2Vd — 1, where 2g is the second highest eigenvalue of the adjacency matrix. For
(c, d)-regular graphs, this condition is generalized to \2 < Vc — 1+ Vd —1. This is
considered as the strictest constraint for expander graphs while allowing for the

existence of infinite families [89].

2.4.3 Connectivity in Pruned Networks

There are studies which analysed the structure of sparse neural networks through
the perspective of expander graphs. [24] showed that the “expander-like” properties of
neural networks (e.g. spectral and algebraic connectivity) played a larger role in their
performance compared to the density of the connections. To analyse neural networks
through this perspective, they represented the layers as bipartite graphs and provided a
definition for the associated adjacency matrix, such that for an unweighted adjacency

matrix Adj, € R"™™:

1 if (t,j)€ Er
Adji{i][3] = (2.3)
0 otherwise

EF, denotes the set of edges of the bipartite graph, and (i, j) the connection between

node i from layer / and node j from layer (/+1). This adjacency matrix is used to induce


sparsity to the weight matrix it corresponds to. The paper proposed various construction
methods to set the probability of a connection between nodes i and j P[Adj[i][j] = 1].
Random construction, for instance, uses P[Adj[#][j] = 1] = k/m where k is a fixed
parameter and m is the number of nodes in one side of the bipartite graph. In random d-
regular expander construction, each node is constructed to have a fixed number of edges
d. The best performing construction method, Fibonacci Rotating Edge Construction,

utilized the Fibonacci numbers to determine the connections between nodes.

[11] based the sparse architecture X-Net on expander graphs, graphs described as
sparse yet highly-connected, inspired by the high connectivity of the state-of-the-art
CNN architectures such as ResNet and DenseNet-BC. As standard pruning methods
are weight-dependent, these methods are agnostic to the connectivity of the network,
which introduces the risk of some nodes being disconnected. These disconnected nodes
not only hinders information flow, they also consume memory without contributing
to the representational power of the model. This study preserved the connectivity in
the model by making sure that every input is connected to every output and that the
number of edges between the bipartite graph representing the layers is proportional to
the product of the two vertices, i.e. using random d-regular bipartite expander graphs
to form the structure of the sparse layers. The expander based models were able to
achieve 4% higher accuracy on MobileNet than group convolutions for the same level

of sparsity.

[26] improved upon the X-Net through their proposed architecture RadiX-Net,
which was based on mixed-radix numeral systems. The main advantage of RadiX-Net
over X-Net is it does not have the requirement of having the same number of nodes in
the input and output layers. These technique saves up on the cost of training a dense
model, as the sparse layers are not uncovered from a dense model, but are built as

sparse.

[25] sought to explain the existence of winning tickets through the connectivity of
its sparse structure, specifically from a graph expansion point of view. They showed
that the rapid decrease in accuracy upon reaching a certain sparsity can be attributed to

the loss of the Ramanujan property of the sparse subnetwork.


[90] proposed the Ramanujan Bipartite Graph Product (RBGP) framework to
generate structured, block-sparse, and highly-connected neural networks based on the
product of Ramanujan graphs. To produce the layers, they defined the bipartite graph
G of a layer as the tensor product of Ramanujan bipartite graphs
G = Gi @,---@,Gx, where K is the number of base graphs. The biadjacency
matrix of a graph product G, = G @), G2 is the tensor product of the biadjacency
matrices of G, and G2, where B, is constructed by replacing the non-zero elements in
B, with the B2 matrix, and the zero elements of B, with Oj, where m,n are the
dimensions of Bz. The authors demonstrate the effectiveness of their RBGP4 sparsity
pattern on CIFAR, where they achieved 5-9x and 2-5x runtime gains over
unstructured and block sparsity patterns respectively, without diminishing the

accuracy.

2.5 Achieving Dynamical Isometry Through Orthogonality

As discussed in Section 2.2.3, dynamical isometry can be achieved by having all
the neural layers be an orthogonal transformation. This section discusses how to
achieve orthogonality in multilayer perceptrons and convolutional neural networks.
This section also discusses how dynamical isometry is lost and restored in pruned

networks, in relation to the orthogonality requirement of dynamical isometry.

2.5.1 Orthogonality in multilayer perceptrons

A multilayer perceptron can be made orthogonal by assigning orthogonal weights,
as its associated transformation is the matrix multiplication of its weight matrix to the

inputs [15].

2.5.2 Orthogonality in convolutional neural networks

The convolution operation can be represented as a doubly block-circulant Toeplitz
(DBT) matrix, as shown in Figure 2.11. For a convolutional layer with kernel
K € RMxCx2x2, input X € RO*4*4, and output Y € R”*3*3, the DBT matrix can be


defined as K € R(MHW)(CHW)_ This matrix can be interpreted as a combination of
block matrices with size W’ x W for each kernel, where each row in this matrix
represents the spatial location of the kernel on its respective input.

Y = Conv(K, X), stride 1

Output ¥ € RM*3%3 Kernel K € RMXCX2x2 Input X € ROx***
MY ss

l

9s + 2, r
> om. ‘ I
€ input channel

| Jom natrix

Input channel |

Output channel |

Output cha

Output channel M

ese

Output channel M

Input channel C

Figure 2.11: Doubly block-circulant Toeplitz matrix representing a convolution
operation.

Due to this structure, orthogonalizing the associated transformation, or the Jacobian,
of a convolution is more challenging than in MLPs. In MLPs, the Jacobian matrix J can
be orthogonalized simply through the use of orthogonal weights as the weight matrix
is multiplied to the inputs to produce the features, while simply orthogonalizing the
weights of filters is not enough to achieve orthogonal convolutions [91; 92; 93]. This
section will discuss the different orthogonalization techniques used for convolutional

neural networks.


2.5.2.1 Delta-Orthogonal Convolutions

[16] proposed an orthogonal initialization scheme for convolutional neural
networks which enabled training of a 10,000-layer CNN without batch normalization
nor residual connections. They developed a mean field of theory for CNNs, similar to
the works of [15; 58] for MLPs, through which they characterized the necessary
conditions to attain dynamical isometry, i.e. the convolution operator should be an
orthogonal transformation in the sense that it is norm-preserving. Their construction
was derived from literature on generating random orthogonal matrices with
block-circulant structure that is reminiscent of the convolution operation. A Cout X Cin
orthonormal matrix is constructed, where each element is assigned to the center
parameter of each kernel of the weight tensor leaving the rest of the parameters as
zero. Although it allows training of very deep neural networks, the authors note that
the accuracy of the vanilla CNN saturates past a certain depth, which suggests that
techniques such as batch normalization and residual connections do more than just

enable training of very deep neural networks.

2.5.2.2. Skew-Orthogonal Convolutions

[94] proposed the gradient norm preserving convolutional layers Skew Orthogonal
Convolution (SOC) that they used to construct a 1-Lipschitz compliant neural network
with an architecture comprised only of convolutional layers and the MaxMin activation
function (dubbed as LipConvNet-N). These layers are characterized as having skew-
symmetric Jacobians in which they acquire the orthogonal convolution from through

the Taylor series expansion of their exponentials.

In their work, they characterized the linear transformation of a 2D multi-channel

convolution with m input and output channels as an m x m matrix whose entries are

2 2

doubly block-circulant matrices with dimensions n* x n* assuming that the input and
output have the same dimension n. From a property of circulant matrices, i.e. the
eigenvectors of a circulant matrix are Fourier basis vectors, they showed that the
singular values of a convolution can be computed through the 2D Fourier transform of

each circulant matrix, and by forming n? number of m x m matrices, whose elements


are the i'* eigenvalue for each i € [n?]. They related the weight tensor to the singular

values of its associated linear transformation through the theorem:

Theorem 1 For any expanded convolution Weony € R°*"*©**, let M be the matrix
encoding the linear transformation of a convolution with W. For each p,q € {n| x [nl],

let P be the c x c matrix computed by

Vs, t € [¢] x [ce], P®[s, t] = (FTW, :, 5, t]Fa)pq (2.4)
where the singular values of the associated transformation M = ane = J are given
by

o(M)= |) o(Pe) (2.5)

pe(n},qe[n]

2.5.2.3 Explicitly-Constructed Orthogonal Convolutions

[95] introduced explicitly-constructed orthogonal (ECO) convolutions. From the
theorem proposed by [94], they theorized that orthogonal convolutions can be achieved
by setting the singular values of P® equal to 1, attained by constructing each
Pp, q,:,:] to be orthogonal. The weight tensor W[;, :, s, t]Vs, t can then be recovered
through the 2D inverse transform of P[:,:, s, t]Vs,t, which is then implemented with
dilation n/k. They implemented this to the same network, LipConvNet-N, as in SOC.
Their method can achieve competitive standard and robust accuracy while being more
efficient in terms of training and inference time compared to two existing orthogonal
convolution methods, Block Convolutional Orthogonal parameterization (BCOP) [96],
and SOC. BCOP requires 4x the standard convolution as it uses complex values, SOC
performs the convolution multiple times on the feature map to acquire the Taylor
expansion, while ECO requires the computational cost of a standard dilated

convolution.


25.3" Dynamical Isometry in Pruned Networks

Pruning weights in neural networks leads to loss of dynamical isometry, and this
loss contributes to the degradation of accuracy by hindering gradient error and signal
propagation during training [13; 14; 97; 98]. We can gain an intuitive understanding
of how pruning destroys dynamical isometry by taking an orthogonal weight matrix,
zeroing out some of the entries, and then finding that the gram matrix of W is no longer
an identity matrix. This happens because the orthogonalization precedes the sparse
Structure instead of constructing specific sparse structures first and then assigning the
appropriate values to achieve orthogonality [99]. Several studies have then presented
methods to recover dynamical isometry in pruned models, which can be categorized as

either rescaling or regularization [14; 13; 98].

[100] proposed finetuning the pruned pre-trained model using an orthogonalized
version of its weight matrix W ¢€ R™*" obtained through QR decomposition:
Q, R = qrd(W), where R € R"*” is an upper triangular matrix, and then acquiring
the orthogonalized version W* of the matrix W, W* = Q © sign(diag(R)). The
recovered neural network outperforms the pruned model, and the speed of recovery
increased with the learning rate. Meanwhile, [101] restored dynamical isometry in
pruned networks by rescaling the weights of linear and 1D convolutional layers using a
scaling factor derived from the weights, width, and the pruning mask. Such rescaling
presented up to 4.25% higher accuracy than just magnitude pruning at 987 sparsity on
ResNet-104 trained on CIFAR-10.

[13] trained the pruned model with regularization of the weight matrix by
optimizing min||(C! © W)T(C! © W’) — T'l[p, where (C! © W’) is the pruned
weight matrix. They found that layerwise dynamical isometry provides better results
with models pruned at initialization as it improves connection sensitivity which is used
as the saliency criterion. Their method showed benefits on the accuracy of various
CNNs such as VGG and ResNet, where the pruned models that were trained with
regularization after pruning achieved up to 0.70% accuracy improvement than without
regularization. Orthogonality preserving pruning (OPP) [98] which adds a penalty

term for the gram matrix G = W © W' of the weights attempts to orthogonalize


“important” filters and drive the rest to zero. They also penalized the learnable scale
and offset parameters of batch normalization layers so that activations from pruned
filters would not be propagated to deeper layers and harm dynamical isometry. Their
method showed up to 1.30% higher accuracy compared to standard pruning methods
on ResNet-56 trained on the CIFAR-10 dataset at 95% sparsity. These methods
employ regularization and additional training on the pruned model, which introduces

additional computation.

2.6 Critical Analysis

Deep neural networks (DNNs) have proved to be useful in various applications.
Common DNN architectures are multilayer perceptrons (MLPs) and convolutional
neural networks (CNNs). Although MLPs built the foundation of DNNs, these models
suffer from overfitting and high computational complexity due to the large number of
parameters. This brought the development of CNNs which has since become the staple

for computer vision tasks, due to its locality, invariance, and use of repeated weights.

The development of DNNs has been driven by depth, supported by theoretical
studies and empirical evidence such as the trend in the winners of the ILSVRC. Along
the path to this development, it was discovered that deep networks suffer from
ill-conditioned gradients, at a certain point, the accuracy saturates with depth and past
that, it starts to get worse. Researchers then proposed specialized techniques such as
batch normalization and residual connections, which improved the performance of the
neural networks in ways more than just mitigating the gradient flow in the network.
Meanwhile, dynamical isometry, achieved through orthogonal weight initialization,

allows the network to maintain trainability even without such specialized techniques.

Although depth improves accuracy, it also results in higher computational costs.
Notably, it took a jump from 22 to 152 layers to reduce the ImageNet classification
error from 6.7% to 3.6%, showing that performance does not improve efficiently with
depth. Due to this, model compression techniques are used, one of which is model

pruning. However, model pruning has been shown to damage the flow of information


in the network in two major ways: first in terms of the connectivity in the architecture

and second in the loss of dynamical isometry in the network.

The studies regarding model pruning and dynamical isometry have all been about
restoring dynamical isometry through different rescaling and regularization methods.
To restore dynamical isometry, [100] performs weight rescaling after pruning a dense
model that has been trained with L2 regularization. More notably, this only
orthogonalizes the weight matrix which does not necessarily translate to orthogonal
convolutions. [102] only rescaled linear and 1D convolutional layers and did not
propose a technique for 2D convolutions which comprise most convolutional neural
networks. Meanwhile, the methods of [13] and [98] are based on the orthogonality
regularization of the gram matrix of the sparse weights. None of these studies present
a heuristic that can provide dynamical isometry and sparsity at initialization, all of
which present additional computational complexity during training. Furthermore, none
of these studies implemented their recovery method on very deep vanilla neural
networks. Though there have been studies which exploited expander graph properties
[24; 11; 26], and specifically Ramanujan graphs [90] in the structure of sparse neural

networks, none have been incorporated with the concept of dynamical isometry.


CHAPTER 3

METHODOLOGY

The first section of this chapter describes the concepts that underpins the proposed
method, i.e., dynamical isometry, Ramanujan property, and orthogonality in neural
layers, followed by a discussion of the proposed method SAO and its implementation
on multilayer perceptrons (MLP) and convolutional neural networks (CNN) which are
based on two orthogonal initialization methods, Delta-Orthogonal Initialization
(SAO-Delta) and Explicitly-Constructed Orthogonal Convolutions (SAO-ECO). The
second section discusses the existing methodologies we adopt for our experimental
procedures, including datasets, models, training schemes, and the pruning methods to

which our proposed method is compared to.

3.1 Dynamical isometry in neural networks

Neural networks attain dynamical isometry when all singular values of the
input-output or end-to-end Jacobian matrix J are unity, corresponding to factorizing
the Jacobian into an orthogonal transformation via singular value decomposition. The
input-output Jacobian matrix of an MLP layer is given by the weight matrix W, while
for convolutional layers, it is given by the associated doubly-block circulant matrix of
all the kernels [103]. Given a function f : R” — R”, the Jacobian matrix J of f is an

m x n matrix defined by [104]:

_ Ofi
Jij= an; (3.1)


where J;,; is the (i, 7)** element of J, and gh is the partial derivative of the 7*”
component of f with respect to the j** variable. In other words, the Jacobian matrix J
expresses the local linear approximation of f near a point x as a matrix transformation.

Specifically, the Jacobian matrix J at a point z is given by:

of, of oft
Or, 0x2 Orn
oh oh Sho
x x: Orn
J(z)= |" * (3.2)
Ofm  Ofm Ofm
6x1 8x2 Orn

The Jacobian matrix for a deep neural network with L layers with input xp € R” as

J ¢ R™*” may also be defined as follows [58] :

ge
J=— =[[p'w' (3.3)
l=1

where Di, = ¢’(hi)6;; and 6,; is the Kronecker delta function and W' is the weight
matrix of the J‘* layer in the neural network. The singular values of J are computed

from the singular value decomposition:

U,E,V" = 0(J) (3.4)

where the singular values are the diagonal elements of & € m x n and U € R™*™

and V € R”*” are complex unitary matrices representing rotations or reflections.

For linear neural networks, dynamical isometry is perfectly attained through
orthogonal weight initialization. Non-linear activation functions compromise
dynamical isometry, but it was empirically observed that employing orthogonal weight

initialization [15; 13] may still produce a well-conditioned Jacobian sufficient to


mitigate vanishing and exploding gradients [105] when the singular values of the
Jacobian closely approximate unity. ReLU networks do not attain approximate
dynamical isometry with simple orthogonal weight initialization [58] but possible with
shifted and smoothed variants [105].

To achieve dynamical isometry with the ReLU activation function, the orthonormal

matrix H is given by:

| | Ho —Ho
H! = (3.5)

—Ho Ho

where Ho! € Re x7 is an orthonormal matrix [106].

3.2 Orthogonal weight initialization for multilayer perceptrons

Figure 3.1 illustrates the Jacobian of the weight matrix, J of an MLP layer. The
Jacobian is given by the weight matrix Wy-, such that orthogonalizing W, results in
an orthogonal J [15].

ome Matrix

Figure 3.1: Transformation of a 4 x 16 MLP layer.


3.3. Orthogonal weight initialization for convolutional neural networks

Figure 3.2 shows a visual illustration of the 2-dimensional convolution operation
expressed as a doubly block-circulant Toeplitz (DBT) matrix where the 2-dimensional
weight “filter” has been unwrapped into a basis of 1-dimensional vectors comprising of

spatially shifted versions of the same vectors.

ARIE
6/;7\\8 Bu
9 liol athe! iDulenlFal
13}14/15|16] |Gis)Hiij ty
Input Filter

123 4 5 67 B 9 10 11 12 13 14 15 16

Ay By Cy Diu Ey Fu Gu Hu In | | | | [13]
An[BifGx| — [DujEa|Fis) |Gi|Har|Tn i [14]

7 x
Cy Du Eu Fy Gy Hy; hy 115 |
[| | (ARBayeR] OnleniFR [Guia] [16]

DBT Matrix

a}a

kon a=
=
Z

Figure 3.2: Doubly block-circulant (DBT) matrix representation of a 2D convolution
operation.

3.3.1 Delta-Orthogonal Initialization

Delta-Orthogonal Initialization for convolutional neural networks enabled training
of a 10,000 layer CNN without special layers like batch normalization and residual
connections [16]. The initialization algorithm is summarized in Table 3.1 and illustrated
in Figure 3.3. Two-dimensional kernels are initialized with a single, non-zero parameter
at its center. The construction was adopted from the literature on generating block-

circulant, random orthogonal matrices.


Table 3.1: Delta-Orthogonal Initialization algorithm for CNN filters.

Delta-Orthogonal Initialization for 2D CNN kermels

Require: k kernel size, cin and Cou input and output channels
Return: ak x k X Cin X Cout Weight tensor Weonv
Step 1. Randomly generate a cin X Coy, matrix H with orthonormal rows

Step 2. Define a k x k x Cin X Cout tensor Weonv in the following way:
for 8, 8’ in 0,1,...,k — 1,7 =0,...., Cour — 1, set

H(i,j) if 6,8’ = [k/2|

0 otherwise

Weoonu(8; 85455) =

Sa) Cr

ces as Be

“a ca ce oe

J
&

+ 5H BH Be

Assign O[s, t] to W[1, 1, s, t] Vs, t € [c] x [c]

Figure 3.3: Delta-Orthogonal Initialization algorithm for CNN filters for Cin = Cou =
4 and k = 3.

3.3.2 Explicitly-Constructed Orthogonal Initialization for CNNs

Orthogonal convolutional weights may be achieved by constraining all singular
values of the Jacobian weight matrix to unity. Rather than explicitly orthogonalizing
the block circulant weight matrix, orthogonality was imposed by initializing matrices
with all-unity singular values in the Fourier domain and subsequently constructing the

orthogonal weight matrix using the inverse Fourier Transform [107].

For an expanded convolution kernel Weony € R°*"*°*°, for each p,q € [n] x [n],


let P®) be the c x c matrix computed by:

PPM s, t] = (FIW[:,:, 5, t]Fa)pq V8.t € [e] x [e] (3.6)

Orthogonal convolution kernels may be obtained according to Table 3.2 and illustrated
in Figure 3.4:

Table 3.2: Explicitly-Constructed Orthogonal Convolutions algorithm for CNN filters.

Explicitly-Constructed Orthogonal Convolutions for 2D CNN kernels

Require: kernel size k, input and output channels cin and Cout

Return: ak x k X Cin X Cou Weight tensor Weonv

Step 1. Construct the orthogonal matrices P[p, q, :,:] V(p, q) € [k] x [k] with
SIZE Cin X Cout

Step 2. Acquire W[:, :, s, t] through IDFT2pP[:, :, s, t] 'V(s,t) € [k] x [k]
Step 3. Perform the dilated convolution with W with dilation n/k

eee

J Fel }

i[Pa[Ralbs| |) ‘ [ i] [ma 5 | ayeafe] = (Baye) [Aalea!

iTbsalBashod| [inn] ha al CIC ic] [Puffa [Pala
ail lin a 2] hae ln} [GasfHHas} La} [Gua] Moa

Py Pos Fava me aa Rieti cae

P

Generate n? number of OER Reshape the tensor to form PI: , 8, t) Perform 2D IDFT:p on PI: :, 8, t)
and assign to Pip, q, :,:) Vp. q € [n} x {n} Vs, 1 € (c} x (c} V8, t € (¢] x [c¢] to acquire the WeR™—

Figure 3.4: Explicitly-Constructed Orthogonal convolutions algorithm for CNN filters
for Cin = Cout = 4 and k = 3

3.4 Sparsity-Aware Orthogonal Initialization

In this section, we formulate our SAO method and describe how we compare the
performance of SAO construction to using conventional pruning methods when

introducing sparsity to multilayer perceptrons and convolutional neural networks. We


apply sparsity-aware orthogonal initialization (SAO) by forming a sparse, orthogonal
matrix S and then using this as the orthogonal matrix from which the weight matrix
will be derived from to attain orthogonal transformations, distinct for multilayer
perceptrons and two different methods, ECO and Delta-Orthogonal Initialization, for
convolutional neural networks. Forming the S € R™*" comprises two key steps:
generating the sparse, Ramanujan binary matrix M € {0,1}™”*” and orthogonalizing
M to generate the matrix S € R™*”. The sparse structure comes first before the values

are assigned, such that the orthogonal initialization is aware of the sparsity.

The sparsity of individual neural layers is controlled by setting the connectivity
parameter of each neuron, or their degree. The degree of a neuron is equivalently the

number of non-zero weighted connections emerging from each neuron to the next layer.

To understand if the advantages of SAO are due to topology alone or co-dependent
on the dynamical isometry property, we also compare against neural networks
initialized with only sparse Ramanujan topology (indicated with RG). RG uses the
same sparse topology as SAO but without orthogonal weight initialization. We
evaluated the RG layers with different weight initialization schemes: random normal

(RG-N), random-uniform (RG-U), and random orthogonal weights (SAO).

3.4.1 Ramanujan Graph Structure

In this work, we leverage (c, d)-regular bipartite graph expanders, 7.e., graphs
whose input and output nodes have degree c and d, to form the sparse structure of the
neural networks layers as in [11], but we use a specific type of expander graphs called
Ramanujan graphs which are maximally sparse. Each layer of a neural network may
be represented as a bipartite graph. Bipartite graphs G(U, V, E) are graphs with two
disjoint sets of nodes U and V and edges E that connect nodes between but not within
sets. Thus, U and V are the neural input and output activations, respectively. The
biadjacency matrix, B € {0,1}"*” represents the connections between input and
output neurons of a layer. Each entry of the matrix indicates the presence (1) or

absence of (0) of an edge between nodes from set U and set V. The biadjacency matrix


is the matrix transpose of the binary mask, M € {0,1}"*" which describes the
topology of the neural layer, where m and n corresponds to the output n,y¢ and input

Nin nodes, respectively.

The full graph adjacency matrix [87] of each layer is defined as:

A= (3.7)

All nodes have the same number of edges, d in a d-regular bipartite graph. For d-
regular bipartite graphs to be expanders [11], the eigenvalues of the adjacency matrix,
A obey |A; — A2| < 1 — A2/d, where ; and 2g are the largest eigenvalues. A graph is
a Ramanujan expander if \, also satisfies \2 < 2\/d — 1. For asymmetric neural layers

where c and d are the degree of the input and output nodes, this condition generalizes

torA2 < Ve—-14+ Vd-1.

We construct the mask M to be a (c, d)-regular block matrix comprised of identical
blocks. We control the variable associated to the smaller dimension: we assign c if

m > n, where d = © < cand assign d if m < n, where c = im <d:

1. Form <n:
M = [Mi;M;;... Men]
M, = Mp =... = Man € {0,1}4*"
where Myis (1, d)-regular

2. Form >n:
M= [MiM2...Me]

M, = M2 =... = Mea € {0,1}™*
where Myis (c, 1)-regular

3. Form =n:


either (1) or (2) is applicable

The first step to construct M is to create the block M, with dimensions and degree
as prescribed, i.e., form <n: (1,d)-regular M, € {0,1}4*4, forn < m: (c, 1)-regular
M, € {0,1}™*<, which is duplicated 2 and © times, and then concatenated in the
vertical and horizontal axis, respectively. For m = n, any of these two construction
methods can be used. After concatenation, the resulting matrix will match the target
degree and dimensions of M. The sparse, Ramanujan matrices that can be generated
through this process has two constraints: the larger dimension mazr{m,n} should be
divisible by the specified degree and the ratio of n and m should be equal to the degree
divided by a number r € Z to guarantee a degree of at least 1 in the larger dimension.

The entire procedure is summarized in Algorithm 1.

Algorithm 1: Constructing the Ramanujan Graph Structure.

Data: degree do, input and output channels cin and Cout;

Result: M ¢ R%* sparse matrix with Ramanujan graph structure;
Assign: c ¢ dg if Cin < Cout else d < do;

Assign: row <— min(Cout, Cin);

Assign: col <— max(Cout, Cin);

Assign: row, < col /c;

Assign: col, < col;

Assign: v; <— zero_tensor(col); /* Tensor with all zeros «/
Assign: v2 < one-_tensor(col); /* Tensor with all ones «/
while v, 4 v2 do

blo ck’. zero_tensor(rows, Col,);

columns <— list(range(col,));

for i in range(row,) do
index <— random-_choice(columns);
while sur index]) == 1 do

| i index < random. ndex) e(colurens);
block[i, index] = 1;

a

end: i jin Mele range(row»), range(coly)) do do
Folockfi. j] if J and sum(block[i]) < c and sum(block[:, j]) # 1 then
ock({i, j]
| We sum(block, 0);
end

en
Me zero_tensor(row, col);
a< row *c/col;
for i in range(a) do
M[row, * i : row, * i + row,] < block;
end
end

Convolutional neural networks are (c, d)-regular by definition where the local k x k

kernel are the (c, d)-sparse connections. Unlike multilayer perceptrons, we modulate


the sparsity of convolutional neural networks in increments of the kernel size k x k,
which is equivalent to pruning entire kernels in a channel. We illustrate this in Figure
3.5, where c = d.

\Aia} Bis} Cis [Av By2|Cy2
|Dy Ey Fy Dy2 Ey. Fy,
Gy Hy, ly Giz Hy. lz

Azs|Bzs|Cia} [AcalBra] Cra
Dos|Exs| Fes] |Dz«|Eza|Fos
Gzs|Hzs| los] |Go4|Hzal los

| Agr B31} Ca A32| B37} C32
Dai} Esi ne |Dsz E32 fal
[Gai] at faa | |Gsz}H32| Hy, it

FH EHH

D-Regular Bipartite Graph D-Regularity Is incorporated in a convolutional layer
at the kernel! level

Figure 3.5: Applying a d-regular structure to a convolutional layer resulting in kernel-
level sparsity.

3.4.2, Orthogonalizing the Ramanujan Graph Structure

We generate the sparse-orthogonal matrix S with approximate dynamic isometry by
processing the mask matrix, M to have mutually orthogonal row vectors. This produces

an orthogonal matrix for m = n, and a semi-orthogonal matrix when m # n.

1. For m <n: Generate n/d sets of orthogonal vectors, where each set has #7
number of vectors in d dimension. One set of orthogonal vectors correspond to
one set of identical rows in M. Assign the values of one vector to the nonzeros

of the row for each set.

2. Form > n: m/c sets of orthogonal vectors, where each set has © number of
vectors in c dimension. One set of orthogonal vectors correspond to one set of
identical columns in M. Assign the values of one vector to the nonzeros of the

column for each set.


3. For m = n: either (1) or (2) is applicable.

Figure 3.6 illustrates the process of generating an 8 x 8 matrix with c = d = 2,

following (1). The process is summarized in Algorithm 2.

als Hie |
a Asal ae |

M, eR [Gufs
| nD

MeR™ 7 SeR™

orthogonal matrix
Generate (1, d)-regular Stack copies of M, tomatch the Generate set of dxd Split each orthogonal matrix into
block matrix size of the layer, generating a orthogonal matrices rows then assign to a set of
d-regular matrix overlapping rows in M

Figure 3.6: Constructing sparse and orthogonal SAO matrices.

Algorithm 2: Orthogonalizing the Ramanujan Graph Structure.

Data: M € R%»*¢out Ramanujan matrix, do degree;

Result: S ¢ R%*°t sparse-orthogonal matrix;

Assign: c < dg if cin < Cout else d ¢ do;

Assign: v < mc if Cin < Cout else v + n/d;

Assign: z < cn/m if Cin < Cour else z + dm/n;

Assign: sao_matrix <- zero_tensor(M.shape);

Assign: num_ortho < int(c * M.rows / M.columns);

Assign: inv, counts <- unique(ramanujan_mask);

Assign: row-_index < [tuple(where(inv = i)[0]) for i, c, in enumerate(counts)
if counts[i] > 4);

if num,ortho == 1 then
to_iterate = inv;

else .
Dail = row-index;

for id i i je tq-iterate do
lentical_row = ramanujan_mask[i];

vals = orthogonal_matrix;
for j in range(identical_row.rows) do
nonzeros = nonzero(identical_row[j]);
identical_row[j, nonzeros] < vals[j];
sao_matrix[i] = identical_row;
end
end .
S < sao_matrix

3.4.3 SAO on Multilayer Perceptrons

When SAO is applied to multilayer perceptrons, S € R™*” is simply assigned as
the weight matrix. The density of a (c, d)-regular layer is given by c/m = d/n, where


c and d refer to the degree of the input and output nodes, respectively. We compared
the performance of SAO to the baseline multilayer perceptron and variants pruned with
one-shot magnitude global pruning with and without pretraining (GMP-T and GMP-
S), local random pruning (LMP) as well as networks with a priori sparse Ramanujan
construction with random uniform and Gaussian initialization (RG-U and RG-N). When
implementing SAO, Ramanujan constructions, and pruning methods, the output layer

is not included which is retained to be dense.

3.4.3.1 SAO Variants
For MLP-N, three variations of SAO were tested:

¢ SAO 1: All the sets v of identical rows/columns have unique orthogonal values,

and all the layers of MLP-N have the same sparse structure.

* SAO 2: All the sets v identical rows/columns have the same values as the other
sets, i.e., each set has the same dg x z or z x do, and all the layers have the same

sparse structure.

* SAO 3: All the sets uv of identical rows/columns have unique orthogonal values

z X do, and all the layers have a unique sparse structure.

3.4.4 SAO on Convolutional Neural Networks

SAO was applied to convolutional neural networks by adapting two methods for
orthogonal weights initialization (i) Delta-Orthogonal Initialization [16; 58; 106] (SAO-
Delta) and (ii) Explicitly Constructed Orthogonal Initialization [95] (SAO-ECO).

3.4.4.1 SAO-Delta

With the SAO-Delta scheme, for a convolutional layer with weight tensor W €
IR*xkxcinXcout_ we generate the sparse orthogonal matrix S € Rt*% and then assign

S[s, t] to Weonw|[§]; (8), s,t] Vs, t, as illustrated in Figure 3.7. All non-center weights


are zeroed at initialization but the weights are not removed. The weights initialized to
zero can assume nonzero values during training as illustrated in Figure 3.7. Sparsity in

SAO-Delta is controlled by removing entire kernels, corresponding to kernel pruning.

| | Baal eas - ji)

f

Esai L

Assign S[s, t] to W[1, 1, s, t] Vs, t € (c] x [c]

Figure 3.7: Applying SAO to a convolutional layer through Delta-Orthogonal
Initialization, resulting to the SAO-Delta method.

3.4.4.2 SAO-ECO

With the SAO-ECO Scheme, all convolutional layers are constructed with the sparse
orthogonal matrix, S € R°«t*%m assigned to the Fourier domain kemel P[p, q, :,:]. To
keep the Weonv real, Pi, j, :, :] := P[(k—7)%k, (k—j)%k, :, :] for each (i, 7) € [k] x [k]
[95]. Due to this constraint, only L unique orthogonal matrices are generated, where
L = (k?+1)/2if k is odd and L = (k?+-4)/2if k is even. Sparsity in SAO is controlled

by removing kernels. The procedure is summarized in Figure 3.8.

For layers where Cin < Cout, We Set Weony € IRkxkxcoutXcout then pad the input with
zeros in the channel dimension to match the number of output channels as in [95]. For
Cin > Cout. We perform the convolution with k x k x cin X Cin then only select the first
Cout Channels. For the final block with input 7% € R"~"%%", we apply dilation n/k
and cyclic padding d(k — 1)/2 on each side of the input as in [95]. When the stride
s is greater than 1, we reshape the input tensor into % € R**+*%»°" and perform the

convolution with Weony € RE***ens?Xcout with s = 1 [108; 95].


Figure 3.8: Applying SAO to a convolutional layer through Explicitly-Constructed
Orthogonal (ECO) convolutions, resulting to the SAO-ECO method.

SAO-Delta and SAO-ECO CNNs are CNNs with Delta-Orthogonal and ECO
weight initialization pruned using RG-S, LRP, LMP-T and LMP-S. The first
convolutional and linear layers are excluded from pruning due to the dimensionality

constraints of SAO.

3.5 Experimental Procedure

This section provides an overview of the experimental procedures, encompassing
the characterization of SAO, description of the datasets, architectures, and their
corresponding training schemes, and the pruning methods to which SAO was

compared to.

The models and techniques were developed through PyTorch, a Python-based deep
learning framework. The experimentation pipeline was streamlined by leveraging the
PyTorch Lightning high-level interface. To train the models, different GPUs were
employed, based on availability, including the NVIDIA Tesla T4, NVIDIA P100,
NVIDIA V100, and NVIDIA A100 from Google Colab.

3.5.1 Characterization of SAO networks

Prior the main experiments comparing SAO to pruning methods through their

implementation on various architectures which were trained on various datasets, these


networks were first characterized in terms of their structure, i.e., whether the
Ramanujan property is satisfied, and in terms of the singular value distribution, i.e.
whether exact or approximate dynamical isometry is achieved. This is to determine
whether the claimed characteristics of SAO, which are expected to bring advantages

over other pruning methods, are indeed present.

3.5.1.1 Determining the presence of the Ramanujan Property

The presence of the Ramanujan property was first determined through the
calculation of the eigenvalues of the adjacency matrix, derived from the mask matrix
of multiple layers with different width and degree. As discussed in Section 2.4.2, a
graph is Ramanujan if the eigenvalues of its adjacency matrix satisfy the condition
A2 < 2Vd—1, where 2g is the second highest eigenvalue of the adjacency matrix,
which can be generalized to \» < Yc—1+ /Vd—T1. To demonstrate whether the
presence of the Ramanujan property in extreme sparsity is unique to SAO, sparse

layers resulting from local magnitude and random pruning were also characterized.

3.5.1.2 Visualizing Input-Output Reachability of Sparse Layers

Sparsity in neural layers may restrict inputs from contributing to neural processing
in deeper layers. It could compromise neural network accuracy by excluding sections
of the input from local neural decision-making or by limiting the trainability of
networks by constraining the paths for gradient error propagation to individual
weights. We introduce a method to visualize the influence of sparse topologies on
connectivity of information and gradient error propagation called reachability. The
reachability matrix R;,; € R™*™ is acquired through (TE; M?) € Fp and we
binarize the matrix, M. When R;_,;[p, q] = 1, the p input node in L; is connected to
the q’* output node in L;. Through R, we can determine if all the inputs may reach
and contribute to decision-making at all of the output nodes. What the reachability
matrix shows is congruent to the idea of global connectivity introduced in [11], i.e., the
connectivity after collapsing the layers of a network. Figure 3.9 demonstrates the

concept of reachability.


0 101

0 101
x

1 010

1 010

M,"

Figure 3.9: Reachability matrix of two sparse layers based on 2-regular bipartite graphs.

To determine whether SAO networks possess global connectivity, the reachability
matrix of a S-layer MLP was first characterized. This was compared to one which was
pruned using local magnitude pruning to show whether global connectivity is distinct

to SAO or not.

3.5.1.3 Measuring the singular values of SAO layers

To determine whether SAO layers grant exact or approximate dynamical isometry,
the singular values of SAO MLP layers were calculated and then compared to a
dense-orthogonal layer and to sparse layers acquired through local magnitude and
random pruning. Four different layers were considered, one without activation, and the
other three with the sigmoidal activation functions Tanh and SiLU, and finally ReLU
to determine whether SAO can maintain exact or approximate dynamical isometry

with different activation functions.

The dense-orthogonal layer is considered as the baseline, i.e., the singular value
distribution of the sparse layers were compared to that of the dense-orthogonal layer.

Exact dynamical isometry is easily achieved through orthogonal initialization and easily


measured in layers with no nonlinearities, i.e., all the singular values are equal to 1. In
our case, since we are using different activation functions with no rescaling, the singular
values is expected to not be comprised entirely of 1 due to the activation function,
and so a good measure of approximate dynamical isometry in such layers would be
the spectrum of a dense-orthogonal layer, i.e., for a sparse layer to have approximate
dynamical isometry, it should have a similar spectrum to the dense-orthogonal layer.
Thus, we acquire a quantitative measure of the similarity of the distributions through the
p values from the Kolmogorov-Smimov test with the distribution of the dense network

as the baseline.

3.5.2 Datasets

In this study, four different image classification datasets were used, specifically:
MNIST, CIFAR-10, CIFAR-100, and CINIC-10. Each dataset offers varying levels of
complexity, making them suitable for testing the proposed methods on a range of deep
neural network architectures with different representation capabilities. Sample images

from each dataset are shown in Figure 3.10.

MNIST mB iat Sens
BuHe05on00 piss

cx filer Ae Rem er fe : rs

comme,
a: Bis ste td a
Blas a fe

out. a pe Bella Paich
CLebe see iRoy ee, 42ehB=snach

Figure 3.10: Sample images from the classification datasets used in the experiments.

The MNIST (Modified National Institute of Standards and Technology) dataset
consists of grayscale images handwritten digits, each measuring 28 x 28 pixels, with a
total of 60,000 training and 10,000 test images. The simplicity and small image size
makes it possible for plain MLPs to achieve practical accuracy in this task [109]. In


this work, the MNIST dataset is used to benchmark MLP models which are known to

have inferior performance than CNNs in image classification tasks [42; 43; 44].

The CIFAR-10 and CIFAR-100 datasets were compiled by the Canadian Institute
for Advanced Research. These datasets consist of colored images of various objects,
where CIFAR-10 has 10 classes comprised of ‘“‘airplane’, “automobile’, “bird’, “cat”,
“deer”, “dog”, “frog”, “horse”, “ship”, and “truck”, while CIFAR-100 is more diverse,
having 100 classes also consisted of various types of animals, plants, vehicles, among
others. Both datasets contain 50,000 training images and 10,000 test images, all with
size 32 x 32. CIFAR-10 has ten classes with 6,000 images each, while CIFAR-100 has
100 classes, containing 600 images each [110]. Being more complex than MNIST and
less complex than ImageNet, CIFAR datasets can be used for proof of concepts, i.e.,
testing out the potential of a new technique before further testing on a larger dataset.
CIFAR-10 was used to evaluate the CNNs with simple and relatively smaller
architectures, while CIFAR-100 was used to evaluate the larger CNNs. CIFAR-100
presents a greater challenge compared to CIFAR-10 due to its larger number of
classes, which in tur increases the number of decision boundaries that the model must
learn. Additionally, the reduced number of training samples for each class makes it
more difficult for the model to distinguish between different classes. Thus, this dataset

can be used to evaluate the generalization capabilities of a model.

CINIC-10 is a new dataset introduced to bridge the gap in complexity between
CIFAR and ImageNet, accomplished by sampling images from each dataset. It contains
270,000 32 x 32 color images equally distributed among the training, validation, and
test sets, making it 4.5x larger than CIFAR. The diversity in data sources results in a
more challenging dataset for image classification tasks and can thus be used to how a

model’s performance scales up on a much larger dataset [111].

3.5.3 Architectures

Two main types of deep neural networks were considered in the study: multilayer

perceptrons (MLPs) and convolutional neural networks (CNNs). MLPs are a


fundamental architecture used in deep learning, and have played a pivotal role in the
development of techniques in field. Thus, in our study, we first developed our
technique on MLPs. This step served as a foundation for our proposed technique,
allowing us to gain an understanding and measure its effectiveness on a
well-understood neural network framework. We then applied our technique to CNNs,
which are widely used architectures in computer vision. This was to see broadly our
technique could be applied, especially to more practical architectures. Additionally,
SAO was also applied to the MLP-based architecture called the Mixer to demonstrate

its potential in modern architectures.

The MLPs and CNNs can be divided into plain architectures, i.e., networks only
comprised of trainable weight layers and activation functions, MLP, Vanilla CNN, and
LipConvNet, and those which use normalization and/or residual connections, i.e.,
Mixers, ResNet, and VGG. Plain architectures have been shown to depend on
orthogonal initialization to maintain its trainability and converge into a practical
accuracy at very large depths. Therefore, to verify whether SAO grants the same
benefits as orthogonal initialization and to separate its effects from architectural
features, we implemented it on plain architectures. Then, we implemented it on
practical networks which utilize normalization and/or residual connections,
architectural features known for mitigating the gradients of networks during training,

to see whether SAO will still grant advantages beyond the benefits of its orthogonality.

For CNNs, there were two different SAO techniques: SAO-Delta and SAO-ECO.
SAO-Delta is based on the delta-orthogonal initialization, which was introduced in the
seminal paper on dynamical isometry in CNNs and is one of the earliest methods that
achieved truly orthogonal convolutions. Since this is a foundational orthogonal
initialization method and relatively easy to implement, it motivates us to first
implement SAO through this approach. Meanwhile, SAO-ECO is based on
explicitly-constructed orthogonal convolutions (ECO). Our motivation is to
demonstrate SAO’s applicability through various methods, and we chose ECO
specifically as it is a relatively new approach that has shown computational advantages

over the previous orthogonal initialization methods.


The architectures or models used are summarized in Figure 3.11. The training
schemes and the layers of each model are discussed in the following subsections. To
describe the layers, the following conventions are used: linear layers are denoted by
linear[Nin, Nout], Where Nin, Nout are the input and output nodes, while convolutional
layers are denoted by conv[Cout, k x k, 5], where Cour, k x k, s are the output channels,

kernel size, and stride, respectively.

| Implementation of SAO on
| deep neural networks

Multilayer Perceptrons Convolutional neural networks
MLP
|
| Test on plain networks Vanilla CNN Vanilla CNN
Uae]
ixers Test on plain networks Test on plain networks

Test influence of residual
connections and layer normalization ResNet LipConvnet

Test influence of residual

Test on plain networks
fonnections and batch normalization| é

VGG

A

|Test influence of batch normalization|

ResNet

Test influence of residual
jconnections and batch normalization)

Figure 3.11: Deep neural network architectures used in the experiments.

3.5.3.1 MLP-N

SAO was benchmarked on vanilla multilayer perceptrons (MLPs) with varying
number of hidden layers, denoted by the suffix N. As a preliminary, SAO was
implemented on a 7-layer MLP as in [13; 14]. MLP-7 was trained on the MNIST
dataset for 90 epochs with Stochastic Gradient Descent (SGD) with an initial learning
rate of 0.01 decayed by 0.1 at the 30** and 60%" epoch and weight decay of 5 x 107+.
We tested linear, ReLU, and Tanh variants of the model. The performance of SAO was
compared with the unpruned (dense) model, GMP-S, and GMP-T. GMP-T js acquired

from training an unpruned MLP-7, pruning it, and then finetuning, such that it


undergoes a total of 180 training epochs. The architecture of MLP-7 is described in
Table 3.3, where the layers are listed in the sequence in which the input data is passed

through the network.

Table 3.3: Architecture of the MLP-7.

Neural Layer Repeats

1 linear[1024, 100] 1
2-6  linear{100, 100) 5

7 linear[100, 10] 1

MLPs with various depths, ranging from N = 50 to N = 1000, were trained on
MNIST with SGD (momentum = 0.9, weight decay = 10-4) with the Tanh activation
function for 100 epochs. The weights of the networks were scaled by a factor of 1.1
as in [15]. The architecture of MLP-N is summarized in Table 3.4. Each trainable
layer is interleaved with the activation function. The learning rates used during training
are listed in Table 3.5, and were chosen to encourage the sparse network to train to

convergence.

Table 3.4: Architecture summary of MLP-N with variable number of layers N.

Neural Layer Repeats
Input Layer _ linear[1024, 512] 1
Downscaling _ linear[512, 256] 1
Symmetric linear[256, 256] N-—1

Output Layer _linear[{256, 10] 1


Table 3.5: Learning rates used for different MLP-N depth.

Depth (N Layers) 60 200 400 600 800 1000

Learning Rate 107? 107? 10°% 10-4 10-4 1074

When applying SAO, Ramanujan constructions, or pruning to deep MLP-N, the
final layer with dimensions 256 x 10 is excluded due to the dimensionality constraints
set by SAO. Meanwhile, for MLP-7, all layers were included to reflect the experiments
of (13; 14]. Notably the first layer with dimensions 1024 x 100 violates the constraint
of SAO. As a workaround, a 100 x 1100 (10, 110)-regular matrix is first constructed
and then cropped (from the right) to match the dimensions of the first layer. This leads
to a set of identical rows which do not match the number of nonzeros of other sets, and
so the length of the orthogonal vectors to be assigned to these rows will be different
from the other sets. This workaround leads to sparse and orthogonal matrices, but are

not (c, d)-regular.

3.5.3.2 Mixers

The MLP-Mixers used in the study, Mixer-Nano [112] and ResMLP [113], were
trained on the CIFAR-10 dataset for 300 epochs using the Adam optimizer with 6; =
0.9 and 82 = 0.99, Cosine learning rate scheduler with warmup for 5 epochs, with an
initial and final learning rate of 10-? and 10~§, respectively. Label smoothing = 0.1
was also applied. For data augmentation, CIFAR-10 AutoAugment [114] was used

combined with CutMix [115] with p = 0.5. Both models utilize residual connections.

The Mixer-Nano is consisted of two types of modules, corresponding to the token-
mixing and channel-mixing layers, which are comprised of layer normalization, linear
(token-mixing) or 1D convolution (channel-mixing), dropout, and GeLU layers. If a
depth of 8 is specified, there will be 8 interleaving token-mixing and channel-mixing
modules, and a final linear classification layer. In the experiment, depth = 8, 16 were
used, corresponding to 33 and 65 trainable layers, respectively. The models have patch

size = 4, hidden layer width = 128, channel-mixing width = 512, and token-mixing


channel width = 64. The modules are summarized in Table 3.6, where the layers are

listed in the sequence in which the input data is passed through the network.
Table 3.6: Architecture of MLP-Mixer-Nano for CIFAR-10.

Input Size Output Size Neural Layer Repeats

“Patch Embedding 32x32 64x 128 conv[128,3x3,1) 1
Mixer Layer 64x 128 64128  conv[64,1x 1,1] Depth
conv[64, 1 x 1, 1]
linear[128, 512]
linear[512, 128]
Classifier 128 10 linear[128, 10] 1

The ResMLP is consisted of modules containing an affine transformation layer,
three linear layers, dropout, and the GeLU activation function. A depth of 12 was used,
corresponding to the number of modules. This is equivalent to 38 trainable layers,
including the input Conv2D layer and the output linear classification layer. The
networks used had patch size = 4 and base width = 48. The structure of the module is

summarized in Table 3.7:
Table 3.7: Architecture of ResMLP for CIFAR-10.

Input Size Output Size Neural Layer Repeats

Patch Embedding 32 x 32 32 x 32 conv[48, 4 x 4, 4] 1
MLP Block 64 x 512 32 x 32 linear[64, 64] 12
64 x 512 32 x 32 linear[48, 192]
64 x 512 32 x 32 linear[ 192, 48]
Classifier 48 10 linear[48, 10] 1

When applying SAO, Ramanujan constructions, or pruning to the MLP-Mixer and
ResMLP, the final layers, whose weights have dimension 10 x 128 and 10 x 48, are
excluded and maintained as dense. This is due to the dimensionality constraints set by

SAO, as mentioned in Section 3.4.1.


3.5.3.3 Vanilla CNN

We implemented a vanilla convolutional neural network which does not use any
special layers or techniques such as batch normalization, residual connections and
dropout to assist the training of the network and only consists of convolutional layers
interleaved with either Tanh or ReLU activation function. All the layers use a kernel
size of 3 x 3, zero-padding, and stride equal to 1, except for the second convolutional
layer with stride = 2. Before the linear output layer, an average pooling layer is used.
We trained models with N = 32,128,256,512, and 768 convolutional layers on
CIFAR-10 with SGD (momentum = 0.9, weight decay = 10~*) and without weight
rescaling. The architecture is summarized in Table 3.9, where the layers are listed in
the sequence in which the input data is passed through the network. At a specific
network depth, we experimented with layers with widths of W = 32,64, 128 and 256.
The initial learning rates are listed in Table 3.8 and are decayed by a factor of 0.1 at the
50‘* and 150" epochs. A learning rate warm-up (10 epochs) was used at N = 768 for
ReLU and N = 512 and above for Tanh with Cosine Annealing. Deep vanilla CNNs
are trainable to convergence only with orthogonal weight initialization, described in

Section III.B above.
Table 3.8: Learning rates of Vanilla CNN for different NV.
Depth (N Layers) 32 128 256 512 768

Learning Rate 10-? 10-3 10-% 10-4 10-4

For CINIC-10 and CIFAR-100, the average pooling layer before the classifier was
replaced with two convolutional layers with stride = 2. The same hyperparameters
were used, except for the adoption of the Cosine Annealing scheduler with warm-up
for 1 epoch for all values of N. A selection from networks with NV = 8, 16,32 and
128 were trained on CINIC-10, depending on the orthogonalization technique, with the
same learning rates as Table 3.8 and 10~? for N < 32. For CIFAR-100, a selection
from networks with N = 8, 16,32 were trained with the same learning rates. Only

ReLU was used for these datasets.


Table 3.9: Architecture of a vanilla CNN with variable number of layers N and width
W for CIFAR-10 and CINIC-10.

Neural Layer Repeats

Input Layer —conv[W, 3x3, 1] 1
Second Layer _conv([W, 3x3, 2] 1

Symmetric conv[W, 3x3, 1] N-1
Output Layer linear[W, 10/ 100] 1

When applying SAO, Ramanujan constructions, or pruning to this network and the
other CNNs that follow, i.e., LipConvNet-N, VGG, and ResNet, the first and final
layers are excluded. Similar to the previous networks, this exclusion is due to the
dimensionality constraints set by SAO, as mentioned in Section 3.4.1. Input layers of
CNNs for RGB datasets typically have dimensions of Cour X 3 x k x k, where Cout iS a
power of 2, e.g., 16, 32, and 64. Furthermore, the input layer performs the initial
feature extraction from the raw data, and therefor, pruning this layer can potentially do
the most harm to the network’s performance. Typically, the input layer contributes the
least amount of parameters among all the convolutional layer, while pruning it leads to
the most accuracy degradation, and so it is recommended to leave the layer as-is [116].
Meanwhile, the output layer of this network has dimensions W x 10 or W x 100,
depending on the dataset, which is similar to the succeeding CNNs. The succeeding
CNNs have output layers with input dimensions determined by powers of 2, followed
by the number of classes, which are either 10 or 100. These final classification layers
do not significantly contribute to the number of parameters and can thus be left

unaltered.

3.5.3.4 LipConvNet-N

We adopt the 1-Lipschitz compliant network from [95] to apply an alternative
weight orthogonalization method for CNNs. The complete network comprises of five
blocks. Each block consists of a single-strided convolutional layer repeated (NV/5 — 1)
times followed by a convolutional layer with stride, s # 1 to down-sample the feature
space. The Max-Min activation function is applied after every convolutional layer. The

first two blocks are initialized with Skew-Orthogonal Convolutions (SOC) from [94]


while the last three blocks are initialized with Explicitly-Constructed Orthogonal
Initialization (ECO) - see Section III.C.S below. This construction yields better
performance than purely ECO initialized layers as shown by [95] as dilated
convolutions early on leads to deterioration of recognition capabilities. We trained the
LipConvNet on CIFAR-10 with Stochastic Gradient Descent for 200 epochs with a
learning rate of 0.01 which is decayed by a factor of 0.1 in the 50** and 150** epochs,
and weight decay 5 x 10°‘. The architecture of LipConvNet-N is summarized in Table
3.10, where the layers are listed in the sequence in which the input data is passed

through the network.

Table 3.10: Architecture of LipConvNet-N parameterized by depth N.

Input Size Output Size Neural Layer Repeats

Block 1 24 x 24 12 x 12 conv[32,3 x 3,1] (N/5—- 1)
conv[64, 3 x 3, 2] 1

Block2 12x 12 6x6 conv[64, 3 x 3,1] (N/5— 1)
conv[128, 3 x 3, 2] 1

Block3 6x6 3x3 conv[128, 3 x 3,1] (N/5— 1)
conv[256, 3 x 3, 2] 1

Block 4 3x3 1x1 conv[256, 3 x 3,1] (N/5—-1)
conv[512, 3 x 3, 3] 1

Block 5 1x1 1x1 conv[512,1x 1,1] (N/5—-1)
conv[1024, 1 x 1, 1] 1

3.5.3.5 VGG

The VGG-16 model with batch normalization was trained on CIFAR-10. The
models were trained for 200 epochs using SGD with momentum = 0.9 and weight
decay = 1074. The initial learning rate used is 0.1 which was reduced by a factor of 0.2
on the 60°" and 120°" epoch. When pruning, the first convolutional layer and the last
fully-connected layer are not included. When implementing SAO-ECO (discussed
below), only the last 6 convolutional layers use the prescribed cycling padding and
dilation. The architecture is summarized in Table 3.11, where the layers are listed in

the sequence in which the input data is passed through the network.


Table 3.11: Architecture of VGG-16.

Input Size Output Size Neural Layer Repeats
Conv. 1 36 x 36 18 x 18 conv[64, 3x3, 1] 2

Conv.2 18x 18 9x9 conv[128, 3x3, 1] 2
Conv. 3 9x9 9x9 conv[256, 3x3, 1] 3
Conv. 4 9x9 3x3 conv[512, 3x3, 1] 3
Conv. 5 3x3 1x1 conv[512, 3x3, 1] 3
Linear 1 512 256 linear[512, 256] 1
Linear 2 256 128 linear[256, 128] 1
Linear 3 128 10 linear[128, 10] 1

3.5.3.6 ResNet

We use the ResNet implementation of [117] for CIFAR-10, the architecture
summarized in Table 3.12, where the layers are listed in the sequence in which the
input data is passed through the network. The input data is fed into Input Layer. The
models were trained for 200 epochs using SGD with momentum = 0.9 and weight
decay = 5 x 10~4. The initial learning rate used is 0.1 decayed by 0.1 in the 100" and
150" epochs. For CIFAR-100, we used the ResNet18 implementation of [21]
summarized in 3.13 and followed the hyperparameter settings of [118], where the
initial learning rate is 0.1, decayed by 0.2 in the 60**, 120**, and 160** epochs, trained
with SGD with Nesterov momentum = 0.9, and weight decay = 5 x 1074. When
pruning, the first convolutional layer and the linear layer are not included. For
SAO-ECO (discussed below), the input size is 36 x 36, all of the layers are initialized
with the initialization method of SAO-ECO, but only the last block uses the prescribed
cyclic padding and dilation.


3.5.4

We benchmark the performance of SAO with several variants of pruning, namely:

global magnitude pruning (GMP), local magnitude pruning (LMP), and local random

Table 3.12: Architecture of ResNet-N for CIFAR-10.

Input Size Output Size Neural Layer Repeats

Input Layer 32 x 32 32 x 32 conv[16, 3 x 3, 1] 1

Layer 1 32 x 32 32 x 32. — conv[16, 3 x 3, 1] N/3
Layer 2 32 x 32 16 x 16 conv[16, 3 x 3, 1] 1
conv[32, 3 x 3,2] N/3-1
Layer 3 16 x 16 8x8 conv[32, 3 x 3, 1] 1
conv[64, 3 x 3,3] N/3-—1
Classifier 64 10 linear[64, 10] 1

Table 3.13: Architecture of ResNet-18 for CIFAR-100.

Input Size Output Size Neural Layer Repeats

Input Layer 32 x 32 32 x 32 conv[64, 3 x 3, 1] 1
Layer 1 32 x 32 32 x 32 conv[64, 3 x 3, 1] 4
Layer 2 32 x 32 16 x 16 conv[64, 3 x 3, 1] 1

conv[128, 3 x 3, 2] 3

Layer 3 16 x 16 8x8 conv[128, 3 x 3, 1] 1

conv[256, 3 x 3, 3] 3

Layer 4 8x8 4x4 conv[256, 3 x 3, 1] 1

conv[512, 3 x 3, 3] 3

Classifier 64 100 linear[64, 100] 1

Comparing SAO to pruning methods and Ramanujan constructions

pruning (LRP). In multilayer perceptrons, individual weights are pruned.

convolutional neural networks,
Figure 3.12 illustrates the process of creating sparse neural networks with weight
pruning applied after training a dense network, indicated by a suffix “T”, e.g. LMP-T
indicates performing local magnitude pruning on a trained dense network which is
then finetuned for the same duration as in training. The suffix “S” implies that upon

initialization, the model is immediately pruned without training, e.g. GMP-S implies


two-dimensional kernels are removed en bloc.

global magnitude pruning performed on initialized weights prior to network training,
such that the pruned model undergoes only half the total training epochs of GMP-T
from initialization. Pruning was performed after orthogonal initialization but prior to

network training in the spirit of the Lottery Ticket Hypothesis [84].

These comparisons are necessary to see whether SAO will present any advantages
over existing methods, either applied from scratch or after training. One of the
motivations of this study is to circumvent the dense pretraining requirement which
contributes to additional costs through SAO. For SAO to have value, it should, at the
minimum, perform better than standard pruning methods from scratch which also do
not incur additional costs from dense pretraining. SAO was also compared to these
pruning methods applied after training, as they are intended to be implemented, which
has double the training time of SAO. This is to see whether SAO can in fact serve as a
replacement for these methods showing its potential to reduce the total training costs

usually required to uncover a sparse neural network.

Orthogonal Train Finetune
Initialization

Figure 3.12: Standard steps for model pruning.

Since SAO is comprised of its Ramanujan graph structure and its orthogonal
weights, we wanted to determine whether its effects and potential benefits can be
attributed to just its structure, the unique way of weight assignments, i.e., sparse
structure is constructed then the weights are assigned per set of identical rows, or if
both its structure and orthogonality are important in its performance. We implemented
Ramanujan pruning from scratch (RG-S) and after training (RG-T) which uses the
same sparse structure as SAO, but used as a pruning method, i.e., a dense model is
initialized (then trained for RG-T) and then pruned using a mask with a Ramanujan
graph structure. If SAO presents no advantage over these methods, then the strength of
SAO comes from its Ramanujan graph structure. To test whether the benefits of SAO
comes from its unique weight assignments, we followed the same procedure as SAO,

only that the weights are not orthogonal but simply have normal and uniform


distributions, denoted by RG-N and RG-U, respectively. All of the techniques which

were compared to SAO are shown in Figure 3.13.

f
Comparison of SAO with other |
| pruning methods

| Standard pruning methods Construction methods
cms ——=——
il | Global magnitude pruning (GMP) Ramanujan a construction (RG)
[Layerwise magnitude pruning (LMP) Scratch : [ Traine
| Layerwise random pruning (LRP) [Normal | aoe
L

Scratch | Trained

Figure 3.13: Different pruning methods and Ramanujan construction techniques
implemented for SAO benchmarks.

3.5.5 Resource Reallocation

To determine whether sparse-deep can outperform dense shallow neural networks
for the same parameter budget, we compared the performance of a dense 4-layer MLP
to SAO MLP, with N = [25, 50, 100, 200] on the CIFAR-10 dataset, where the degree
for each MLP is the degree that will give the network the closest number of nonzero
parameters to that of the dense 4-layer network. The models were trained on the CIFAR-
10 dataset with the SGD optimizer with momentum = 0.9 and weight decay = 5 x 10-4
for 200 epochs. Different initial learning rates were used for different number of layers,
which was decayed by 0.1 at the 60%, 120**, and 160" epoch. Besides performing
reallocation with a plain SAO MLP structure, this was also accomplished with an SAO

MLP architecture leveraging residual connections or skip connections.

3.6 Summary

This chapter includes a discussion on the existing methodologies we adopt. We

briefly discussed the datasets used, i.e, MNIST, CIFAR-10, CIFAR-100, and


CINIC-10 image classification datasets. Then, we listed the deep neural network
architectures considered and their corresponding training schemes, with the two main
types including multilayer perceptrons (MLP) and convolutional neural networks
(CNNs). The architectures can be grouped as either plain, i.e. comprised only of
weight layers and activation, or those which utilize specialized architectural features
like residual connections and normalization layers. Then, we discussed dynamical
isometry, Ramanujan property, and how orthogonality is achieved in MLP layers, i-e.,
by orthogonalizing the weight matrix, and in convolutional layers, ie., by
orthogonalizing the DBT matrix. Two orthogonal initialization methods for CNNS
were introduced: the Delta-Orthogonal Initialization and Explicitly-Constructed
Orthogonal Convolutions. Then, the proposed method, Sparsity-Aware Orthogonal
Initialization (SAO) was discussed starting from the construction of its Ramanujan
construction, the orthogonalization of the sparse structure, and its straightforward
implementation on MLPs, and then on CNNs through the two orthogonal convolution
initialization methods resulting to SAO-Delta and SAO-ECO. The resource
reallocation experiment leveraging SAO was also discussed for MLPs, wherein two
types of SAO MLPs were used: plain MLPs and MLPs with residual or skip
connections. This chapter also introduced the reachability matrix which can be used to
visualize the global connectivity of a sparse neural network. Finally, the standard
pruning methods and alternative Ramanujan graph constructions against which SAO is
compared were discussed. The experiments for SAO benchmarking are summarized in
Table 3.14, where e denotes the number of training epochs, o the optimizer, lrg the
initial learning rate, Ir the final learning rate, l2 the weight decay, s the learning rate
scheduler, m the milestones, and the learning rate decay. The models employing

variable learning rates based on N are denoted as f (JV).


Table 3.14: Summary of Experiments (Benchmarks)

Method Architecture Training Scheme Methods Applied
+ 90, o: SGD, Iro: 0.01, la: Se-4
1 | sao MLP-7 MNIST CARTE BUS SAO, GMP-S, GMP-T
s: multistep, m: [30, 60], 7: 0.1,
SAO, GMP-S, GMP-T,
2 | sao MLP.N €: 100, 0: SGD, Iro: f(N), 2: 1e-4 | LMP-S, LMP-T,RG-S,
RG-T, RG-U, RG-N
: 200, 0: SGD, tro: f(N), la: 5e-4
MLP.N oe aaa
3 | sao CIFAR-10 | s: multistep, m: (60, 120, 160],

Resource Reallocation

7: 0.1

SAO-Delta

SAO-Delta

SAO-Delta

e: 300, o: Adam, Iro: le-3, Ir: 1e-6,
1g: Se-4, s: cosine
e: 300, o: Adam, Ir: 1e-3, lr: 1e-6,
lg: Se-4, s: cosine
e: 200, o: SGD, Iro: var, Iz: 1e-4
s: multistep, m: (50, 150), -y: 0.1
e: 200, o: SGD, Iro: var, lz: 1le-4
s: multistep, m: (50, 150], -y: 0.1

e: 200, o: SGD, iro: 0.1, Lz: Se-4,
s: multistep, m: [100, 150], -: 0.1

SAO, LMP-S, RG-S

Vanilla CNN CIFAR-10

Vanilla CNN CINIC-10 SAO, LMP-S, LMP-T

ResNet-20, ResNet-32,
ResNet-44, ResNet-56

SAO, LMP-S, RG-S

SAO-Delta

SAO-ECO

€: 200, o: SGD, iro: 0.1, L2: Se-4,
8: multistep, m: [60, 120, 160),

4: 0.2
e: 200, o: SGD, Iro: 0.01, lo: Se-4
s: multistep, m: (50, 150), -y: 0.1

e: 200, o: SGD, iro: var, la: le-4
Vanilla CNN CIFAR-10 oe
s: multistep, m: (50, 150], -y: 0.1
¢: 200, o: SGD, Irg: 0.01, l2: 1e-4
SAO, LMP-S, RG-S,
VGG CIFAR-10 | s: multistep, m: (60, 120],
RG-T
7: 0.2

ResNet-18

CIFAR-100

LipConvNet-N

ResNet-20 ¢: 200, o: SGD, Iro: 0.1, 12: Se-4 SAO, LMP-S, LMP-T,
CIFAR-10
ResNet-32

8: multistep, m: (100, 150], y: 0.1 RG-S, RG-T
oe ee

¢: 200, o: SGD, Iro: 0.1, la: Se-4
s: multistep, m: [60, 120, 160],
7: 0.2

SAO, LMP-S


CHAPTER 4

RESULTS AND DISCUSSION

This chapter first presents an analysis of SAO layers in terms of the Ramanujan property
and dynamical isometry. Then, the performance of SAO was evaluated on various types

of multilayer perceptrons (MLP) and convolutional neural networks (CNN).

4.1 Properties of Sparsity-Aware Orthogonal Initialization (SAO)
4.1.1 Ramanujan Property

Expander graphs are a type of graph topology with maximally connected and
sparse nodes. Graphs possess the expander property if the topology fulfills the edge
expansion, vertex expansion, or spectral expansion criteria. Generating expander graph
topologies are more challenging, but several construction algorithms are known for
selected expander graph families. Since expander graphs are maximally connected, it
is reasoned that this type of topology would not unduly impede the forward and
backward propagation of gradients between neurons while the sparsity property

achieves the fewest weights to connect layers.

Ramanujan graphs are a family of expander graphs with maximally sparse
connectivity and are described as the optimal expanders [88]. It is expected that
Ramanujan topology would consist of the fewest edges (and hence the fewest number
of weights in a neural layer) as compared to other expander graph families. Ramanujan
graphs satisfy the spectral expansion condition Az < 2A; — I where A, is the largest

eigenvalue of the binary adjancency matrix and 2 the second largest eigenvalue [119].


The topology of each MLP layer, in the MLP-N and Mixer architectures as
described in Sections 3.5.3.1 and 3.5.3.2, was designed to be a bipartite Ramanujan
graph, achieved by constructing M as a (c,d)-regular block matrix with identical
blocks as discussed in Section 3.4.1. To show that the construction results in bipartite

Ramanujan graphs, the following proof is provided:

Definition 1 The eigenvalues of the adjacency matrix A comes in pairs \ and —X

where |A| is a singular value of the biadjacency matrix B [87].

From Definition 1, we can calculate the eigenvalues, and therefor determine whether
a certain graph satisfies the Ramanujan property through the singular values of B, which
in our case can be calculated through the singular values of M, equal to that of B, being
its transpose. Thus, in this proof, we take interest in the singular values of M as its

structure is what we directly control.

Conjecture 1 The rows and columns of M can be rearranged to a block diagonal

matrix Myq comprised of blocks with dimension c x d whose entries are all equal to 1.

Conjecture 1 is demonstrated in Figure 4.1 for d-regular matrices, which can both
be represented as M = [Mj; M2], where M; = Mp. The matrices can be permuted
to form a block diagonal matrix through elementary matrix operations, i.e., row and

column interchanges. Such operations do not affect the spectrum of matrices.


123 4 13 2 4

1 1 1
2 _3 3
3 2 2
4| i | 4 4
M M In Pg Co > Cy
123 4 123 4 1432
1 1 1
2 __3 3
3 2 2
4 4 4
M lp Pg Co Cy

Figure 4.1: A block diagonal matrix can be formed from M through row and column
interchanges.

Definition 2 The singular values of a block diagonal matrix is given by the union of

the singular values of the blocks on its diagonal [120].

Due to Definition 2, representing the matrix M in its block diagonal form allows
ease of analysis of the singular values, especially as the blocks comprising its diagonal
are the same, as described in Conjecture 1. Investigating one block is then enough
to know about the singular values of every other block, and therefor the entire block
diagonal matrix.

Definition 3 Rank is the length of the longest linearly independent list of columns of a
matrix [120].

Definition 4 The number of non-zero singular values of a matrix is determined by its
rank [121]

Definition 5 The Frobenius norm of a matrix is equal to the Lz norm of its singular


values [122].

From Definition 3, we can determine that the rank of each block is equal to 1, as
all the entries are the same, and consequently all of the columns are linearly dependent.
Definition 4 then tells us that the blocks of Myg each have only one non-zero singular
value. The lone non-zero singular value can be acquired through Definition 5. The L2
norm of the singular values is given by VX 0, and as there is only one non-zero
singular value, simplifies to Vo? = 0;. Meanwhile, the Frobenius norm of the blocks
given by ,/ 507" 305 |ziy|? is equal to Vcd, as it has dimension c x d where all the entries
are 1. We then know that the singular values of M are comprised only of Ved and zero,
and that the eigenvalues of A are comprised of Vcd, — Vcd, and 0 which guarantees
that A» < 2X; — 1 is always satisfied as 2 is always zero.

In Figure 4.2, we investigated the Ramanujan property of individual MLP layers
with different width and sparsity, constructed using RG (also the sparse structure used in
SAO), LMP-S, and LRP-S. As magnitude and random pruning produce sparse bipartite
graphs that are not d-regular, we used the definition of the Ramanujan gap from [25]:
2/1 — I — Xo, where , is the largest eigenvalue of the adjacency matrix. For RG,
the Ramanujan gap remained above zero for all the layers. Meanwhile, for magnitude
and random pruning, some of the layers have lost the Ramanujan property at 96.875%
sparsity, while at 98.4375% sparsity, none of the layers are Ramanujan. These two
pruning methods also exhibit a more rapid decrease in Ramanujan gap with the sparsity,

especially in the wider layers.

Figure 4.3 shows the eigenvalue spectra of layers with different width, denoted by
each curve, with RG construction. The spectra of the layers are consisted only of three
values: the trivial eigenvalues d and —d and the non-trivial eigenvalues equal to zero.
The largest eigenvalue \; is then given by d while the second largest eigenvalue 2 is
zero, which guarantees that the condition for the Ramanujan property \2 < 2/2, —1
is always satisfied with RG, and consequently, with SAO which uses RG as its sparse

structure.

We expect this structure to have advantages over standard pruning methods. First, as


Local Magnitude Pruning Local Random Pruning Ramanujan Graph Construction

128 512 128 512 \ 128 512
256 1024 256 —~— 1024 ~~ 256 —— 1024

Janujan gap

Rar

Sparsity (%) ° “ Sparsity (%) Sparsity (%)

Figure 4.2: Ramanujan gap vs. sparsity of an MLP layer that was pruned and one that
was sparsified through Ramanujan construction.

we use d-regular graphs, all input nodes have a path towards the output nodes of a layer
and all output nodes are receiving signals from the input nodes. This assures that there
won’t be any dead neurons in the neural network, i.e. neurons which do not contribute
to the output of the model. Second, as the d-regular graphs we use are also expander
graphs, we assure that every input node is sensed by every output node, thus providing
global connectivity as proved in [11] in their work on expander neural networks. These
two benefits point to the maximization of the connectivity which is one of the factors
affecting information flow in the network during training [11; 123]. Furthermore, as
we use Ramanujan expanders, which are described as the best possible expanders, we
are maximizing the benefits of expander properties in our construction of sparse neural

networks.

The benefits of having Ramanujan layers are not limited to the connectivity within
each layer. [11] showed that using expander graphs for each layer can assure global
connectivity for a given number of layers. They defined global connectivity as having
all the inputs have a path towards the outputs. In our work, we refer to this quality as
reachability. We illustrate this in Figure 4.4, where we compare the reachability of a
sparse MLP-5 uncovered through local magnitude pruning and one which was

constructed using Ramanujan graphs. The figure labeled N = 1 is the transpose of the


Ramanujan Graph Construction (Sparsity = 98.4375%)
Oe ee ) 400)

Gam W= 128 @mm Ww = 256
120)

100 300


60,
100)
t) 0 - r

c
ou =2.0 -15 -10 65 0.0 os to ts 2.0 -4 -3 2 -1 ° 1 2 3
2° swe) 2000)
z mmm W=512 mmm Ww = 1024
800) 1750
| |
bees | 1250 |
1000) |
400)
750)
|

200)

|
Oa = a a z a 6 o—=15 =i0 = ° 3 10 15

Eigenvalues of the adjacency matrix

Figure 4.3: Adjacency matrix eigenvalue spectra of an MLP layer with Ramanujan
graph structure of different width.


mask of a single 256 x 256 layer, where the coloured pixels indicate the presence (1)
or absence (0) of a connection between the input nodes (rows) and the output nodes
(columns) of this specific layer. While the connections in magnitude pruning are
sporadic, a pattern can be seen in the Ramanujan construction, coming from the
d-regularity and in a characteristic specific to our construction, where the second half
of the graph is identical to the first half. The figure N = 3 shows the connections
between the input nodes of the first layer N = 1, to the output nodes of the third layer.
At this point, the input nodes are able to reach more output nodes, in contrast to
N = 1, where for the Ramanujan construction, each node can only reach four output
nodes. In this case, the Ramanujan construction has a sporadic structure that is
relatively less sparse, while some white streaks are starting to emerge in magnitude
pruning. At NV = 5, RG was able to achieve full connectivity by, i.e. all the input nodes
reach the output nodes, in contrast to magnitude pruning where some input nodes are
disconnected from some output nodes of the Sth layer where the presence of white
streaks have solidified. The connectivity in RG is carried over in deeper layers, which
agrees with the report of [11], thus proving that Ramanujan networks are able to
achieve full connectivity despite the sparsity.

N=1 ; N=3__

Local Magnitude Pruning

Ramanujan Construction

Figure 4.4: Reachability matrix of sparse neural networks obtained through local
magnitude pruning and constructed using Ramanujan graphs.


For convolutional neural networks such as in Section 3.5.3.3, 3.5.3.4, 3.5.3.5, and
3.5.3.6, the Ramanujan structure is exhibited at the kernel level, i-e., each entry in the
Ramanujan matrix corresponds to the values of each kernel with size k x k. This
structure also satisfies the Ramanujan property in the same way as the MLPs from a
kernel-level perspective, while in a weight-level perspective, the non-trivial
eigenvalues of the weight tensor, when flattened to be 2-dimensional, would be equal
to +kVcd joined by its trivial eigenvalues equal to zero. This assures that
convolutional layers also satisfy the Ramanujan property, also bringing benefits such

as local and global connectivity.

4.1.2 Dynamical Isometry

Linear multilayer perceptrons may attain dynamical isometry with orthogonally
initialized weights [15]. All singular values of the weight Jacobian matrix are unity,
and there are no dynamical modes that cause vanishing or exploding gradients during
model training. Nonlinear activations compromise dynamical isometry [58] by
introducing singular values that are both greater and less than unity, which engenders
dynamical modes capable of propagating exploding and vanishing gradients,
respectively, through deep networks. Sigmoidal activation functions may attain
approximate dynamical isometry with the majority of singular values being arbitrarily
close to unity [15]. ReLU did not achieve dynamical isometry due to the sizeable
presence of zero-valued singular values which engender vanishing gradients, in

contrast to its smoothed variant, SiLU [58].

We compared the singular value distribution of the Jacobian of an orthogonally
initialized single MLP layer pruned through standard pruning methods, LMP and LRP
and those which were constructed using Ramanujan graphs RG and SAO at 99.2%
sparsity, to that of the dense-orthogonal network without and with nonlinear
activations, i.e. Tanh, ReLU, and SiLU. Figure 4.5 shows the singular value
distribution of the Jacobian of the dense-orthogonal network and that of SAO. For the
linear network, the singular values of both the dense network and SAO are all equal to

1, indicating dynamical isometry. When an activation function is introduced, the


dynamical isometry is lost for both networks. The Tanh and SiLU networks have a
skewed distribution on both dense and SAO networks, with Tanh towards 1 and SiLU
towards 0. The Tanh network has what is called approximate dynamical isometry,
where the singular values are arbitrarily close to 1 and there are very few extremely
small and large (none in our case) singular values [15], which is expected behaviour
from sigmoidal activation functions. Meanwhile, SiLU which is also a sigmoidal
activation, has more singular values equal to 0 than 1. Even then, it maintained values
that are arbitrarily close to 1 which indicates that dynamical isometry is not entirely
destroyed. For ReLU, the singular values become either 1 or a value that is very close
to zero, demonstrating the incapability of ReLU networks to achieve dynamical

isometry through simple orthogonal initialization due to its induced sparsity [58].



200}

150)




_ Singular value distribution (Linear) _

Dense ™™) SAO (S = 99.2%)


Singular value distribution (ReLU) _

Dense

06 08 1.0

77 SAO (S = 99.2%)


Singular value distribution (Tanh)
Dense mH SAO (S = 99.2%)

Dense

08 10

02 3) 06
Singular value distribution (SiLU) |

“9 SAO (S = 99.2%)

Figure 4.5: Comparison of SV distribution of a dense orthogonal MLP layer and SAO
construction.

Figure 4.6 shows the distributions for the dense-orthogonal network and that of

LMP, LRP, and RG. For the linear network, the sparse networks have already lost

dynamical isometry despite the orthogonal initialization, having distributions which

are skewed to the left. This pattern persists for all nonlinearities, even in the sigmoidal

activation functions Tanh and SiLU, where the networks were not able to achieve at

least approximate dynamical isometry. Thus, we show that achieving a similar

distribution to the dense-orthogonal network is a feat exclusive to SAO networks.


Singular value distribution (Linear) Singular value distribution (Tanh)

Dense ump ; Dense ump
RG LRP RG LRP
|
|
eae |
ld |
2 04 é 08 10 5.0 2 0.4 06 0.8
Singular value distribution (ReLU) Singular value distribution (SiLU)
Dense LMP 25 Dense LMP
RG LRP RG LRP

O60. 02 04 6 08 10 0.0 0.2 0.4 06 08 10

Figure 4.6: Comparison of the singular value distribution of the Jacobian matrix of an
MLP layer after orthogonal initialization, while dense and after pruning, without and
with various non-linear activation functions.

To quantify the degree of similarity of the distributions to that of the dense
network, we calculate p values from the Kolmogorov-Smirnov test with the
distribution of the dense network as the baseline which is tabulated in Table 4.1. Only
the SAO network was able to achieve some degree of similarity to the dense network,
while others had p values which approached zero. This demonstrates that SAO can
achieve a similar singular value distribution to a dense orthogonal network despite

extreme levels of sparsity for different activation functions.


Table 4.1: Similarity between the singular value distribution of a dense MLP orthogonal
layer and sparse versions.

Method Dense SAO RG LMP LRP

Linear 1 1 4.23E-153 4.23E-153  4.23E-153
Tanh 1 0.83958 4.74E-107 8.92E-54 2.97E-102
ReLU 1 0.99998 2.07E-30 2.07E-30 2.07E-30

SiLU 1 0.41603 1.42E-96 1.53E-62 2.55E-93

Due to this, it is expected that SAO networks will possess the benefits granted by
orthogonal initialization on the training dynamics of dense networks. That is, SAO
networks are expected to remain trainable even at thousands of layers without
specialized techniques such as batch normalization and residual connections, have
faster convergence, and have depth-independent learning times. These effects are
expected to carry over convolutional layers, having their transformations

orthogonalized by SAO.

4.2 SAO on Multilayer Perceptrons

This section presents the results of SAO implemented on MLPs, namely MLP-N and
MLP-Mixer. MLP-7 serves as a preliminary experiment to validate the effectiveness of
SAO by demonstrating that the method can achieve competitive accuracy vs. existing
pruning methods on a simple neural network architecture. Then, it was implemented on
MLP with greater N to demonstrate its trainability in very deep vanilla MLPs of up to
1000 layers.

4.2.1. MLP-7

As a preliminary, SAO was applied on a relatively shallow MLP, specifically the
MLP-7, and compared its performance to GMP-S and GMP-T at 90% sparsity. The


experiment settings are summarized in Table 3.14 row 1. We included the
measurements of the dense network to serve as the baseline. Figure 4.7 shows the
accuracy, while Figure 4.8 and the mean and variance of the singular values of the
MLP-7 networks with different nonlinearities. GMP-S was omitted from the figures as
it underwent drastic accuracy degradation upon pruning (up to 7% reduction) despite
going through the same number of training epochs as SAO and had high SV
distribution variance. Meanwhile, GMP-T and SAO achieved similar accuracy to the
dense network. Furthermore, although GMP-T underwent training before pruning
while SAO was initialized to be sparse, there was not a huge discrepancy in their
accuracy, where SAO was even able to exceed the accuracy of GMP-T for the linear
variant of the model. This implies that SAO can achieve similar accuracy to
pruning-after-training methods without training prior to the introduction of sparsity in

the model.


MLP-7-Linear MLP-7-Tanh

ff oe
4 4
et Spee anneinnesn acne
o roy)
fed ©
5 £
9 Fe
a 4 90
88 88
8 86
*y| GMP-T ~ Dense
sao
0 0 ) 40 $0 60 a x ) 10 x rr 5¢ 60 90
epochs epochs
MLP-7-ReLU MLP-7-SiLU
94 4
>
Boz Bo2
©. e
= |
ro] 3
Y 90 Yo 90
< <
88 88
86 86
GMP-T —*= Dense GMP-T —- Dense
4) Sao oa SAO
0 16 20 30 40 50 (i) 76 [1] 90 ° 0 20 30 40 50 6 70 80 90
epochs epochs

Figure 4.7: Accuracy of Dense, GMP-T, and SAO on MLP-7 at 90% sparsity with
different activation functions trained on the MNIST dataset. The dense network is

included as the baseline.

With regards the singular values, GMP-S (not included in the figure) underwent a
rapid increase in the SV mean and variance throughout training. Meanwhile, SAO had
relatively stable SV statistics with a mean closest to 1, except for the ReLU variant, and
relatively low variance. This suggests that the singular values of SAO networks, except
for ReLU, is better conditioned than sparse networks pruned using GMP-S and GMP-T,

to which the performance of SAO can be attributed.


MLP-7-Linear MLP-7-Tanh

| y a
i, iullnn0 lexan ve | AA
a eT Serene
Il | WA
‘ | i il i
=| Ey
: ° ° ° epochs ; ns * * ° eachs

MLP-7-ReLU

Lt ull

s 2 ua per i rt Hl ULL HAT A | Ht Hi il si min
mH | HAT n TIFT
a ena aay th it A See ie il ‘i
TT TTT
} | | 9 | } |
a) } | 4 |
o 10 20 30 epochs 60 70 80 9 60 10 20 30 epochs 60 70 80 90

Figure 4.8: SV distribution of dense, GMP-T, and SAO MLP-7 across training epochs
on MNIST.

4.2.2 MLP-N

It has been established that it is possible to train very deep vanilla neural networks
through orthogonal weight initialization that allows dynamical isometry. However, this
has not been demonstrated for sparse neural networks and may only be possible
through orthogonality regularization during training, as methods prior SAO which can
introduce sparsity in neural networks without destroying dynamical isometry incur
additional training costs, more than what is needed to train an uncompressed model. In

the previous sections, we showed that SAO can achieve a similar singular value


Ly Livla birls Livle Lirle Lirlio Livli2 Livlas

|
| \|
baa]
| ||
|

Figure 4.9: Reachability matrix of MLP-50 after global magnitude pruning and
Ramanujan construction.

distribution as orthogonal dense networks across various activation functions, and thus
we expect their training dynamics to also be similar. To show that SAO can achieve the
same effects as orthogonal weight initialization on dense networks, i.e. preserving
trainability on very deep sparse vanilla neural networks, we applied it to plain MLPs
with up to a thousand layers. The experiment settings are summarized in Table 3.14

row 2.

Figure 4.9 shows the reachability matrices Ri_,; of the symmetric layers of
MLP-50 when pruned with GMP-S and RG, represented as a binary colour map,
where the coloured pixels indicate the presence of a connection (1) and the absence of
a connection (0). This reachability measurement is congruent to the idea of global
connectivity introduced in [11] which we can determine to be present in a network if
all the elements of R are equal to one. ZL; shows the connections between the input
(rows) and output nodes (columns) of the first layer, while L; —> L4g represents the
connections between the input nodes of the first layer and the output nodes of the 49th
layer. RG has a solid colour map for L; — L4g which indicates that all the inputs
going through L; reaches the output nodes of L4g. On the other hand, the colour map
of GMP-S had grey lines, which indicates that some input nodes do not reach the
output nodes of the last symmetric layer L49. Through this, we show that RG pruning

can maintain the global connectivity of neural networks.

The pruning methods RG-S, RG-T, RG-U, RG-N, and SAO were implemented on
MLP-N with d = 4 and GMP-S and GMP-T with the corresponding sparsity of 98.6%.
All the networks used a gain of 1.1, i.e. the magnitude of the weights were scaled by 1.1.


The default learning rate used is 10-? for N = 50 and 10~? for N > 50, which is reduced
when the sparse network does not train. Figure 4.10 shows the accuracy and the mean
singular values for different N, where the broken red lines indicate dynamical isometry.
The dense and SAO networks used these default learning rates as these networks did not
face any trainability problems for all N. On the other hand, GMP-S, GMP-T, RG-S,
and RG-T resulted in untrainable networks for learning rates ranging from 10-5 to 10~?,
while RG-U and RG-N networks can be trained up to a thousand layers using a lower
learning rate 10-5. These networks are considered untrainable, as they consistently
failed to surpass 10% accuracy even after several epochs. This accuracy level aligns
with random guessing on the MNIST dataset, which comprises 10 classes, translating

to a 1/10 chance of making a correct guess.

The training dynamics of these various networks could be attributed to the
conditioning of their J. The untrainable networks had mean singular values which
approached zero, resulting in a vanishing gradient problem that hinders these networks

from learning [15].

Meanwhile, the singular value distributions of RG-U and RG-N, networks which
maintained trainability but experienced accuracy degradation, had mean values that are
arbitrarily close to 1 but have very high variance implying the presence of very large and
very small singular values. The presence of these very large and small values increases
the risk of vanishing and exploding gradients. To mitigate this, a lower learning rate
was used, which results in slower training compared to those with lower variance for
the same mean [58]. Finally, the SAO network maintained approximate dynamical
isometry, as indicated by the mean and variance of the singular value distributions, thus

achieving the best accuracy among all the sparse networks.

Notably, the dense network maintained its performance despite having mean
singular values nearer 0 than 1. This may be explained by several factors. First, as this
network was initialized with orthogonal weights, uses a sigmoidal activation function
(Tanh), and had not been altered in any way that may interfere with the orthogonality,
e.g., pruning, then this network is assured to possess approximate dynamical isometry

at initialization. Having this condition at the beginning of training allowed the dense


network to learn the necessary representations and achieve good accuracy before the
magnitude of the singular values decreased to a critical level, i.e., much closer to 0,
during training. Furthermore, in contrast to the other models, based on the
post-training singular values ranging from 0.13 to 1.48, the dense network did not
entirely inhibit the gradient flow. The gradient flow may be attenuated, but not to the
point where signal could no longer propagate in the network, which still allowed the
model to learn, albeit slower. Second, the MNIST dataset is among the simplest image
classification datasets. After 100 training epochs, the model had enough time to learn
the representations even with slower learning.

MLP-N on MNIST MLP-N on MNIST

Accuracy
SV Mean

+ Dense + UMPT
SAO GMP-S
RG-S GMP-T

$ RGT RG-U
uMP-s RG-N

B00 1006 50 20

200 400 600 c 0 405 0 800
Number of Layers (N) Number of Layers (N)

Figure 4.10: Accuracy and singular value mean vs. number of layers N of standard
pruning methods, Ramanujan constructions, and SAO on MLP-N trained on MNIST.

Only SAO was able to achieve similar accuracy to the dense network, where RG-U
and RG-N, even though can be trained with a lower learning rate, does not come close
to the performance of the dense and SAO networks. RG-S and RG-T also resulted in
untrainable networks despite having the same Ramanujan property and reachability as
SAO, RG-N, and RG-U. The implications of these results are: 1.) reachability and the
Ramanujan property are prerequisites but are not guarantees for maintaining trainability
in very deep sparse neural network, and 2.) the unique assignment of weights in SAO
resulting in the unique distribution in the weights is not the key to its trainability but
the orthogonality itself. Thus, we prove that SAO grants trainability in very deep sparse

neural network.

We also conducted some exploratory experiments on the implementation of SAO

on MLP-N shown in Table 4.2. These models were trained for 10 epochs in contrast to


the 100 epochs in the main experiment, to demonstrate the trainability of SAO models
early in the training, which have some effects on the training behaviour later on. Two
variations of SAO were tested: one where all the overlapping rows have the same values
and all the layers have the same sparse structure (SAO 2), and one where each layer
has a different sparse structure (SAO 3). These models used a degree of 2 and 8, which
correspond to sparsity levels of 99.2% and 97.2% respectively. In theory, the acceptable
sparsity level is the highest possible sparsity without incurring any accuracy loss, which
depends on the model’s architectural features and complexity of the task. The sparsity
level can be medium, i.e., 50%, or extreme up to 99%. This level of sparsity (or higher,
up to 99.9%) is highly-coveted in model compression techniques such as pruning and
sparse constructions due to its maximal parameter efficiency, which can translate to
maximal memory storage, inference time, and compute efficiency. The preference for
large and deep models that are sparse instead of shallow and dense stems from prior
research which have shown that sparse deep networks are capable of achieving better
performance than smaller, dense counterparts for the same memory footprint [70].

Table 4.2: Accuracy of SAO network variants with d = 2 and d = 8 on the MNIST
dataset.

SAO 1 SAO 2 SAO 3

Layers d=2 d=8 d=2 d=8 d=2 d=8

50 = 92.40 96.24 92.48 96.03 94.15 96.39
200 95.36 97.11 95.06 96.35 95.40 96.84
400 94.71 96.65 95.79 96.78 95.60 97.19
600 94.64 96.41 94.56 96.65 94.88 96.86
800 91.53 96.38 92.73 96.17 91.68 96.53

1000 92.07 94.31 92.11 95.54 91.09 96.86


The difference between the accuracy of SAO 1-3 are subtle and could be dismissed
as statistical discrepancies as the MNIST dataset is very small. Even then, it can be
said that having the Ramanujan structure and sparsity-aware orthogonality is sufficient
to maintain the trainability at very deep networks, while the advantage of one variant
over the other can be explored in further studies. This also shows that trainability can

be maintained at the maximum sparsity for d-regular layers at 99.2% sparsity.

4.2.3. Resource reallocation

As we’ve shown that SAO networks can maintain the trainability in deep MLPs,
we leverage these networks in the depthwise parameter reallocation. Basically, we
compare the performance of neural networks with different number of layers but have
roughly the same parameter budget, made possible by the increasing sparsity of the
deeper networks, as shown in Table 4.3. The experiment settings are summarized in
Table 3.14 row 3. In contrast to our hypothesis, the accuracy of the SAO networks
decreased with the depth. We considered that this could be a consequence of the depth,
where plain neural networks encounter a degradation problem, in which the accuracy
saturates and then degrades rapidly with depth, despite convergence [21]. However,
the dense equivalents for each MLP-N only experienced saturation but not
degradation, and so at the minimum, these SAO networks should retain the accuracy of

the dense baseline to serve any utility.

Table 4.3: Accuracy of MLP-N on CIFAR-10 for different N with the same parameter
budget.

Layers Degree Parameters Dense SAO SAO + Skip

4 Dense 2.43E+06 56.54 - -
25 216 2.52E+06 56.23 55.99 56.80
50 108 2.43E+06 55.72 53.53 56.64
100 54 2.38E+06 56.52 52.10 55.02
200 27 2.36E+06 56.44 49.15 39.71

The SV mean of SAO networks starts on a higher value than the dense network,


but ends with a lower value, and is relatively more consistent throughout the training.
This trend persists for all number of layers. Other than this, there is no substantial
difference in the behaviour of their singular values. The degradation occurring in the
SAO networks may be a matter of network capacity, rather than the condition of the

Jacobian.

We then incorporated skip connections in the SAO networks such as in [21], where
we added a skip connection from the output of layer / to the preactivation of layer
1+ 1. According to [124], the benefits of skip connections go beyond just preventing
vanishing gradients. Thus, we considered that this might improve the performance of
the SAO networks. Through this, we were able to mitigate the accuracy degradation up
to 100 layers, but resulted in a massive decline in the accuracy for 200 layers. From
Figure 4.11, the mean of SAO + Skip fell off for 200 layers, different from the other
models. In this case, the skip connection seems to have worsened the condition of the
Jacobian. So far, no advantages have been found from using deep-sparse SAO networks
against shallow-dense networks of the same parameter budget. However, we should not
dismiss the possibility that there might be better training and more deliberate design

procedures which can benefit these deep-sparse SAO networks.

4.3 SAO on Mixers

SAO was implemented on two modern MLP architectures namely the MLP-Mixer
and the ResMLP. We are interested in these architectures as they can be used on
computer vision tasks while being based entirely on multi-layer perceptrons, unique to
popular state-of-the-art models which rely on convolutions or self-attention
mechanisms. As we have shown the benefits of SAO in MLP-based architectures, we
are interested if these benefits can be extended to modern architectures which may be
able to match convolutional neural networks in terms of predictive power. The
experiment settings to produce the following results for these two models are

summarized in Table 3.14 rows 3 and 4.


__SOlayers

SV mean

(" —— Dense |
ala —— SAO |
| —— SAO + Res |
0.4) —— Dynamical lsometry |

0 20 40 60 80 100 120 140 160 180 200 0 20 40 60 80 100 120 140 160 180 200
epochs epochs
__100 Layers 200 Layers

1.6, 1.4)
1.0

1.2

1.0

0.8,

0 20 40 60 80 100 120 140 160 180 200 “0 20 40 60 60 100 120 140 160 160 200
epochs epochs

Figure 4.11: SV mean of SAO of different number of layers with and without skip

connections during training on the MNIST dataset. The dense model was included to

serve as the baseline.


4.3.1 MLP-Mixer

The MLP-Mixer utilizes layer normalization and residual connections, both of
which are techniques used to assist the training of the model. Residual networks have
been found to be on the edge-of-chaos by default [101], a special regime in which a
deep neural network can operate in that leads to the faithful propagation of gradients.
When a DNN is at the edge of chaos, the singular values of its Jacobian are
concentrated around 1, regardless of the depth, congruent to the concept of dynamical
isometry [15]. Meanwhile, layer normalization serves the same purpose as batch
normalization, i.e. to normalize the activities of the neurons in order to mitigate the
risk of vanishing and exploding gradients, only that this technique avoids the
dependence on the mini-batch size and can be readily applied to recurrent neural
networks [125]. Thus, this type of network may not be as dependent on orthogonality
to preserve trainability as the previous networks, and consequently, the advantage of
SAO networks over pruned networks may not be as prominent. We compared LMP-S,
RG-S, and SAO on the MLP-Mixer with 33 and 65 layers trained on the CIFAR-10
dataset, as shown in Figure 4.12. The dense networks can achieve up to 90% accuracy,
lower than the accuracy of ResNet20 without much hyperparameter tuning [21], but is
nonetheless a good accuracy for an MLP-based architecture. For N = 33, the
advantage of SAO over LMP-S increases with the sparsity. For N = 65, the advantage
of SAO over LMP-S is not as apparent, although it consistently achieves higher
accuracy than RG-S. The non-SAO sparse models did not suffer as badly as in the
earlier experiments, which could be attributed to the presence of residual connections
and layer normalization in the model that provides other ways of maintaining the
trainability despite the sparsity. The residual connections also assure a path between

layers despite possible disconnection between some nodes.


MLP-Mixer on CIFAR-10

~=-= Dense [> ~LMP-S (" RG-S — SAO
MLP-Mixer (N= 33) MLP-Mixer (N = 65)

\65.06

L __ Ee
0.875
Sparsity

i . 0.9375 0.9375
Sparsity

Figure 4.12: Accuracy of LMP-S, RG-S, and SAO implemented on the MLP-Mixer
Nano trained on the CIFAR-10 dataset.

4.3.2 ResMLP

The ResMLP is also an MLP-based computer vision architecture that uses residual
connections, but unlike MLP-Mixer, uses an affine transformation in place of layer
normalization, which shifts and scales the inputs elementwise. A similar trend can be
observed in the comparison of LMP-S and SAO in Figure 4.13: for lower sparsity LMP-
S is better, but for higher sparsity, SAO is better. However, the difference between the
two methods is more drastic than in the MLP-Mixer, which could be through a variety
of reasons. First, ResMLP has a much lower number of parameters than MLP-Mixer.
Second, the use of layer normalization may be more effective when mitigating gradients
than just a simple affine transformation, which is used as an alternative when the goal

is to make the network more efficient.


ResMLP (N = 39) on CIFAR-10

=| = Dense
80 [5 ~LMP-S
SAO
'70.98|
S60 |
Uu | | |
© | | |
dal |
40; |
ot

0.932

Sparsit

Figure 4.13: Accuracy of LMP-S and SAO implemented on ResMLP trained on the
CIFAR-10 dataset.

4.4 SAO on convolutional neural networks

Convolutional Neural Networks (CNNs) are locally connected networks (not full)
with the same set of weights applied across all neurons in each channel of a neural layer.
CNN models reduce the number of model parameters for a regularizing effect [126],
which makes the model less prone to overfitting. CNNs have been especially successful
in machine vision applications compared to MLPs. The computation is implemented by
convolving small weight matrices across the input image to produce neural feature maps
[30]. Repeated weight parameters may be efficient for computation, storage, and model
convergence. However, it is challenging to impose orthogonality on weight vectors that
are shifted versions with repeated values [127; 128] and more so if the weight vectors

must also be sparse.


4.4.1 SAO-Delta
4.4.1.1 Vanilla CNN

We report the performance of SAO-Delta CNN for the CIFAR-10 classification task
using sigmoidal and linear unit activations, i.e. Tanh and ReLU, in Figures 4.14 and
4.15, with a variety of depth, width, and sparsity (50% and (1 — a)%) configurations.
The experiment settings to produce these results are summarized in Table 3.14 row 5.
The models which did not train, i.e., those which did not move past 10% or random
guess accuracy, were not included in the table. We benchmark the sparse SAO-Delta
CNN against an equivalent vanilla CNN (Dense) baseline. In our experiments, the
vanilla CNN required Delta-Orthogonal Initialization to remain trainable to a practical
accuracy beyond a depth of 128 layers. We also attempted to compare local magnitude
pruned (LMP-S and LMP-T) version of the vanilla CNN and CNN with Ramanujan-
only constructions (RG-S and RG-T). However, only SAO remained trainable for all

but one case at (1 — a % sparsity and on 256 layers and above at 50% sparsity.

SAO-Delta is yet to achieve comparable performance with state-of-the-art CNN
networks with special layers, but it contributes an incremental improvement towards
sparse and energy-efficient deep CNN. State-of-the-art ResNet-101, which
incorporates batch normalization and skip connections with customized training
algorithms, achieves accuracy in excess of 93% on CIFAR-10 [129]. In a selected few
SAO-Delta configurations with improved performance, we attribute the slight accuracy
boost to the regularizing effect of sparsity and optimal sizing of network width and

depth [130].


Vanilla CNN (Tanh) on CIFAR-10
~~ Dense GBB LMP-S GE LMP-T © MNBRG-S GE SAO

__N=128,W=128

__N=32,W=256 N= 128, W = 32

Accuracy

os 08:
N= 512, W = 32

Accuracy

= Sparsity

Figure 4.14: Accuracy of standard pruning methods, Ramanujan construction, and
SAO-Delta on a vanilla CNN (Tanh) trained on CIFAR-10.


Vanilla CNN (ReLU) on CIFAR-10

Dense LMP-S SE) LMP-T = RERG-S_ SAO.

N = 32, W = 256 N = 128, W = 32 N = 128, W = 128

Accuracy

{ios

pe]

0.875 0.875 0.875

N = 256, W = 32 N = 512, W = 32 N = 768, W = 32

0875 Osparsity O87
Figure 4.15: Accuracy of standard pruning methods, Ramanujan construction, and
SAO-Delta on a vanilla CNN (ReLU) trained on CIFAR-10.

Accuracy

To test the generalization capabilities of SAO on larger datasets, we examine the
performance of SAO-Delta CNN on CINIC-10 [111]. Figure 4.16 reports the accuracy
of SAO-Delta on CINIC-10 for a variety of depth and sparsity. The experiment
settings to produce the results are summarized in Table 3.14 row 6. The models use the
ReLU activation function, and the same layer width i.e., Cin = Cout = 128. We
compared SAO-Delta to the dense baseline network and to LMP-S. Similar to the
results of CIFAR-10, SAO can maintain trainability at large depths and sparsity, in
contrast to the standard pruning method LMP-S which failed at high sparsity levels at
shallow depths, and even at low sparsity levels at greater depths. Notably among
N = 8,16,32, the robustness of SAO-Delta against sparsity increased with depth,
which could be attributed to the increase in model capacity with depth, just before it
leads to accuracy decline due to training complications, as shown in the dip in
accuracy at N = 128 of the dense baseline, and consequently of SAO-Delta, similar to


the results of CIFAR-10 and in literature, where attenuation of non-uniform modes is
worse in deeper networks [16].
Vanilla CNN on CINIC-10

—-— Dense Ga LMP-S GH LMP-T §6G@ SAO
N = 8, W=128
B32

Accuracy

Accuracy

Accuracy

0.75 0875
N= 128, W= 128

Accuracy

os 075 0.875 0.9688

Sparsity

Figure 4.16: Accuracy of LMP-S, LMP-T, and SAO-Delta on a vanilla CNN trained on
CINIC-10.

SAO-Delta was tested on the more complex CIFAR-100 dataset, which has the same
number of images as CIFAR-10 but with 10 times more classes. This uses the same
training settings used for CIFAR-10. This poses a challenge due to a smaller number of
training samples available per class, and a more intricate decision boundary that is more
difficult for the model to learn. The performance of SAO-Delta and a baseline dense
model on CIFAR-100 are presented in Table 4.4. The accuracy declined with depth
faster than in CIFAR-10 and CINIC-10. The fast decline in accuracy with depth could
be due to how much harder it is to learn, which requires longer convergence time, such
that non-uniform modes which slows down training prohibits convergence within the set
number of training iterations. To support this, MLP on MNIST experienced decrease in
mean singular value across the number of layers as shown in Figure 4.10, but maintained
the accuracy up to 1000 layers, i.e., the depth also attenuated non-uniform modes but

due to the dataset’s simplicity, slower learning was still sufficient to reach convergence.
Table 4.4: Accuracy of SAO-Delta on a vanilla CNN trained on CIFAR-100.
“Sparsity Method N=8 N=16 N=32
"0% Dense 67.35 66.52 63.10
~ 50.00% SAO 66.49 64.17 62.23
"75.00% SAO 63.12 63.47 61.10
~96.875% SAO 51.61 53.75 54.23

4.4.1.2  ResNet

We explored the behaviour of SAO-Delta and RG constructions on residual
networks. Residual networks (ResNets) have become popular in computer vision
applications because they are able to achieve very good performance on a wide range
of tasks with relatively fewer number of layers. The distinctive feature of these
networks is the use of residual connections, which are shortcuts from the input to the
output without passing through some layers. This feature allows the training of very
deep neural networks without suffering from vanishing gradients [21]. Such networks

are shown to be in the edge-of-chaos by default [131; 101], as described in Section 4.3


Thus, these networks are expected to remain trainable even without orthogonal
initialization. Furthermore, as these networks guarantee a path between the input and
output besides the actual layers, it is guaranteed that some connection will remain even
after such models are pruned aggressively. Thus, it is worth exploring whether
orthogonal initialization will present any benefits to such networks. The experiment

settings to produce the results are summarized in Table 3.14 rows 7 and 8.

SAO-Delta was applied on ResNet20, 32, 44, and 56 on CIFAR-10 and ResNet18
on CIFAR-100 to investigate its effects on a CNN with residual connections and batch
normalization. The results are shown in Figure 4.17 and Figure 4.18, respectively. SAO
presents no advantage over LMP-S and RG-S in both datasets, which could be because
the role that is assumed by the presence of dynamical isometry, i.e. conditioning the
gradient [94], and local and global connectivity i.e., assuring that there will always be a

path for signal to propagate, is already taken care of by the residual connections.


ResNet on CIFAR-10
--- Dense i LMP-S HMB RG-S MB SAO

ResNet-20 ResNet-32

Accuracy

0.5 0.875

ResNet-56

Accuracy

Sparsity

Figure 4.17: Accuracy of LMP-S, RG-S, and SAO-Delta on ResNet-20, 32, 44, and 56
trained on CIFAR-10.


ResNet-18 on CIFAR-100

paees a ie — Dense
: 5 LMP-S
67.73 G3) SAO
> |
s)
g
|
ul
<x I
|
|
|
0.5 0.96
Sparsity

Figure 4.18: Accuracy of LMP-S and SAO-Delta on ResNet-18 trained on CIFAR-100.

These results are similar to that of the Mixers which also use residual connections
and specialized layers for gradient conditioning, only that even at higher levels of
sparsity, there are no visible benefits presented by SAO.

44.2 SAO-ECO
4.4.2.1 LipConvNet

The explicitly constructed orthogonalization (ECO) scheme was first created as
one of several adaptions to CNNs to ensure trainability by preserving the gradient
norm. CNNs compliant to 1-Lipschitz constraints [132] learn well-conditioned and
improved feature representations with better generalization capabilities on unseen data
[133]. These more interpretable gradient errors facilitate tracing sources of error or
biases in the model [134] and enhanced robustness against adversarial attacks since
variations in the network output are bounded by the magnitude of perturbations to the
input [135]. To meet 1-Lipschitz constraint, the modified CNN dubbed LipConvNet
[94; 95] utilized the gradient norm preserving MaxMin activation function [136] and
orthogonalized the input-output Jacobian matrix with the weight initialization


schemes, SOC and ECO. ECO cleverly defines orthogonal kernels in the Fourier

domain to generate orthogonal convolutions in the spatial domain.

Our SAO-ECO makes a novel contribution by showing how spatial domain sparsity
can be achieved by sparsifying the Fourier kernels. If a sparse orthogonal matrix S
is used to generate slices of the Fourier domain kernel P[p, q,:, :] Vp, q, there will be
several P{:,:,s,t] comprised entirely of zeros. Consequently, the spatial kernel W{:
3258; t}, which will be recovered through the inverse Discrete Fourier Transform will
also be sparse. Therefore, defining sparse orthogonal Fourier kernels is equivalent to
structured pruning, where entire spatial-domain filter kernels are zeroed out. Kernel-
level sparsity in the spatial domain can be directly related to d-parameterized sparsity

of the orthogonal Fourier domain vectors.

We compare SAO-ECO to pruned versions of LipConvNet as shown in Figure
4.19. The experiment settings to produce the results are summarized in Table 3.14 row
9. At lower sparsity levels, the pruning methods outperformed SAO, but only SAO was
able to maintain trainability at 93.75% sparsity and above for all N. LipConvNet
initialized with sparse RG-S and RG-T topology also failed to train to convergence
beyond > 50% sparsity. We examined the singular values of the Jacobian from all
pruned networks, which failed to converge, and found that mean singular values
approached zero as training progressed. Pruning destroys orthogonality in LipConvNet

and compromises the trainability of the CNN [58].


LipConvNet on CIFAR-10

—— Dense Gi LMP-S LMP-T «= RG-S) RG-T = SAO
LipConvNet-5

Accuracy

Accuracy

0.9375
LipConvNet-15

Accuracy

Sparsity

Figure 4.19: Accuracy of standard pruning methods, Ramanujan construction, and
SAO-ECO on LipConvNet-N trained on CIFAR-10.


4.4.2.2 Vanilla CNN

SAO-ECO was also implemented on a vanilla CNN with Cin = Cout = 128 on
the CINIC-10 dataset, shown in Table 4.5. The experiment settings to produce the
results are summarized in Table 3.14 row 10. Only up to N = 16 was tested due to the
decline in accuracy from N = 8. Similar to the previous cases, SAO-ECO maintains
trainability even at high levels of sparsity. The deeper network is also found to be more

robust against sparsity.

Table 4.5: Accuracy of SAO-ECO on vanilla CNN trained on CINIC-10.

Sparsity Method N=8 N=16

0% Dense 79.10 77.85
50.00% SAO 77.92 75.24
96.875% SAO 68.45 69.87

4.4.2.3 VGG

Batch normalization is a technique used to accelerate the training of deep neural
networks by reducing the internal covariate shift which complicates the training. This
also enables the use of higher learning rates with low risk of vanishing and exploding
gradients [56]. In this regard, it could be said that batch normalization serves the same
purpose as maintaining dynamical isometry in a neural network, that is, these methods
maintain trainability by conditioning the gradient flow in the network. Thus, we seek
to answer whether SAO can still have an advantage over other pruning methods if
every model has a mechanism which conditions the gradient flow independent from

the weight initialization.

We compared SAO to other pruning methods on VGG16 with batch normalization
on the CIFAR-10 dataset, as shown in Figure 4.20. The experiment settings to produce
the results are summarized in Table 3.14 row 11. Unlike in the vanilla CNN, the other
pruning methods maintained trainability (despite the low accuracy of certain models)
on extreme sparsity which could be attributed to the use of batch normalization. For

this model, there is no visible advantage to using SAO over the layerwise magnitude


Pruning, though it achieves slightly better accuracy than RG-S and RG-T despite
having the same sparse structure. This may be attributed to the orthogonality in SAO
models, which may have presented benefits in the model such as better conditioning of
the Jacobian at the beginning of training and removing filter redundancies that stifles

the expressivity of the model.

100 ——___ VGG-16 on CIFAR-10


0.875
Sparsity

Figure 4.20: Accuracy of LMP-S, Ramanujan construction, and SAO-ECO on VGG-16
trained on CIFAR-10.

4.4.2.4 ResNet

Figure 4.21 shows that the Ramanujan constructions, RG-S and SAO, generally
achieved better accuracy than LRP-S and LMP-S on ResNet on CIFAR-10. However,
these methods, including RG-T, could not match the accuracy of LMP-T, proving the
significance of prior training which allows the weights to evolve and strengthen the
important connections before pruning [74]. Furthermore, SAO presents insignificant
advantage over the Ramanujan constructions, which could also be attributed to the
residual connections which places ResNets on the edge of chaos by default as
described in Section 4.4.1.2. This also implies that the sparse structure plays a more
fundamental role in the accuracy of the networks, more than the weights in such
networks [137]. Meanwhile, for ResNet-18 on CIFAR-100 shown in Figure 4.22,
LMP-S achieved higher accuracy than SAO which further shows that SAO presents no

benefits for models with residual connections and batch normalization. The


experiment settings to produce the results are summarized in Table 3.14 rows 12 and
13.

ResNet on CIFAR-10
== Dense G35: LMP-S G3 ~LMP-T 5 RG-S GB RG-T Ge SAO

100. a ResNet-20
eas ie oem oe dio Ss da ss i SE ee ee ee ee eee
> 60
fd
S
o
2 40
0 05 a 0.75 0.875
“On ResNet-32
| Bap Rg a Nee ae ee eee
o 60
£
S
o
2 40
20| |

Sparsity

Figure 4.21: Accuracy of standard pruning methods, Ramanujan construction, and
SAO-ECO on ResNet-20 and ResNet-32 trained on CIFAR-10.


ResNet-18 on CIFAR-100

-=— Dense
Gy LMP-S
M3 SAO

ae ae 68.69]

Accuracy

os 075 0.875

Sparsity
Figure 4.22: Accuracy of LMP-S and SAO-ECO on ResNet-18 trained on CIFAR-100.

For both SAO-Delta and SAO-ECO, we show that the trainability of deep CNN
can be maintained with tunable levels of sparsity without requiring special layers.
Absent the use of special layers like residual connections [21] and batch normalization
[56], our results for CNNs are consistent with behaviors observed in MLPs in
Section 4.2.2, where both Ramanujan connectivity and weight orthogonality are

integral to maintaining trainability of deep and sparse neural networks.

We argue that the SAO framework for tunable sparsity adds value to applying
dynamical isometry to CNNs. SAO preserves trainability while deep CNN is made
sparse without additional parameter or computational overheads. SAO does not
introduce additional model parameters [56] and provably minimizes uncontrolled
gradients [138] through dynamical isometry unlike batch normalization nor does it
impose additional gradient computations as in the case of residual connections.
Without the benefits of sparsity, the advantages of dynamical isometry in CNNs are
less clear. Similar to [58], we observed no clear performance benefits when increasing
the depth of CNNs beyond 200 layers. Furthermore, CNN with ReLU activations do

not possess dynamical isometry but are still favored for image recognition tasks [139]


and outperform CNNs with sigmoidal activations. We observe these results

empirically, and ReLU CNNs are trainable to convergence.

Furthermore, in networks which utilize normalization and residual connections,
maintaining the Ramanujan connectivity and orthogonality does not result in better
performance in sparse neural networks. This is because networks with residual
connections always have a path between layers due to these shortcuts and since these
networks are always at the edge of chaos, that is, even without orthogonal
initialization, these singular values of these networks are always well-conditioned.

Meanwhile, networks with normalization have a mechanism to mitigate gradient flow.

4.5 Potential benefits of Ramanujan construction

Sparse matrices are usually stored using the coordinate format (COO) or the
compressed sparse row (CSR) formats. CSR format stores sparse matrices using three
arrays: nnz, col, and rowptr which contains the nonzero values, the column indices of
each nonzero value, and the indices in the nnz array of the values which are the first
entries of each row and the length of nnz, respectively. Figure 4.23 shows the CSR
format for a 50% sparse matrix and the total number of values to be able to store the

matrix and reconstruct it, which is equal to 21.

X|/1234

ljaQbec
2/0de0
3/00 fg
4j0h00
nnz = (a,b,c, d,e, f, 9, h]
col = [0, 2,3, 1,2, 2,3, 1]
rowptr = [0,3, 5,7, 8]

numbers = 2nnonzeros + Nrows + 1 = 21

Figure 4.23: CSR representation of a sparse matrix.


On the other hand, from our methods, it is known that the Ramanujan constructions
have repeated structure, i.e., these matrices are block matrices with identical blocks.
This repeated structure can be exploited when storing such matrices. Instead of
requiring the column index for all the nonzero elements, it is sufficient to store only the
column indices of the nonzero elements in the first block as this can be reused for the
succeeding blocks. Next, instead of requiring an array to point the first elements of
each row, knowing the degree can determine when to move to the next row. In
Figure 4.24, the required number of elements to store a sparse Ramanujan matrix

through the proposed method is less than that for completely random Sparse structure.

X/1234

a

ab00
00cd
ef00
O00gh

> WO DN

nnz = [a,b,c,d,e, f, 9, h]
col = (0, 1,2, 3]
deg = 2

numbers = Nnonzeros + Neot + 1 = 13

Figure 4.24: Proposed format to represent Ramanujan constructions.

Furthermore, since SAO can reuse the same sparse structure as shown in
Section 4.2.2, it may also be possible to reuse the list of indices for different layers,
allowing further compression of the model given the right optimization library or
hardware. This may also present some advantages in operations where the weights are

accessed simultaneously, such as in backpropagation.

4.6 Drawbacks and Limitations

SAO permits sparse weight and model topology initializations without reference to

a dense model but has several limitations. Firstly, SAO imposes dimensional


1.0) ——~ CSR a

Ramanujan v4

0.8 A
0.6 A

Number of elements to store
\
\
‘
\

0.0

0.0 0.1 0.2 0.3 0.4 0.5
Density

Figure 4.25: Number of elements to store a random sparse matrix and a Ramanujan
sparse matrix with size 1024 x 1024.

constraints where the inputs, outputs, and their connectivity must obey the condition in
Section 3.4.1. The (c, d)-regular graphs to achieve Ramanujan property also imposes
discrete increments in sparsity levels in each layer. Second, it is observed from
experiments that the performance of the baseline dense model serves as the ceiling on
the performance of SAO networks in most cases. As there are not yet CNN and MLP
models that achieve state-of-art performance without special layers like batch
normalization and skip connections, SAO models are not yet competitive
accuracy-wise. Third, SAO cannot leverage the many, publicly available foundational
and pretrained dense models as a good starting point because of differences in network
topology. However, SAO networks may still benefit through knowledge distillation
approaches albeit with the SAO network acting as a student network to the dense
model as teacher network. Finally, the level of sparsity employed in the experiments,
ie., weight level for MLP and MLP-Mixers and kernel level for CNNs, cannot be
readily exploited by commonplace hardware. Thus, specialized libraries or

frameworks are needed to enjoy the benefits of SAO.


4.7 Summary

In this chapter, SAO layers were shown to possess the Ramanujan property and to
have a distinctively similar singular value distribution to dense orthogonal networks,
granting it exact dynamical isometry in linear layers and approximate dynamical
isometry in sigmoidal activations. Through the Ramanujan property, it is assured that
the network possesses local and global connectivity and that the Sparsity will not
hinder information flow, while dynamical isometry assures that SAO networks will

remain trainable despite being very deep and very sparse.

SAO was compared to pruning methods including magnitude and random pruning,
both global and local, and to Ramanujan constructions without orthogonality in
different types of neural network architectures. In plain MLPs and CNNs, SAO
showed huge advantage over other pruning and construction methods, even those
which underwent prior training. Only SAO maintained trainability in very networks
with up to 1000 layers, even without regularizing the gram matrix. Meanwhile, in
MLPs and CNNs which employ normalization and residual connections, SAO did not
present huge advantages as these architectural features mitigate gradient flow.
Resource reallocation was also performed on an MLP, where the depth of the network
was increased without increasing the number of parameters by increasing the sparsity
of the layers, leveraging the SAO structure. Doing this did not result in better
performance, even when residual or skip connections are employed. This implies that
increasing depth over density does not necessarily result in better performance, and

that this may require more deliberate design procedures.

This chapter also introduced a potential method of storing SAO layers that requires
less number of elements to be stored, granted by the regularity of its Ramanujan
structure. Lastly, the drawbacks and limitations of the method are discussed, which
includes the dimensional constraints, accuracy ceiling, and inability to leverage

pretrained models.


CHAPTER 5

CONCLUSION AND RECOMMENDATIONS

5.1 Conclusion
5.1.1 Conclusion

In this study, we combined the concepts of expander graph properties and
dynamical isometry and introduced the method sparsity-aware orthogonal initialization
(SAO), which allows the explicit construction of sparsely connected deep neural
networks possessing dynamical isometry. We accomplished this through the use of
Ramanujan expander graphs to define the sparse connectivity of each neural layer
whose weights are made orthogonal during the initialization scheme. The orthogonal
initialization is aware of the sparse topology of the network, i.e., the orthogonality
comes after the sparse construction and is considered in how the weights are assigned.
The proposed method, SAO, effectively maintains trainability in very deep and sparse
MLPs and CNNs without requiring dense pretraining, specialized architectural
features, or regularization, in contrast to standard pruning methods. We have shown
that utilizing the Ramanujan graph expander structure allows sparse structures to be
leveraged in the explicit construction of deep and parameter-efficient networks, whose
properties assure local and global connectivity, which maintains the connections for
the flow of information in the network all while maximizing the sparsity. In plain
MLPs and CNNs, SAO outperformed the standard pruning methods and Ramanujan
constructions without orthogonality in very deep and very sparse configurations.
SAO’s exceptional performance is then attributed to two critical factors: the
Ramanujan property, which grants the sparse network with maximal connectivity, and

orthogonality, which grants dynamical isometry. This emphasizes the value of


orthogonal weights as the superior choice for very deep and sparse structures in plain
networks. Having achieved better performance than pruning after training, SAO can
save up on the additional dense pretraining required by standard pruning methods all
while achieving better performance in certain configurations, i.e., plain networks.
Meanwhile, SAO did not present any advantages over standard pruning methods in
DNNs which use specialized architectural features such as residual connections and
normalization layers, attributable to the function of these features to mitigate gradient
flow. On the other hand, it was found that reallocating the parameters of a shallow
neural network to create additional layers, even while leveraging SAO, does not lead to
an automatic improvement in accuracy and may require more deliberate architecture

design.

5.1.2. Recommendations

1. The two methods proposed for convolutional neural networks, SAO-Delta and
SAO-ECO, did not present any advantage over standard pruning methods for the
more practical networks, VGG and ResNet. These methods may need to be
redefined for such networks to account for the effects of batch normalization and
residual connections to the Jacobian of the network, such that exact dynamical

isometry can be achieved.

2. It is recommended to extend the method to other types of neural networks, such
as recurrent neural networks (RNN) or the more practical transformer model,
and other deep learning tasks such as natural language processing (NLP). The
RNN, although no longer considered a practical model for NLP, would make an
interesting case for SAO as they are known to suffer from vanishing and
exploding gradients, and as sequential data like text results in long-distance
dependencies, challenges which dynamical isometry can answer to. Meanwhile,
the transformer model is a more practical option, similar to ResNets, which
utilize specialized techniques such as self-attention and layer normalization. We
question whether the orthogonality provided by SAO will present no benefits in

such models, similar to the results in CNNs.


3. The methods SAO, and SAO-Delta and SAO-ECO were applied to induce
weight-level and kernel-level sparsity, respectively, which are granularity levels
that currently cannot be exploited by commonplace hardware. We have hinted to
the optimization potential in terms of memory brought by the repeated structure
of the sparse layers, and thus it is recommended to explore how to exploit such

structure through the development of a framework.

4. Deep learning models that are 1-Lipschitz compliant are known to be robust
against adversarial attacks. Thus, it is recommended to perform adversarial
training on SAO and other sparse neural networks to see if it will present any

advantages, especially in the practical CNNs.


REFERENCES

{1] A. Krizhevsky, I. Sutskever, and G. Hinton, “ImageNet Classification with Deep
Convolutional Neural Networks,” Advances in Neural Information Processing

Systems, vol. 25, 2012.

[2

S. Liang and R. Srikant, “Why Deep Neural Networks for Function
Approximation?” Neural Networks for Signal Processing - Proceedings of the

IEEE Workshop, no. 2015, pp. 21-29, oct 2016.

[3

D. Rolnick and M. Tegmark, “The power of deeper networks for expressing
natural functions,” 6th International Conference on Learning Representations,

ICLR 2018 - Conference Track Proceedings, pp. 1-14, may 2017.

[4

J. Guo, W. Liu, W. Wang, C. Yao, J. Han, R. Li, Y. Lu, and S. Hu, “AccUDNN:
A GPU memory efficient accelerator for training ultra-deep neural networks,”
Proceedings - 2019 IEEE International Conference on Computer Design, ICCD
2019, pp. 65-72, 2019.

[5

N. C. Thompson, K. Greenewald, K. Lee, and G. F Manso, “The
Computational Limits of Deep Learning,” 2020. [Online]. Available: http:
/larxiv.org/abs/2007.05558

[6] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild,

D. So, M. Texier, and J. Dean, “Carbon Emissions and Large Neural Network

Training,” pp. 1-22, 2021. [Online]. Available: http://arxiv.org/abs/2104.10350

[7] Y. Cheng, D. Wang, P. Zhou, and T. Zhang, “A Survey of Model

Compression and Acceleration for Deep Neural Networks,” IEEE Signal
Processing Magazine, vol. 35, no. 1, pp. 126-136, jan 2018. [Online]. Available:
http://ieeexplore.ieee.org/document/8253600/


(8)

[9

[10]

(11]

{12]

{13]

[14]

[15]

Y. L. Cun, J. S. Denker, S. A. Sola, and T. B. Laboratories, “Optimal
Brain Damage,” in Advances in Neural Information Processing Systems, 1989,
pp. 598-605. [Online]. Available: https://proceedings.neurips.cc/paper{ _}files/
paper/1989/file/6c9882bbac1c7093bd2504 188 1277658-Paper.pdf

T. Liang, J. Glossner, L. Wang, S. Shi, and X. Zhang, “Pruning and quantization
for deep neural network acceleration: A survey,” Neurocomputing, vol. 461, pp.

370-403, oct 2021.

J. Liu, Z. Xu, R. Shi, R. C. C. Cheung, and H. K. H. So, “Dynamic Sparse
Training: Find Efficient Sparse Network From Scratch With Trainable Masked
Layers,” in International Conference on Learning Representations, 2020, pp. 1-

14.

A. Prabhu, G. Varma, and A. Namboodiri, “Deep expander networks: Efficient
deep networks from graph theory,” Lecture Notes in Computer Science
(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes

in Bioinformatics), vol. 11217 LNCS, pp. 20-36, 2018.

H. Tanaka, D. Kunin, D. L. K. Yamins, and S. Ganguli, “Pruning neural networks
without any data by iteratively conserving synaptic flow,” Advances in Neural

Information Processing Systems, pp. 6377-6389, jun 2020.

N. Lee, T. Ajanthan, S. Gould, and P. H. S. Torr, “A Signal Propagation
Perspective for Pruning Neural Networks at Initialization,” in International

Conference on Learning Representations, 2019, pp. 1-16.

H. Wang, C. Qin, Y. Bai, and Y. Fu, “Dynamical Isometry: The
Missing Ingredient for Neural Network Pruning,” 2021. [Online]. Available:
http://arxiv.org/abs/2105.05916

A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the
nonlinear dynamics of learning in deep linear neural networks,” in International

Conference on Learning Representations, 2014, pp. 1-22.


[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

L. Xiao, Y. Bahri, J. Sohl-Dickstein, S. S. Schoenholz, and J. Pennington,
“Dynamical isometry and a mean field theory of CNNs: How to train 10,000-
layer vanilla convolutional neural networks,” in 35th International Conference

on Machine Learning, ICML 2018, vol. 12, 2018, pp. 8581-8590.

M. Telgarsky, “Benefits of depth in neural networks,” Journal of Machine

Learning Research, vol. 49, no. June, pp. 1517-1539, feb 2016.

S. Sun, W. Chen, L. Wang, X. Liu, and T. Y. Liu, “On the depth of deep neural
networks: A theoretical view,” 30th AAI Conference on Artificial Intelligence,

vol. 30, pp. 2066-2072, 2016.

M. Thoma, C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,”
Proceedings of the IEEE Computer Society Conference on Computer Vision and

Pattern Recognition, vol. 07-12-June, no. May, pp. 1-9, 2015.

K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for
Large-Scale Image Recognition,” 3rd International Conference on Learning
Representations, ICLR 2015 - Conference Track Proceedings, pp. 1-14, sep
2014.

K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,’ Proceedings of the IEEE Computer Society Conference on

Computer Vision and Pattern Recognition, vol. 2016-Decem, pp. 770-778, 2016.

S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” Proceedings - 30th IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017, vol. 2017-Janua, pp.
5987-5995, 2017.

K. H. Tan and B. P. Lim, “The artificial intelligence renaissance: Deep learning
and the road to human-Level machine intelligence,” APSIPA Transactions on

Signal and Information Processing, vol. 7, no. 2018, pp. 1-19, 2018.


[24]

[25]

[26]

[27]

[28]

[29]

[30]

(31]

[32]

A. Bourely, J. P. Boueri, and K. Choromonski, “Sparse Neural Networks
Topologies,” pp. 1-12, 2017. [Online]. Available: http://arxiv.org/abs/1706.

B. Pal, A. Biswas, S. Kolay, P. Mitra, and B. Basu, “A Study on the Ramanujan
Graph Property of Winning Lottery Tickets,” in 39th International Conference
on Machine Learning, Baltimore, Maryland, USA, 2022.

J. Kepner and R. Robinett, “RadiX-Net: Structured sparse matrices for deep
neural networks,” Proceedings - 2019 IEEE 33rd International Parallel and
Distributed Processing Symposium Workshops, IPDPSW 2019, pp. 268-274,
2019.

J. Schmidhuber, “Deep Learning in neural networks: An overview,” Instituto
Dalle Molle di Studi sull’Intelligenza Artificiale, Manno-Lugano, Tech. Rep.,
2014.

A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, Dive Into Deep Learning, \st ed.
Cambridge: Cambridge University Press, 2023.

Y. Zhao, G. Wang, C. Tang, C. Luo, W. Zeng, and Z.-J. Zha, “A Battle of
Network Structures: An Empirical Study of CNN, Transformer, and MLP,”
2021. [Online]. Available: http://arxiv.org/abs/2108.13002

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp.
2278-2324, nov 1998. [Online]. Available: http://ieeexplore.icee.org/document/
726791/

A. Namdari and T. S. Durrani, ““A Multilayer Feedforward Perceptron Model in
Neural Networks for Predicting Stock Market Short-term Trends,” Operations
Research Forum, vol. 2, no. 3, pp. 1-30, 2021.

S. Pang, Y. Wang, and Y. Bai, ‘Credit Scoring Model Based on Miltilayer

Perceptron,” in IEEE International Symposium on Intelligent Control, 2003, pp.


[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

501-505.

A. Mubalaike and E. Adali, “Multilayer perceptron neural network technique
for fraud detection,” in 2nd International Conference on Computer Science and

Engineering, UBMK 2017, 2017, pp. 383-387.

H. Yan, Y. Jiang, J. Zheng, C. Peng, and Q. Li, “A multilayer perceptron-based
medical decision support system for heart disease diagnosis,” Expert Systems

with Applications, vol. 30, no. 2, pp. 272-281, 2006.

Y. Matsuzaka and R. Yashiro, “Applications of Deep Learning for Drug
Discovery Systems with BigData,” BioMedInformatics, vol. 2, no. 4, pp. 603-
624, 2022.

W. Safat, S. Asghar, and S. A. Gillani, “Empirical Analysis for Crime Prediction
and Forecasting Using Machine Learning and Deep Learning Techniques,” JEEE
Access, vol. 9, pp. 70 080-70 094, 2021.

S. Ha, D. Lee, H. Kim, S. Kwon, E. Kim, J. Yang, and S. Lee, “Neural Network
for Metal Detection Based on Magnetic Impedance Sensor,” pp. 1-13, 2021.

H. Boughrara, M. Chtourou, and C. B. Amar, “MLP neural network based face
recognition system using constructive training algorithm,” Proceedings of 2012
International Conference on Multimedia Computing and Systems, ICMCS 2012,
pp. 233-238, 2012.

J. B. Levitt, D. C. Kiper, and J. A. Movshon, “Receptive fields and functional
architecture of macaque V2,” Journal of Neurophysiology, vol. 71, no. 6, pp.

2517-2542, 1994.

A. Patil and M. Rane, “Convolutional Neural Networks: An Overview and
Its Applications in Pattern Recognition,” Smart Innovation, Systems and

Technologies, vol. 195, pp. 21-30, 2021.

X. Bu, Y. Wu, Z. Gao, and Y. Jia, “Deep convolutional network with locality and


[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

sparsity constraints for texture classification,” Pattern Recognition, vol. 91, pp.

34-46, 2019. [Online]. Available: https://doi.org/10.1016/j.patcog.2019.02.003

H. Mhaskar, Q. Liao, and T. Poggio, “When and why are deep networks better
than shallow ones?” 3/st AAAI Conference on Artificial Intelligence, AAAI 2017,
pp. 2343-2349, 2017.

K. O’Shea and R. Nash, “An Introduction to Convolutional Neural Networks,”

pp. 1-11, 2015. [Online]. Available: http://arxiv.org/abs/1511.08458

S. Ge, V. Singla, R. Basri, and D. Jacobs, “Shift Invariance Can Reduce
Adversarial Robustness,” Advances in Neural Information Processing Systems,

vol. 3, no. NeurIPS, pp. 1858-1871, mar 2021.

H. Shao, L. Wang, R. Chen, H. Li, and Y. Liu, “Safety-Enhanced Autonomous
Driving Using Interpretable Sensor Fusion Transformer,” no. CoRL, pp. 1-18,

2022.

L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y. Duan, O. Al-
Shamma, J. Santamaria, M. A. Fadhel, M. Al-Amidie, and L. Farhan, Review
of deep learning: concepts, CNN architectures, challenges, applications, future

directions. Springer International Publishing, 2021, vol. 8, no. 1.

I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,
J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, M. Lucic, and A. Dosovitskiy,
“MLP-Mixer: An all-MLP Architecture for Vision,” in 35th Conference on
Neural Information Processing Systems, 2021, pp. 1-16.

H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave,
G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, and H. Jegou, “ResMLP:
Feedforward Networks for Image Classification With Data-Efficient Training,”

IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

H. Liu, Z. Dai, D. R. So, and Q. V. Le, “Pay Attention to MLPs,” Advances in
Neural Information Processing Systems, vol. 11, no. Mim, pp. 9204-9215, 2021.


[50]

[51]

[52]

[53]

[54]

[55]

[56]

[57]

[58]

F. Mai, A. Pannatier, F. Fehr, H. Chen, F. Marelli, F. Fleuret, and J. Henderson,
“HyperMixer: An MLP-based Green AI Alternative to Transformers,” in Annual
Meeting of the Association for Computational Linguistics, Geneva, Switzerland,

2023, pp. 15 632-15 654.

Y. Bengio, “Learning deep architectures for Al.” Foundations and Trends in

Machine Learning, vol. 2, no. 1, pp. 1-27, 2009.

Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review
and new perspectives,” [EEE Transactions on Pattern Analysis and Machine

Intelligence, vol. 35, no. 8, pp. 1798-1828, 2013.

M. Bianchini and F. Scarselli, “On the complexity of shallow and deep neural
network classifiers,” 22nd European Symposium on Artificial Neural Networks,
Computational Intelligence and Machine Learning, ESANN 2014 - Proceedings,
vol. 25, no. 8, pp. 371-376, 2014.

Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang, “The expressive power of neural
networks: A view from the width,” Advances in Neural Information Processing

Systems, vol. 2017-Decem, no. Nips, pp. 6232-6240, 2017.

X. Glorot and Y. Bengio, “Understanding the difficulty of training deep
feedforward neural networks Xavier,” in International Conference on Artificial

Intelligence and Statistics, Chia, Sardinia, Italy, aug 2010, pp. 249-256.

S. loffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” 32nd International Conference on

Machine Learning, ICML 2015, vol. 1, pp. 448-456, 2015.

D. Balduzzi, M. Frean, L. Leary, J. P. Lewis, K. W. D. Ma, and B. McWilliams,
“The shattered gradients problem: If resnets are the answer, then what is the
question?” 34th International Conference on Machine Learning, ICML 2017,
vol. 1, pp. 536-549, 2017.

J. Pennington, S. S. Schoenholz, and S. Ganguli, “Resurrecting the sigmoid in


[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

deep learning through dynamical isometry: Theory and practice,” Advances in
Neural Information Processing Systems, vol. 2017-Decem, no. Nips, pp. 4786-
4796, 2017.

G. Yang, J. Pennington, V. Rao, J. Sohl-Dickstein, and S. S. Schoenholz,
“A mean field theory of batch normalization,” in International Conference on

Learning Representations, 2019.
H. Daume, A Course in Machine Learning, 2012.

A. H. Jiang, D. L. K. Wong, G. Zhou, D. G. Andersen, J. Dean, G. R. Ganger,
G. Joshi, M. Kaminksy, M. Kozuch, Z. C. Lipton, and P. Pillai, “Accelerating
Deep Learning by Focusing on the Biggest Losers,” 2019. [Online]. Available:
http://arxiv.org/abs/1910.00762

S. Bianco, R. Cadene, L. Celona, and P. Napoletano, “Benchmark analysis
of representative deep neural network architectures,” JEEE Access, vol. 6, pp.

64 270-64 277, 2018.

X. Liu, W. Xia, and Z. Fan, ““A Deep Neural Network Pruning Method Based on
Gradient L1-norm,” 2020 IEEE 6th International Conference on Computer and
Communications, ICCC 2020, pp. 2070-2074, 2020.

R. Dave, N. Seliya, and N. Siddiqui, “The Benefits of Edge Computing
in Healthcare, Smart Cities, and IoT,” Journal of Computer Sciences and

Applications, vol. 9, no. 1, pp. 23-34, 2021.

H. Xu, L. Zuo, F Sun, M. Yang, and N. Liu, ‘“Low-latency Patient
Monitoring Service for Cloud Computing Based Healthcare System by Applying
Reinforcement Learning,” in International Conference on Computer and

Communications, ICCC 2022. EEE, 2022, pp. 1373-1377.

“Artificial Intelligence Index Report,” Stanford University Human-Centered

Artificial Intelligence, Tech. Rep., 2023.


[67]

[68]

[69]

[70]

[71]

[72]

[73]

[74]

[75]

B. Hassibi, D. G. Stork, and G. J. Wolff, “Optimal brain surgeon and general
network pruning,” in IEEE International Conference on Neural Networks, 1993,
pp. 293-299.

B. R. Bartoldson, A. S. Morcos, A. Barbu, and G. Erlebacher, “The
generalization-stability tradeoff in neural network pruning,” Advances in Neural

Information Processing Systems, 2020.

S. Han, J. Pool, J. Tran, and W. J. Dally, “Learning both weights and connections
for efficient neural networks,” Advances in Neural Information Processing

Systems, vol. 2015-Janua, pp. 1135-1143, 2015.

M. H. Zhu and S. Gupta, “To prune, or not to prune: Exploring the efficacy of
pruning for model compression,” in 6th International Conference on Learning

Representations, 2018.

A. L. Tierney and C. A. Nelson, “Brain Development and the Role of Experience
in the Early Years.” Zero to three, vol. 30, no. 2, pp. 9-13, 2009. [Online].
Available: http://www.ncbi.nlm.nih.gov/pubmed/23894221

N. Lee, T. Ajanthan, and P. H. Torr, “SnIP: Single-shot network pruning
based on connection sensitivity,” in 7th International Conference on Learning

Representations, 2019.

C. Wang, G. Zhang, and R. Grosse, “Picking Winning Tickets Before
Training by Preserving Gradient Flow,” International Conference on Learning

Representations, no. 2019, pp. 1-11, 2020.

J. Frankle, G. K. Dziugaite, D. M. Roy, and M. Carbin, “Pruning Neural
Networks at Initialization: Why are We Missing the Mark?” International

Conference on Learning Representations, 2021.

H. Li, H. Samet, A. Kadav, I. Durdanovic, and H. P. Graf, “Pruning filters for
efficient convnets,” 5th International Conference on Learning Representations,

ICLR 2017 - Conference Track Proceedings, no. 2016, pp. 1-13, 2017.


(76] Y. He, X. Zhang, and J. Sun, “Channel Pruning for Accelerating Very Deep
Neural Networks,” Proceedings of the IEEE International Conference on

Computer Vision, pp. 1398-1406, 2017.

(77] D. Blalock, J. J. G. Ortiz, J. Frankle, and J. Guttag, “What is the State of Neural
Network Pruning?” in 3rd MLSys Conference, 2020.

[78] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y. Wang, and W. J. Dally, “Exploring the
Granularity of Sparsity in Convolutional Neural Networks,” in IEEE Computer

Society Conference on Computer Vision and Pattern Recognition Workshops,

2017, pp. 1927-1934.

[79] S. Anwar and W. Sung, “Coarse Pruning of Convolutional Neural Networks With
Random Masks,” in Sth International Conference on Learning Representations,

2017.

[80] J. Wang, G. Li, and W. Zhang, “Combine-net: An improved filter pruning
algorithm,” Information, vol. 12, no. 7, 2021.

[81] J. van Amersfoort, M. Alizadeh, S. Farquhar, N. Lane, and Y. Gal,
“Single Shot Structured Pruning Before Training,” 2020. [Online]. Available:
http://arxiv.org/abs/2007.00389

[82] NVIDIA, “NVIDIA cuDNN.” [Online]. Available: https://developer.nvidia.com/

cudnn

[83] S. Han, X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally,
“EJE: Efficient Inference Engine on Compressed Deep Neural Network,” 43rd

International Symposium on Computer Architecture, vol. 16, pp. 243-254, 2016.

[84] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding sparse,
trainable neural networks,” 7th International Conference on Learning

Representations, ICLR 2019, pp. 1-42, 2019.

[85] “Graph Figure,” 2023. [Online]. Available:  https://en.wikipedia.org/wiki/


[86]

[87]

[88]

[89]

[90]

[91]

[92]

[93]

[94]

[95]

Graph{ _}theory

T. Piesk, “Bipartite graph figure,” 2023. [Online].
Available: https://en.wikipedia.org/wiki/Bipartite{_}graph{#}/media/File:
Simple{ _}bipartite{_}graph;{-}two{_}layers.svg

G. Brito, I. Dumitriu, and K. D. Harris, “Spectral gap in random bipartite
biregular graphs and applications,” Combinatorics Probability and Computing,
2021.

A. Lubotzky, “Expander graphs in pure and applied mathematics,” Bulletin of the
American Mathematical Society, vol. 49, no. 1, pp. 113-162, 2012.

A. Lountzi, “Expander Graphs and Explicit Constructions,” Uppsala University
Department of Mathematics, Tech. Rep., 2015.

D. T. Vooturi, G. Varma, and K. Kothapalli, “Ramanujan bipartite graph products

for efficient block sparse neural networks,” Concurrency Computation, 2021.

D. Xie, J. Xiong, and S. Pu, “All you need is beyond a good init: Exploring
better solution for training extremely deep convolutional neural networks with
orthonormality and modulation,” Proceedings - 30th IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2017, vol. 2017-Janua, pp.
5075-5084, 2017.

R. Balestriero and R. Baranuik, “A Spline Theory of Deep Learning,” in
International Conference on Machine Learning, Stockholm, Sweden, 2018, pp.
374-383.

R. Balestriero and R. G. Baraniuk, “Mad Max: Affine Spline Insights into Deep
Learning,” Proceedings of the IEEE, vol. 109, no. 5, pp. 704-727, 2021.

S. Singla and S. Feizi, “Skew Orthogonal Convolutions,” in Proceedings of the

38th International Conference on Machine Learning, 2021.
T. Yu, J. Li, Y. Cai, and P. Li, “Constructing Orthogonal Convolutions in an


[96]

[97]

[98]

[99]

[100]

[101

[102]

[103]

[104]

explicit manner,” in International Conference on Learning Representations, no.

2021, 2022, pp. 1-14.

Q. Li, S. Haque, C. Anil, J. Lucas, R. Grosse, and J. H. Jacobsen, “Preventing
gradient attenuation in lipschitz constrained convolutional networks,” Advances

in Neural Information Processing Systems, vol. 32, no. NeurIPS, 2019.

J. Fischer, A. Gadhikar, and R. Burkholz, “Lottery Tickets with Nonzero
Biases,” pp. 1-16, oct 2021. [Online]. Available: http://arxiv.org/abs/2110.11150

H. Wang and Y. Fu, “Structured pruning meets orthogonality,” in International

Conference on Learning Representations, 2022, pp. 1-15.

G. S. Cheon, S. G. Hwang, S. H. Rim, B. L. Shader, and S. Z. Song, “Sparse
orthogonal matrices,” Linear Algebra and Its Applications, vol. 373, no. SUPPL.,
pp. 211-222, 2003.

H. Wang, C. Qin, Y. Bai, and Y. Fu, “Rethinking Again the Value of Network
Pruning - A Dynamical Isometry Perspective,” pp. 1-17, 2022. [Online].
Available: https://github.com/pytorch/examples/tree/master/imagenet

S. Hayou, J.-F. Ton, A. Doucet, and Y. W. Teh, “Robust Pruning at Initialization,”

in International Conference on Learning Representations, 2020.

S. Hayou, J.-f. Ton, A. Doucet, and Y. W. Teh, “Robust Pruning at Initialization,”
in International Conference on Learning Representations, Vienna, Austria, 2021,

pp. 1-39.

J. Wang, Y. Chen, R. Chakraborty, and S. X. Yu, “Orthogonal convolutional
neural networks,” Proceedings of the IEEE Computer Society Conference on

Computer Vision and Pattern Recognition, no. Table 1, pp. 11 502-11 512, 2020.

M. Borel and G. Vénizélos, ‘Vector Calculus,” in Movement Equations 2.
Hoboken, NJ, USA: John Wiley & Sons, Inc., jan 2017, pp. 1-33. [Online].
Available: https://onlinelibrary.wiley.com/doi/10.1002/9781119379065.ch1


[105] J. Pennington, S. S. Schoenholz, and S. Ganguli, “The emergence of
spectral universality in deep networks,” International Conference on Artificial

Intelligence and Statistics, AISTATS 2018, no. 1, pp. 1924-1932, 2018.

[106] R. Burkholz and A. Dubatovka, “Initialization of ReLUs for dynamical
isometry,” in 33rd Conference on Neural Information Processing Systems,

vol. 32, Vancouver, Canada, 2019.

[107] H. Sedghi, V. Gupta, and P. M. Long, “The singular values of convolutional
layers,” 7th International Conference on Learning Representations, ICLR 2019,
pp. 1-12, 2019.

[108] K. D. Maduranga, K. E. Helfrich, and Q. Ye, “Complex unitary recurrent
neural networks using scaled cayley transform,” in AAAI Conference on Artificial

Intelligence, 2019, pp. 4528-4535.

[109] L. Deng, “The MNIST database of handwritten digit images for machine learning
research,” IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 141-142, 2012.

[110] A. Krizhevsky, Learning Multiple Layers of Features from Tiny Images, 2009.

{111] L. N. Darlow, E. J. Crowley, A. Antoniou, and A. J. Storkey, “CINIC-10
is not ImageNet or CIFAR-10,” no. September, 2018. [Online]. Available:
http://arxiv.org/abs/1810.03505

[112] Omihub777, “MLP-Mixer-CIFAR,” 2021. [Online]. Available: https://github.
com/omihub777/MLP-Mixer-CIFAR

[113] Rishikksh20, “ResMLP-pytorch,” 2021. [Online]. Available: https://github.com/
rishikksh20/ResMLP-pytorch

[114] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaugment:
Learning augmentation strategies from data,” Proceedings of the IEEE Computer
Society Conference on Computer Vision and Pattern Recognition, vol. 2019-June,

no. Section 3, pp. 113-123, 2019.


{115}

[116]

[117]

[118]

[119]

[120]

(121)

[122]

[123]

[124]

S. Yun, D. Han, S. Chun, S. J. Oh, J. Choe, and Y. Yoo, “CutMix: Regularization
Strategy to train strong classifiers with localizable features,” in Proceedings of
the IEEE International Conference on Computer Vision, vol. 2019-Octob, 2019,
pp. 6022-6031.

L. Geng and B. Niu, “Pruning convolutional neural networks via filter similarity
analysis,” Machine Learning, vol. 111, no. 9, pp. 3161-3180, 2022. [Online].
Available: https://doi.org/10.1007/s10994-022-06193-w

Y. Idelbayev, “Proper ResNet implementation for CIFAR10/CIFAR100 in
PyTorch,” 2018. [Online]. Available: https://github.com/akamaster/pytorch{ _}

resnet{ _}cifar10

T. DeVries and G. W. Taylor, “Improved Regularization of Convolutional Neural
Networks with Cutout,” 2017. [Online]. Available: http://arxiv.org/abs/1708.

M. R. Murty, “Ramanujan Graphs: An Introduction,” in Academy of Discrete

Mathematics and Applications, India, vol. 6, no. 2, 2020, pp. 91-127.

R. A. Horn and C. R. Johnson, Matrix Analysis, 2nd ed. Cambridge: Cambridge
University Press, 1985.

D. A. Simovici, “Singular Values,” Linear Algebra Tools for Data Mining, pp.
565-617, 2012.

M. Dahleh, M. A. Dahleh, and G. Verghes, Lectures on Dynamic Systems and
Control, Cambridge, 2004.

S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, “Deep
information propagation,’ 5th International Conference on Learning
Representations, ICLR 2017 - Conference Track Proceedings, pp. 1-18,
2017.

A. E. Orhan and X. Pitkow, “Skip connections eliminate singularities,” in 6th


[125]

[126]

[127]

[128]

[129]

[130]

[131]

[132]

[133]

International Conference on Learning Representations, 2018.

J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer Normalization,” 2016. [Online].
Available: http://arxiv.org/abs/1607.06450

Z. Li, Y. Zhang, and S. Arora, “Why Are Convolutional Nets More Sample-
Efficient than Fully-Connected Nets?” International Conference on Learning

Representations, 2020.

J. Wang, Y. Chen, R. Chakraborty, and S. X. Yu, “Orthogonal convolutional
neural networks,” Proceedings of the IEEE Computer Society Conference on

Computer Vision and Pattern Recognition, pp. 11 502-11 512, 2020.

A. Trockman and J. Z. Kolter, “Orthogonalizing convolutional layers with
the Cayley transform,” International Conference on Learning Representations,
no. 2019, pp. 1-21, 2021. [Online]. Available: https://github.com/locuslab/

orthogonal-convolutions.

J. Frankle, D. J. Schwab, and A. S. Morcos, “Training BatchNorm and
Only BatchNorm: On the Expressive Power of Random Features in CNNs,”
International Conference on Learning Representations, vol. 10, no. Figure 1,

pp. 1-28, 2021.

M. Tan and Q. V. Le, “EfficientNet: Rethinking model scaling for convolutional
neural networks,” 36th International Conference on Machine Learning, ICML

2019, vol. 2019-June, pp. 10691-10700, 2019.

G. Yang and S. S. Schoenholz, “Mean Field Residual Networks: On the Edge of
Chaos,” 3/st Conference on Neural Information Processing Systems, no. Nips,

2017.
J. Heinonen, Lectures on Lipschitz Analysis, Jyvaskyla, 2004.
P. L. Bartlett, D. J. Foster, and M. Telgarsky, “Spectrally-normalized margin

bounds for neural networks,” Advances in Neural Information Processing


[134]

[135]

[136]

[137]

[138]

[139]

Systems, vol. 2017-Decem, no. Nips, pp. 6241-6250, 2017.

D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry, “Robustness
may be at odds with accuracy,” 7th International Conference on Learning

Representations, ICLR 2019, pp. 1-24, 2019.

M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier, “Parseval
networks: Improving robustness to adversarial examples,” 34th International

Conference on Machine Learning, ICML 2017, vol. 2, pp. 1423-1432, 2017.

C. Anil, J. Lucas, and R. Grosse, “Sorting out Lipschitz function approximation,”
36th International Conference on Machine Learning, ICML 2019, vol. 2019-
June, pp. 432-452, 2019.

Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value of
network pruning,” 7th International Conference on Learning Representations,

ICLR 2019, pp. 1-21, 2019.

K. Hu, “Revisiting Exploding Gradient: A Ghost That Never Leaves,” vol. 1,
2016.

T. Szandata, “Review and Comparison of Commonly Used Activation
Functions for Deep Neural Networks,” 2018. [Online]. Available: https:
/Nink.springer.com/chapter/10.1007/978-98 1-15-5495-7{-}11


LIST OF PUBLICATIONS

[1] K. Esguerra, M. Nasir, T. B. Tang, A. Tumian and E. T. W. Ho, ”Sparsity-Aware
Orthogonal Initialization of Deep Neural Networks,” in IEEE Access, vol. 11, pp.
74165-74181, 2023, doi: 10.1109/ACCESS.2023.3295344.
