Preconditioning the Support Vector Machine Algorithm to Suit Outlier and Margin Priors of Gaussian Data

Shaira Lee Lopoy Pabalan

University of the Philippines Diliman

Master of Science in Applied Mathematics (Optimization and Approximation Track)

July 2019

Abstract

The Support Vector Machine (SVM) Algorithm is one of the most popular clas-
sification method in machine learning and statistics. However, in the presence of
outliers, the classifier may be adversely affected. In this paper, we experiment on
the hinge loss function of the SVM Algorithm to suit prior information about non-
linearly separable sets of Gaussian data. Taking an inspiration from Huber’s least
informative distribution model to desensitize regression from outliers, we parametrize
some elements of the hinge loss function to challenge the hard margin of 1 convention
and to promote insensitivity of the classification to outliers. Using statistical anal-
ysis, we will determine if at some level of significance, we could see a considerable
improvement with respect to the number of misclassified data.

Chapter 1

Introduction

1.1. Machine Learning

Machine learning is a science of computer programming that deals with algorithms
that can make predictions or decisions based on input data. There are three types
of machine learning: the first type is supervised machine learning, where algorithms
learn from labeled data. The second one is unsupervised machine learning in which
the algorithm is responsible in finding the class of each given input data by the user.
The third one is semi-supervised machine learning, a combination of the two men-
tioned above. Supervised learning can be divided into two categories: classification
and regression. In this study we will only deal with binary (Class (+1) and (—1))
supervised classification.

Classification is a technique of predicting the class of given data points on the
basis of a training set of data containing observations of known class membership.
Four main groups of classification are (1) frequency table, (2) covariance matrix, (3)
similarity functions and (4) artificial neural network and support vector machine. In
this paper, we only focus on support vector machines.

1.2 Support Vector Machines

Support Vector Machine (SVM) which was firstly proposed by Vladimir Vapnik
and Corrina Cortes (Cortes and Vapnik, 1995) is a supervised learning technique
based on statistical learning theory that can be applied to classification, regression
and pattern recognition. SVM is a kind of binary classifier which takes a set of input
data and then classifies each input into two possible classes. In SVM, a data point is
viewed as a p-dimensional vector and then SVM will separate each data points with a
(p — 1)-dimensional classifier. The SVM algorithm makes use of a hinge loss function

Chapter 1. Introduction 2

In practical applications, training observations are often disturbed by outliers and
some even have wrong labels. The presence of outliers can lead to poor classification
performance of the SVM algorithm since the hinge loss function puts a large penalty
to outliers. This results to a severe deviation of the current classifier from the best
classifier. A number of researches have been made to provide alternative solutions
regarding the effect of outliers and their robustness (see [17], [24], [25]).

1.3. Robust Machine Learning

It is known that SVM fits in the regularization structure of regularization + loss
using the hinge loss function Ipinge = max(0,1— z) where z is the margin variable.
The loss term is in charge of the misclassification and the regularization term is used
to avoid overfitting. In many applications, contamination of data due to the presence
of outlier may not be avoided. Unfortunately, SVM is sensitive to outliers because
the quadratic unboundedness of the hinge loss function cause the outliers to have
large losses and therefore greatly affects the SVM algorithm. In order to mitigate the
effects of outliers, different approaches have been suggested to improve the SVM.

Xua et al. proposed a rescaled hinge loss function l-hinge = B(1 — e~™rinse)) (see
Figure 1.1) based on Correntropy-induced loss function which is then used to develop
a new robust SVM using half-quadratic optimization [25]. The resulting rescaled
hinge loss is a monotonic, bounded and nonconvex loss. The authors have shown
that the rescaled hinge loss function is effective in classifying training observations
with the presence of noise or outliers. The paper [24] proposed a truncated hinge
loss function (see Figure 1.2). The mentioned loss function is nonconvex thus solving
the problem involves nonconvex optimization. The authors decomposed the truncated
hinge loss function as a difference of two convex functions and utilized the difference of
convex algorithm, which minimizes a sequence of convex subproblems. In classifying
large data scale with the presence of outliers, Cevikalp and Franc [2] also used the
truncated hinge loss function to lessen the negative effect of outliers, decomposing
the primal problem into convex and concave parts, which is known as the concave-

convex procedure. The optimization is accomplished iteratively by solving a sequence

Chapter 1. Introduction 3

of convex problems obtained by linearly approximating the concave loss function.
Stochastic gradient optimization is used to solve the convex problem. To keep good
classification performance, [17] proposed a robust Lagrangian support vector machine

with modified kernel matrix.

hinge

z

Figure 1.1: Rescaled hinge loss function

Figure 1.2: Truncated hinge loss function

1.4 Gradient Methods for Solving SVM Algorithm

Many works performed different gradient methods in the implementation of the
SVM algorithm. For example, Shwartz et al [20] used the Pegasos algorithm, an ap-
plication of stochastic sub-gradient descent method, against an optimization problem

Chapter 1. Introduction 4

with loss function given by
L(w, b) = max(0, 1 — y(w" 2; + b))

_ fo, if y;(w"2; +b) >1
1l—y(w'z;+ 6), if y(w'2;+6) <1.

Collins et.al. [5] applied exponentiated gradient method against a loss function
that is based on log-linear models. The loss function is the negative log-likelihood
lx(w, ti, yi) = —log p(y; |zi, w) where p is the conditional distribution. Trust region
Newton method is used by Lin et al in [16] to solve an optimization problem where
the loss function is max{0,1— y;w'z,}*. Chang et al proposed a novel coordinate
descent algorithm for solving linear SVM with L--loss function which is defined as
max(1 — y;w"z;, 0)? [3]. Chapelle [4] used Newtons method in minimizing SVM with
hinge loss function and quadratic loss function. A dual coordinate descent method
is used in solving linear SVM with L,-loss function given by max(1 — y,w'=;,0) and
L,-loss function [10]. Yacat also presented nonlinear conjugate gradient method as
optimization method in solving SVM algorithm with quadratic and nonquadratic reg-

ularizations (see [26]).

The remainder of this paper is arranged as follows: the second section states the
concepts and preliminaries used in this study; challenging the hard margin parameter
of 1 is presented in Section 3; the proposed hinge loss function is discussed in Section
4; the fifth section gives the algorithm of the optimization method used in this study
as well as the statistical tests conducted; Section 6 shows experiments and results

and; we conclude the paper in Section 7.

Chapter 2

Preliminaries

2.1 Basic Concepts
Here, we present some basic concepts that would be used throughout this thesis.

Definition 2.1.1. [19] In a p-dimensional space, a hyperplane is an affine subspace
of dimension p — 1.

For example, in a 2-dimensional space, a hyperplane is a line. If p = 3, a hyper-
plane is a plane. Mathematically, in two dimensions, a hyperplane in R? is defined to
be the set of all vectors (21,22) € R? such that

WZ, + Wet2 +b =0 (2.1)

with real-valued parameters b, w;, and w2. Equation (2.1) extends naturally to p-

dimensional spaces where we have
WT, + Wet, +--+ Wet, +b=0 (2.2)

as the defining equation for a hyperplane.
We recall the definition of support vectors in Convex Analysis.

Definition 2.1.2. [1] Let H be a Hilbert space endowed with the inner product (- | -).
Let D be a non-empty subset of H, let s € D, and suppose that u is a nonzero vector
in H. If
sup {y|u) < (z|u),

then the set {y € H | (yu) = (xlu)} is a supporting hyperplane of D at x, and z
is a support point of D with normal vector u. In finite-dimensional spaces, and if
D is nonempty, closed, and convex, then the support points of D are precisely the
boundary points of D [1].

This definition can make sense even if we have a discrete set of points. Suppose
that A is a finite subset of 2. Then the points in A along the boundary of the

5

Chapter 2. Preliminaries 6

polytope formed by the convex hull of A will be referred to as the support vectors of
A (see Figure 2.1).

2 a x s _® &

> ®e ‘®; © @

ad es © @ °
ce %e @

Figure 2.1: Left: There are two classes of observations with three separating
hyperplanes, out of many possible. Right: There are two classes of observa-
tions with the maximum margin classifier shown as the broken line. The mar-
gin is the distance from either of the solid line to the broken line. (Source:
hub.packtpub.com/what-is-a-support-vector-machine/]

2.2 Classification by Hyperplanes
Now, suppose that z+ € R? does not satisfy (2.2), say
WZ + Weteg +--+ Wplp +b>0 (2.3)

This tells us that x lies on one side of the hyperplane. On the other hand, if

Wi Ty + Wet. +--+ + WyeTp +b <0 (2.4)

then z lies on the other side of the hyperplane. From this, we can think that a hyper-
plane divides a p-dimensional space into two halves. Thus, we can identify on which
side of a hyperplane a given vector = lies by looking at the sign of the left hand side
of Equation (2.2).

Let X = {(zi, yi) }isicw be a subset of R? x {—1,+1}. We call X our training
set. where the z;s are the input observations and the y;s are the corresponding class
labels.

We wish to find a classifier that will correctly classify the training observations.

Chapter 2. Preliminaries 7

Suppose we can classify the training observations perfectly according to their classes
(see Figure 2.2). We can label the observations by y; = +1 on the green class and
by y; = —1 on the blue class. We then have the following property of a separating
hyperplane:

w'a;+b>0 (2.5)
if y, = 1, and

wis, +b<0 (2.6)

if y; = —1. Equivalently, we can write
yi(w'a; +b) >0 (2.7)

for all i = 1,--- ,N. We classify the training observation z depending on the sign of
f(z) = w'r+o. If f(z) is positive then r belongs to Class 1. If f(z) is negative then x
belongs to Class —1. In essence, we want to find a linear functional f : RP + {—1,1}
such that f(x;) = y; for each i € {1,..., N}.

&
A A &
a* &
&
°.
A BS
@ ®

Figure 2.2: There are two classes of observations, shown in green and in blue.
The gray line is the hyperplane that separates the green class from the blue class.
[Source:www.google.com/ /linearlyseparablesets]

2.3. Linearly Separable Case

In general, if our data can be perfectly separated by a hyperplane, then we can
choose an infinite number of hyperplanes that will classify the data by perturbing the
line without touching any of the observations. Now, how do we determine which is

Chapter 2. Preliminaries 8

the best in classifying our observations? Intuitively, the best choice is the maximal

margin classifier.

A maximal margin classifier is the separation of a line that is closest to the class
points. That is, we compute the perpendicular distance from each training obser-
vation to a given separating hyperplane; the smallest such distance is the minimal
distance from the observations to the hyperplane, known as the margin. The maxi-
mal margin classifier is the separating hyperplane for which the margin is the largest.
In other words, it is the hyperplane that has the farthest minimum distance to the

training observations in each class.

Linearly separable data can be divided into two classes using the maximal margin
classifier. Let us consider constructing a maximal margin hyperplane based on a. set
of N training observations (z1,--- ,2~) € IR?*% and class labels y; € {—1,1}. Note
that the distance between z; and the hyperplane H,,, = {y € R?|w'y + b = 0} is

|w' x; +5]

In practice and by convention, we want a hard margin of at least 1 between the
hyperplane and the support vectors of each class. The hard margin between z; and
the hyperplane H,,, is defined to be |w'x; +6]. Thus, we want to find w € R? and
b € R such that

jw" a; + b| > 1. (2.8)

for alli € {1,...,N}-.

Assume that y; = 1 for alli = 1,...,l and y; = —1 for all j =1+1,...,N. If
f(z) = w'z +b, we want that f(z;) > 0 for all i = 1,...,/ and f(z;) < 0 for all
j =1+1,...,N. Thus, the aforementioned formula for the distance of z; from the
hyperplane is equal to

yi(w' a; +b)

, where i = 1,2,...,N.
Hel

So, the maximal margin classifier is a solution to the optimization problem

Chapter 2. Preliminaries 9

3 ‘3 b :
me Bh Do 7 ae ) subject to y;(w' a; +b) > 1. (2.9)

By our convention in (2.8) of hard margin, (2.9) is equivalent to

—— ] le T 2
wb wl subject to y,(w'z,+6) >1. (2.10)

Clearly, any solution (w, b) € R? xR to (2.10) will also solve the equivalent formulation

winits simi? subject to y,(w' a; +6) > 1. (2.11)

2.4 Nonlinearly Separable Case

In many instances, a hyperplane that will exactly separate two classes may not
exist because there might be a small mixture of classes. However, we can extend the
concept of separating hyperplane in order to find a classifier that almost separates the
data into two classes. Instead of finding the largest possible margin such that every
observation is on the correct side of the hyperplane, we allow some observations to be
on the incorrect side of the hyperplane. However, to minimize these misclassifications,
we penalize the violation of the hard margin rule by adding a slack variable € > 0 in

the constraint of (2.11). The new optimization problem becomes

2 P ayT x. >1-&
deinen 5 dtl subject to y,(w'z, +b) >1-&. (2.12)
The slack variable €; controls the position of the observations relative to the margin
and the separating hyperplane. The new constraint permits a margin that is less than

1. We can move the linear constraint of (2.12) into the objective function as follows

> wll? + N $30 _ yi( wT a+ b))+ (2.13)

weRP, Tr beR 2 =]

where C is a positive constant and (£), = max(0, £) whenever E is a real number.

Chapter 2. Preliminaries 10

Figure 2.3: Hinge Loss function

The term (1 — y,(w'z; + 6)), is the hinge loss function. Hinge loss acts as a
penalty function whenever a constraint is violated. It is defined as

0, if y;(w'2;+6) >1
L(w,) = (1 - yi(w" 2, +), = a )
1—y(w'z +0), if y(w's, +b) <1.

The hinge loss is both convex and continuous but it is not differentiable at points
where y,;(w' x; +6) = 1. This nondifferentiability however does not pose any problem
to a gradient-based algorithm because the point where the function is nondifferen-
tiable is an optimal point. Hence, we can just assign a zero gradient at this point
(where y,(w'z,; + b) = 1) so that the algorithm terminates. Moreover, the hinge loss
function has 0 as a subgradient of points where y,(w' 2; +6) = 1. This verifies that
they are optimal points (cf. Fermat’s Theorem [1]) of the hinge loss function. Figure
2.3 shows the hinge loss function. Correctly classified points lying outside the margin
boundaries of the support vectors are not penalized while points within the margin
boundaries or points on the wrong side of the hyperplane are penalized linearly.

If there is a clear classifier that is nonlinear in nature, there is another approach

in classification using kernel method. The training observations are mapped to an

Chapter 2. Preliminaries 11

appropriate higher dimensional space where a separating hyperplane can be easily
found. Many works used kernel methods in classification see [2], [17], [24], [25]. The

use of kernel methods is not in the scope of our research.

Chapter 3

Hinge Loss Function with Varying Margins

Given nonlinearly separable Gaussian data sets X, = {(z;,1) |i = 1,--- , J} and
X2 = {(z;,-1)|j = 1 +1,---,N} that have different margins, i.e., the distance
between the two classes differs, we challenge the hard margin parameter of 1. Consider
the primal problem with L-regularization term shown below

L

. cy
eenin_, sll? + =D max(0, 1 — ys(wTas + 6) (3.1)

t=1

where C’ is a positive constant and max(0, 1 — y;(w"z,;+6)) is the hinge loss function.

Remark 3.1 We can suppress b in Problem 3.1 by replacing our variables w and x
by [w' b]’ and [xz 1]" respectively. Clearly,

w'e+b=[w' br" y"
and dimension of xz is increased by 1.

We want to investigate whether changing the hard margin of 1 significantly im-
proves or worsens the resulting classifier as we increase the distance between the two
given classes. The distance between the two classes is determined by how far the two
classes are from each other with respect to their means. Figure 3.1 shows the hinge

loss function with a variable margin a.
Considering the margin a, we now have our new optimization problem. Instead

of the hard margin rule of 1, we examine different values of a. Thus, we have the

following unconstrained optimization problem with modified hinge loss function

1 ce
A 2 T
amin, sllwll" +3 » max(0, a — yw’ z4)

12

Chapter 3. Hinge Loss Function with Varying Margins

Figure 3.1: Hinge loss function with margin a
where C > 0 and

0, if yw' a; >a

max(0,a — y;w'2;) =
a—yw's, if yw'a; <a.

13

Chapter 4

Proposed Hinge Loss Function

In 1964, Huber introduced a new approach towards handling data that is riddled
with outliers. The theory of robust statistics is popularly attributed to him. His PhD
dissertation was on the estimation of a location parameter for contaminated normal

distributions and exhibiting estimators that are asymptotically more robust among
all invariant estimators.

Now, let 6, ...,bm be i.id. random variables with a common distribution function
F. We are then interested to find the M-estimator T.,, of the location parameter T
by solving the minimization problem

m
min > p(b; — T)
where p is symmetric, convex and differentiable function.

According to Huber, since contamination are mainly present for large sample sizes
then it does seem necessary to optimize large sample robustness properties. For a
particular p, a measure of robustness for asymptotically normal estimators is the
supremum of the asymptotic variance

ACR, p) = EB?)

(Er(¢"))?
over the set of all distribution F = (1 — ¢)®+ eH for fixed 0 < « < 1, H is the
distribution for outliers and ® is the distribution where most of the data fall under.
Adopting this measure of robustness and restricting the estimators to M-estimator,
then the most robust estimator can be uniquely produced and corresponds to the

following function

122, if |t| << k

px(t) =
klt|— 3k?, if |t| > k,

14

Chapter 4. Proposed Hinge Loss Function 15
where k € [0, +00] satisfy

€

20'(k)
k l-e

— 26(—k) =

. 1 J —z?
and &(j) = Van exp (+) dz is the cumulative function of the Gaussian dis-
—co

tribution.

The function mentioned above is quadratic for small values of ¢ and linear for
large values, with equal values and slopes of the different sections at the two points
where |t| = k. This is the Huber loss function used in robust regression. Figure 4.1
shows a Huber loss function at k = 5. The Huber loss function describes the penalty
incurred by an estimation procedure. It is strongly convex in a uniform neighborhood
of its minimum t = 0 and has a differentiable extension to an affine function at points

t = —k and t = k making it less sensitive to outliers since it combines the mean and
median unbiased estimators.

Figure 4.1: Huber loss function

Now, to adapt the Huber loss function in our classification, we replace b; — T by

max(0,1 — y;w"z,) obtaining the optimization problem

min 5 > p(max(0,1 ~ yto"2)).

t=1

Chapter 4. Proposed Hinge Loss Function 16

where max(0, 1 — yw'z;) is the hinge loss function. We now want to find the com-
position of p and the hinge loss function. Let

0, if yww'a;>1
l-yw'2;, if yw's <1
and

2 *

5, if |t| <k
pr(t) = 4”

kt| — ©, if |t| > k

The composition p, o f(w) is

0,

pro f(w) = ¢ Ge cw 7 if yw" a; € (1 —k, 1)

kil — yw'a,| — gE if yw'a; <1-k

if yw'a; >1

From this, we formulate our proposed hinge loss function. We have

2H, 0 f(w) = PHL(w;a =1,8 =1—k,y =o)

0, if yw'2; >1
= 4 (1—yw's)?, if yw"; € (1—k,1)

2k|1 — yw'a,|—k?, ifyw'e;<1-—k

In general case, letting arbitrary values of a and 8, we have

2M, © f(w)

PHL(w;a, 8B = 1—k,7 = 00)

0, if yw'r; >a

(a — yw'z;)?, if yw" x; € (B,a)

(28 — 2a)(yw's;— B)+(a-B)?, ifyw's; < B

where a, € R.

Chapter 4. Proposed Hinge Loss Function 7

Inspired by Huber, given two classes of nonlinearly separable Gaussian data
with known expected mixture and expected outlier percentage, we impose a greater
penalty cost on misclassified bonafide data compared to data that are considered
outliers (both misclassified and not misclassified). We penalize misclassified bonafide
data quadratically and on the other hand, we impose linear penalty to misclassified
and correctly classified outliers. The linear penalty function imposed to all outliers
will have the same slope. We note that in this study, data considered as outliers are
those that are far away from the two classes, i.e, their means are significantly bigger

or smaller than the means of the bonafide data.

Given the training set X = {z;, y;}1<:<w, the proposed hinge loss function PH Le,a(w)

is
0, if yw's; >a
PHL a(w) = 4 (a — yw" 2,)?, if B <yw'at;<a
(28 — 2a)(yw'2; — B)+(a—- 8), if yw'a; <B
where a, f € R.

We divide the instances into three categories according to where the data is lo-
cated. We have zero cost for correctly classified bonafide data. Quadratic cost for
misclassified bonafide data and linear on misclassified outliers.

Our optimization problem now becomes

_ 1
min =

cw.
min, 5 llwll? +5 > PHL, (wv, zi, yi) (4.1)

t=1

where the first term is the regularization term used to avoid overfitting, the second

term is the penalty term given for misclassifications and C > 0.

The gradient of the proposed hinge loss function with respect to w is

Chapter 4. Proposed Hinge Loss Function 18

0, if yw's;>a

VPHLaa(w) = 4 2(a — yw" 2;)(—yx;), if B< yw'n <a

(28 — 2a)(y:2:), if yw" 2, < B
Figure 4.2 shows the graphical representation of the proposed hinge loss function

PHL.

PHL(w,x,y)

Figure 4.2: Proposed hinge loss function

Chapter 5

Numerical Implementations

In this chapter, we discuss on how we implement the SVM algorithm by utilizing
nonlinear conjugate gradient method. Statistical tests that are used in verifying our
results are also discussed here.

5.1 Nonlinear Conjugate Gradient Method

Conjugate gradient method is one of the most useful techniques for solving large
linear systems of equations with positive definite coefficient matrices. It can also be
viewed as a minimization algorithm in nonlinear functions [18]. In nonlinear conju-
gate gradient method, we extend the linear conjugate gradient method by replacing
the gradient of the linear function with the gradient of the nonlinear objective func-
tion. Inexact line search method for finding steplength is recommended. The two
widely used nonlinear conjugate gradient methods are Fletcher-Reeves (FR) method
and Polar-Ribiere (PR) method.

In our study, nonlinear conjugate gradient method with a version of Fletcher-
Reeves (NCG-FR) is used in minimizing both unconstrained objective function using
the hinge loss as penalty function and using the proposed hinge loss as penalty func-
tion. Below is the pseudocode of the algorithm mentioned.

Algorithm 1. NCG-FR optimization method

Input: training dataset (z;,y:)%1, 0 <c1 < c < 3, f, VS, initial guess: w = [0; 0; 0]
Output: w

1: Set search direction pp = —V fo, iteration k = 0, « > 0;

2: while ||V fxl| > «€ do

3: Determine steplength a, s.t. strong Wolfe rule is satisfied;

19

Chapter 5. Numerical Implementations

4: Update wy41 = we + Onpy;
. : FR _ VoegiV Seti,
5: Obtain k+l = WHT,
6: Update peri = —V fear + Ber peat;
7: Sttk=k+1;

8: end while

In Algorithm 1, replace f by Problem 3.1 or 4.1.

5.1.1 Line Search Algorithm for Strong Wolfe Conditions

20

The choice of line search parameter a, is crucial. In order to ensure that p; is a

descent direction, we require a, to satisfy the strong Wolfe conditions

f(z + oupe) < f (re) + cr0KV fy Pe

IVF (ve + oxpe)" Pel < colV fy Pel

(2.1a)

(2.1b)

with 0 < c¢ < @ < 3. The algorithm has two stages. The first stage (Algorithm
2) begins with an initial a, and keeps increasing it until it finds an acceptable step
length or an interval that brackets the acceptable step lengths. For the second stage,
it calls another function (Algorithm 3) which keeps on decreasing the size of the in-

terval until an acceptable step length is found.

Algorithm 2. Strong Wolfe Algorithm

Input: training dataset (x;,y:)%,,0<a <<}, f, Vf

Output: a,

1: Set ap = 0, a; > 0 , maz > 0;
2: Initialize i = 1;

3: while 1 do

4: if f(a;) > f(0) + crasf'(0) or [f(ai) > f(ai-1) and i>]
a, = z200m(q,_1, a@;) and stop

5: if |f'(ai)| < —c2f'(0)

a, = a; and stop

6: if f"(a4) > 0

Chapter 5. Numerical Implementations 21

a, = z00m(q;,a;-1) and stop
7: Choose a:+1 € (0;, maz);

8: Seti =2+1;

9. end while

Algorithm 3. Zoom function

Input: aio, Oni
Output: a;

1: Initialize j = 0;

2: while 1 do

3: Interpolate using bisection method to find a trial step length a; between a, and
Oni

4: if f(a;) > £(0) + cra; f"(0) or f(a;) > f(a)
Ons = 0;

5: else

if |f'(a;)| < —caf'(0)

a, = a, and stop

6: if f'(a;)(@ni — a) > O

Oni = Oe;

7: Set ay. = a;;

8: Set j= 7 +1;

9: end while

5.2 Statistical Analysis

In this section, we discuss two nonparametric statistical tests. These two statisti-
cal tests are used in verifying whether our obtained results have significant effect on
SVM’s classification performance at some level of significance 0.05.

Chapter 5. Numerical Implementations 22

5.2.1 Wilcoxon Signed Rank Test

Wilcoxon signed rank is a nonparametric test which aims to detect differences
between variables from the same sample before and after intervention by calculating
the differences between their ranks. It is used to test the null hypothesis that the
two samples come from the same population, that is they have the same median or,
alternatively whether observations in one sample tend to be larger than observations
in the other sample (see [9]). We use the Wilcoxon signed rank test in determining
whether there is a significant difference between the number of misclassified data ob-
tained from using hinge loss function and from using the proposed hinge loss function
at some level of significance.

5.2.2. Spearman Rank Correlation Test

Spearman rank-order correlation is a nonparametric test used in measuring the
degree of association between two variables. Spearman test does not carry the distri-
bution of the given data and data should be at least in an ordinal scale (see [9]). We
use the Spearman rank correlation test in identifying whether there is a significant

relationship between outlier percentage and number of misclassified data.

Chapter 6

Experiments and Results

6.1 Implementation and Initialization

6.1.1 NCG-FR Algorithm and Line Search

The NCG-FR algorithm used in this study adopts strong Wolfe conditions for step-
size selection. The numerical code was implemented using MATLAB and tests were

run on a 64-bit Windows 10 machine with 8.00 GB RAM and 15 — 5300U CPU@2.30
GHz processor.

The NCG-FR implementation used the origin as the initial point. The NCG
maximum iteration maziter is 1000 and the termination criteria tol is 1 x 10-5. For
the line search parameters, initial steplength ¢ is 1, c, = 0.0001, cp = 0.01 and strong
Wolfe maximum iteration mazit is 100. This is the default configuration unless stated

otherwise.

6.1.2 Statistical Analysis

The statistical tests conducted in this study were run in a statistical software
called Statistical Package for the Social Sciences (SPSS). It is designed to solve re-
search problems by means of hypothesis testing and predictive analysis. It is used to
understand and analyze data and trends. For the manual instructions see [15].

6.1.3. Parameters C, a, 8

We choose different values of a exhaustively. We consider 0.05 < a < 1 with an
increment of 0.05 and 1 < a < 10 with an increment of 0.5. We also examine the case
when a = 20,50. For the values of beta: we consider —3 < 6 < 0 with an increment
of 0.5. We use C = 0.01 in comparing the classification performance of the hinge loss
function and the proposed hinge loss function.

23

Chapter 6. Experiments and Results 24

6.2 Conventional Hinge Loss Function with Vary-

ing Margins

Table 6.1 shows the experimental Gaussian datasets with different means p and
standard deviations o. As class distributions in the datasets are balanced, we based
our classification results with respect to the number of misclassified data. Experimen-
tal datasets 1 to 8, which we denote by D1-D8, are arranged such that the distance
between two classes is increasing. A % mixture means the intersection of data points
between Class +1 and Class —1. If the percentage intersection of data points is higher
then it means that the distance between two classes are closer. In our case, D1 has
the least margin while D8 has the greatest. margin. The % mixture is computed by

finding the area under the normal curve where two normal distributions intersect.

DATA SETS | Class +1 (4,0) | Class -1 (4,0) | Dimension
Di (4,0),2 (4,0),2 2000| 4.5%
D2 (15,0),6 (-15,0),6 2 2000| 1.2%
D3 (4,0),1.5 (-4,0),1.5 2 2000 | 0.7%
D4 (100,0),35 (-100,0),35 2 2000/ 0.4%
D5 (50,0),17 (-50,0),17 2 2000 0.3%
D6 (100,0),33 (-100,0) 33 2 2000 0.25%
D7 (6,0),2 (-6,0),2 2 2000} 0.2%
D8 (65,0),20 (-65,0),20 2 2000 | 0.1%

Table 6.1: Nonlinearly Separable Gaussian Data Sets

After performing NCG-FR with strong Wolfe inexact line search algorithm to the
hinge loss function with parameter a, we obtain the following results summarized in
Table 6.2. The best results are highlighted in bold. For example, in D1, we obtain
the least misclassified data at a = 0.05. In D8 we obtain the least misclassified data
at a = 0.15. In cases where we have acquired the same number of misclassified data,
we choose the a where it corresponds to smallest objective function value given the
objective function presented in Problem 3.1. For example, in D2, the a that classifies
the two classes best is a = 0.05 since it has the smallest objective function value.
Notice that in each datasets, the a that has the least number of misclassified data is
between 0.05 and 1. Moreover, if we set a = 0, the solution is infeasible.

25

Chapter 6. Experiments and Results

Number of Misclassified Data

D8

D7

D5 D6

D4

al

sel
~

0.70
mi

[2
[2
a
[2
—
[2
[2
[2
[2
[2
[2
[2

2

2

2

2

2

6
6
[6 |
[6
[6
[6
[6
[6
6
6
6
6
6
6

6

6
re
re
rs
|

5

5

5

5

5

5

6

N
~~

Table 6.2: Number of Misclassified Data from D1-D8 with different values of a

Chapter 6. Experiments and Results 26

A Spearman rho nonparametric test was conducted to determine whether the
choice of a has a significant effect on identifying the best separating hyperplane given
margin priors of Gaussian data. Table 6.3 presents the Spearman correlation coeffli-
cient p = —0.464 (weak negative correlation), its significance value (0.247) and the
sample size that the calculation was based on. At some significance level 0.05, since
p — value = 0.247 > 0.05, then we have sufficient evidence to say that there is no
correlation between changing the hard margin of 1 and varying the distance between
two classes, i.e, no matter how far the distance between two classes are, the perfor-
mance of SVM classifier is not significantly affected when we vary a.

Sig.(2-tailed) 0.247
| sds alpha | Correlation Coefficient | -0.464 | 1.000

Sig.(2-tailed)
Table 6.3: Spearman Correlation Coefficient

margin | alpha
Spearman’s rho | margin | Correlation Coefficient | 1.000 | -0.464
0.247

6.3. Proposed Hinge Loss

Given nonlinearly separable Gaussian datasets with known expected mixture and
outlier percentage, we compare the performance of the SVM classifier in the presence
of outliers with respect to the number of misclassified data using the hinge loss func-
tion and the proposed hinge loss function. Table 6.4 shows eight (8) experimental
Gaussian data sets with known outlier percentage and distribution. The means and

standard deviations of each class are denoted by p and o respectively.

Based from the results of Section 6.2, since we have shown that changing a has
no significant effect on the performance of the SVM classifier as we vary the distance
between two classes, we fixed a = 1 for the following implementations.

Chapter 6. Experiments and Results

27

Class -1 (4,0) | Dimension | Size | Outlier (Dis- |
2 | tribution)
(4,0),2 (-4,0),2 2 2000 | Uniform
(15,0),6 (-15,0),6 2 2000 | Uniform
(1.96,0),1 (-1.96,0),1 2 2000 | Uniform
(100,0),35 (-100,0),35 2 2000 | Uniform
(15,0),6 (-15,0),6 2 2000 | Gaussian
(3.5,0),2 (-3.5,0),2 2 2000 | Gaussian
(6,0),2 (-6,0),2 2 2000 | Gaussian
(4,0),1.5 (-4,0),1.5 2 2000 | Gaussian

Table 6.4: Nonlinearly Separable Gaussian Data Sets with Outliers

6.3.1 Implementation of Proposed Hinge Loss function

We peformed the SVM algorithm with the proposed hinge loss function using
nonlinear conjugate gradient method with a version of Fletcher-Reeves. In each data
sets, denoted by D1-D8, we consider values of B ranging from —3 < B < 0 with
an increment of 0.5. Table 6.5 shows the summarized results of the SVM classifiers
performance with respect to the number of misclassified data. The best results, that
is, the ones that acquired the least number of misclassified data are highlighted in
bold.

steelers

pea 88 | 4 10,

Table 6.5: Number of Misclassified Data in D1 with different values of 6 and 7

From Table 6.5 shown, we can observe that as we decrease our 8 = 0 by 0.5, the
number of misclassified data increases. Also, we can see that 8 = 0 or 8 = 0.5 gives

the least number of misclassified data across all the datasets given.

Spearman rho test was run to test the null hypothesis that there is no significant

Chapter 6. Experiments and Results 28
relationship between 8 and number of misclassified data in each datasets. Tables 6.6-
6.13 show the Spearman rho’s correlation coefficient and the corresponding p-value
from D1-D8. For example in D4, at some significance level 0.05, we do not reject our
null hypothesis since p — value = 0.310 > 0.05. Same scenario happens with D1, D2
and D3. However, in D5, D6, D7 and D8, the p-value is 0.027 which means that the

correlation between beta and number of misclassified data is significant at the 0.05

beta
No of MD | Correlation Coefficient -0.412
—— Sig.(2-tailed) 0.310 |
Correlation Coefficient

Sig (2talled) [0.310 [ |

Table 6.6: Spearman Correlation Coefficient in D1

No of MD
Spearman te | 's rho | No of No otip Correlation Coefficient | 1.000 | -0.412 |

Sig. (2-tailed) 0.310
a Correlation Coefficient | -0.412 1.000
Sig.(2-tailed) 0.310

Table 6.7: Spearman Correlation Coefficient in D2

No of MD

Table 6.8: Spearman Correlation Coefficient in D3

No of MD | beta

| Spearman’s rho [No of MD | Correlation Coefiicient | 1.000 120.412

Sig. (2-tailed) 0.310
Correlation Coefficient | -0.412 1.000
Sig.(2-tailed) 0.310

Table 6.9: Spearman Correlation Coefficient in D4

No of MD

Sig.(2-tailed)
Correlation Coefficient
Sig.(2-tailed)

[0.037 |
—

Td

| No of MD | Correlation Coefficient 1.000 -0.764

a a i ce
Sig.(2-tailed) 0.027 |_|

No of MD

0.027
Correlation Coefficient 1.000
Sig.(2-tailed)

Table 6.13: Spearman Correlation Coefficient in D8

29

Chapter 6. Experiments and Results 30

6.4 Comparison of HL and Proposed HL

We compare the classification performance of the hinge loss function and the
proposed hinge loss function with respect to the number of misclassified data and
with respect to the accuracy value (misclassification error) given 80% of our data is
in the training set and the remaining 20% comprises the testing set.

6.4.1 Comparison in the Number of Misclassified Data

Table 6.15 shows the number of misclassified data (MD) obtained when we use
hinge logs and the proposed hinge loss in the presence of outliers. We compare the
classification performance of the two loss functions at a = 1 and 8 = 0. We know
from Table 6.14 that when there are no outliers, the performance of HL and proposed
HL is comparable to each other on the given datasets. However, in the presence of
outliers, the proposed HL performs better than HL in classifying two classes with
respect to the number of misclassified data. Lesser number of misclassified data are
highlighted in bold (see Table 6.15). To verify our claim, Wilcoxon Signed Rank
nonparametric test was conducted between the obtained number of misclassified data
using hinge loss and the proposed hinge loss. Tables 6.16 and 6.17 show the results
after conducting Wilcoxon Signed Rank test.

Number of Misclassified Data

Table 6.14: Hinge loss vs Proposed Hinge loss (without outliers)

In Table 6.16, larger differences between number of misclassified data is 36 which
falls under the negative ranks. This means that greater number of misclassified data
is attained using hinge loss function compared to the proposed hinge loss. Further-
more, Table 6.17 shows the exact significance level (p-value) equal to 0.008. Since our
p—value = 0.008 < 0.05, then at some level of significance 0.05 we have sufficient

Chapter 6. Experiments and Results 31

Datasets | Number of Misclassified Data

Table 6.15: Hinge loss vs Proposed Hinge loss (with outliers)

evidence to say that there is a significant reduction of number of misclassified data
from using hinge loss to using the proposed hinge loss.

Table 6.16: Wilcoxon Signed Ranks

Asymp. Sig. (2-tailed)

Exact Sig. (2-tailed)
Exact Sig. (1-tailed)

Table 6.17: Wilcoxon Signed Rank Test

6.4.2 Comparison in the Accuracy Value

We randomly split our data into 80% training set and 20% testing set. The
training set is responsible for the training of the SVM algorithm using the proposed
hinge logs function. It is used to fit the parameters of the separating hyperplane. The
testing set is used for testing the proposed hinge loss function on data that was not in-
cluded in the training set and see if the proposed hinge loss also works on unseen data.

We implemented the SVM algorithm with the proposed hinge loss function using
the nonlinear conjugate gradient method. It returns the SVM classifier and we will

Chapter 6. Experiments and Results 32

use this in predicting the labels in the test set. For the hinge loss SVM we used the
function fitcsum in training the data. It is a built-in function in MATLAB software
that trains support vector machine model for one-class or two-class classification.
This function returns an SVM classifier trained using the training set and the class
labels (+1,—1). In predicting the labels of the testing set, we used the sumpredict
function.

The accuracy value is computed by getting the total number of test samples that
have the same predicted label and true label and then divide it by the total number of
test samples. The higher the accuracy value, the better the classification performance.
The following table shows the accuracy values of SVM algorithm with proposed hinge
loss and the normal SVM with the hinge loss function.

D3 D4
Accuracy Values | Accuracy Values PHL EL Valves

-—
[| Pat [HL PHL [AL | PAL

| ist run | 0.9650 | 0.9475 | 0.9975 | 0.9675 Tere a | 0.9925 | 0.9325 |
| 2st run | 0.9750 | 0.9625 | 0.9875 | 0.9850 | 0.9725 | 0.9400 ne 0.4750

[0.9750 | 0.9525 | 0.9975 as asa KoseaeR Oey

| 3rdrun__|
| 4th ran] 0.9765 | 0.9400 | 0. sis 0.9800 | 0.9700 | 0.945 aes | 9825
[oth ran__| 0.9765 | | 0.9765 | [oes 0.9875 | 0.9925 | 0.9575 | 0. Haare

0.975 | 0.9740 | 0.9475 0.7955

Table 6.18: Accuracy Values from D1-D4 for five runs

|__| Accuracy Values | Looe Vain | Values ‘Aaa Values a Values
| PHL | HL |

PHL |
SI se 0.9700
0

Ce EO a 9950 _| 0.9625 | 0.9975 | 0.9525
0.9975 SSS 0.9975 | 0.9675
atk ran [0.9975 09595 [-0.9600[ 08095 | 1 [sere 1 [09735
| 5thrun | 0.9975 | 0.9500 | 0.9525 | 0.8725 | 0.9975 | 0.9625 | 0.9925 | 0.9500
AVERAGE | 0.9955 | 0.9570 | 0.9555 | 0.8500 | 0.9980 | 0.9660 | 0.9965 | 0.9585

Table 6.19: Accuracy Values from D5-D8 for five runs

In Tables 6.18 and 6.19, we run our SVM algorithm with proposed hinge loss and
the hinge loss function five times and then we get the average among the five. Based
from the results seen in the two tables, PHL has higher accuracy values than HL.

Chapter 6. Experiments and Results 33

Hence, PHL performs well in the testing set and has a better classification perfor-
mance than HL.

6.5 Comparison on Outlier Percentage

In this section, we conduct experiments to compare the SVM classifier’s per-
formance in the presence of different outliers percentage. Table 6.20 presents two
datasets (D1, D2) with four different sizes of outliers, 10, 50, 100, and 250. These
outliers have Gaussian distribution. The respective means and standard deviations
of D1 and D2 are denoted by pu and o. Table 6.21 shows the number of misclassified
data acquired given different outlier percentages in both D1 and D2 respectively.

Class +1 (1,0) Class -1 (1,0) Size of Outliers
bi [50.2 | 3.50)2
|__D2 | (1.96,0),2 (-1.96,0),1 (6,-8),1 | 10,50,100,250

Table 6.20: Nonlinearly Separable Data Sets with Outliers

| Size | Number of Misclassified Data
| | Di |

10 | 72 46

50 | 76 44
100 | 87 48
250 | 147

Table 6.21: Number of Misclassified Data in Different Outlier Percentage

From Table 6.21, we can see that as we increase the outlier percentage, the number
of misclassified data also increases. This event happens both in D1 and D2. With
this result, we can infer that number of outliers present in a data greatly affects the
SVM algorithm with respect to the number of misclassified points. The fewer the
outliers, the lesser the number of misclassified data.

Statistically, a Spearman’s rank-order correlation was run to determine the rela-
tionship between number of outliers present and number of misclassified data. The
p — value is 0.000 < 0.01. There is a perfect positive correlation between the said
two variables both in D1 and D2, which is statistically significant at level 0.05 (see

Chapter 6. Experiments and Results 34

Tables 6.22 and 6.23).

outliers | No of MD

Correlation Coefficient
an Sig (@tailed) |__| 0
|__| Noof MD | Correlation Coefficient | 1.000 | 1.000 |
a Sig (@iailed) [0 | ___

Table 6.22: Spearman Correlation Coefficient in D1

No of MD
Correlation Coefficient | 1.000 | 1.000 _|

| Sig. (2-tailed) 0
ss Correlation Coefficient | 1.000 1.000
a es |

Table 6.23: Spearman Correlation Coefficient in D1

6.6 The case when C = 1 and C = 0.0001

We studied the classification performance of SVM algorithm using the proposed
hinge loas function when C = 1 and C = 0.0001. The C’ parameter tells the support
vector machine algorithm how much we want to avoid misclassifying training datasets
as well as maximizing the margin.

In this section, we fixed a = 1 and 8 = 0. The set of data used is the same as in
Section 6.3. The table below shows the number of misclassified data obtained from
D1-D8 when we let C = 1 and C = 0.0001.

Based from the table we can see that C' = 1 has greater numbers of misclassified
data compared to C = 0.0001. If we have to choose between the two, we will likely
choose C = 0.0001. However, in general case, for large values of C’, the SVM algorithm
will choose a hyperplane that does classify all the training datasets correctly while
having a smaller margin. Conversely, as C' — 0, the algorithm will look for a larger
margin separating hyperplane even if the hyperplane misclassifies more datasets.

Chapter 6. Experiments and Results

Table 6.24: Number of Misclassified Data when C' = 1 and C = 0.0001

35

Chapter 7

Conclusion

In this paper, inspired by Huber robust regression, a new loss function is pro-
posed that is robust to outliers. We explained the robustness of the said function
with respect to the number of misclassified data. Furthermore, we parametrized
some elements of the hinge loss function. We varied the margin a as we increased the
distance between the two classes. The experiment was conducted by investigating
8 Gaussian data sets with different margins. From the experimental results, letting
0.05 < a < 1 and 1 < a < 10, we have shown that varying the hard margin 1 has no
significant effect on the SVM’s classifier given margin priors of the Gaussian data.

We also presented different values of 8 from the proposed hinge loss function that
is used in classifying the Gaussian data. We considered —3 < # < 0.5 with an in-
crement of 0.5. Results show that decreasing @ increases the number of misclassified
data. We verified our results by conducting nonparametric statistical analysis given
a level of significance 0.05.

We compared the robustness of the hinge loss function and the proposed hinge loss
function in the presence of outliers by fixing a = 1 and 8 = 0. We compared eight
Gaussian datasets with the same number of outliers present. Based on the results,
the proposed hinge loss is effective in dealing with outliers and can perform better
than the hinge loss function. We also compared the effectiveness of our proposed
hinge logs function through accuracy values. Results show that the proposed hinge
loss function acquired higher accuracy value than the hinge loss function. Moreover,
increasing the outlier percentage (number of outliers) on a given data set negatively
affects the SVM’s classifier with respect to the number of misclassified data. We also
studied different values of C. Letting C = 0.0001 and C = 1 reveals that C = 0.0001
acquired lesser number of misclassified data compared to C' = 1.

List of References

(1] H.H. Bauschke and P.L. Combettes, Conver Analysis and Monotone Operator
Theory in Hilbert Spaces (Canadian Mathematical Society, 2010).

[2] H. Cevikalp and V. Franc, Large-scale robust transductive support vector ma-
chines, Pattern Recognition 235, 199-209 (2017).

[3] K.W. Chang, C.J. Hsieh and C.J. Lin, Coordinate descent method for large scale
1,-loss linear support vector machines, J. Mach. Learn. Res. 9, 1369-1398 (2008).

[4] O. Chapelle, Training a support vector machine in the primal, Neural Comput.
19, 1155-1178 (2007).

[5] M. Collins, A. Globerson, T. Koo, X. Carreras and P.L. Bartlett, Exponentiated
gradient algorithms for conditional random and max-margin markov networks,
J. Mach. Learn. Res. 9, 1775-1822 (2008).

[6] B. Frenay and M. Verleysen, Classification in the presence of label noise: a
survey, IEEE Trans. Neural Netw. Learn. Syst. 25, 845-869 (2014).

[7] T. Hastie, J. Friedman and R. Tibshirani, The Elements of Statistical Learning:
Data Mining, Inference and Prediction (Springer, 2009) 2nd ed.

[8] C.A. Hesse, J.B. Ofosu and E.N. Nortey, Introduction to Nonparametric Statis-
tical Methods (Akrong Publications Ltd, 2017).

(9] J.J. Higgins, An Introduction to Modern Nonparametric Statistics (Brooks/Cole,
2004).

{10] C.J. Hsieh, K.W. Chang, C.J. Lin, S.S. Keerthi and S. Sundararajan, ”A dual
coordinate descent method for large-scale linear SVM”, in Proceedings of the
25th International Conference on Machine Learning 2008, (ACM New York),
pp. 408-415.

(11) P.J. Huber, Robust estimation of a location parameter, Ann of Math. Stat. 35,
73-101 (1964).

[12] P.J. Huber and E.M. Ronchetti, Robust Statistics (John Wiley & Sons, 2009).

55

List of References 56

[13] G. James, D. Witten, T. Hastie and R. Tibshirani, An Introduction to Statistical
Learning (Springer, 2013).

[14] S. Lambert-Lacroix and L. Zwald, Robust regression through the Hubers crite-
rion and adaptive lasso penalty, Electronic Journal of Statistics (2011).

(15] S. Landau and B.S. Everitt, A Handbook of Statistical Analyses using SPSS
(Chapman Hall/CRC Press, 2004).

(16] C.J. Lin, R.C. Weng and S.S. Keerthi, ”Trust region newton methods for large-
scale logistic regression”, in Proceedings of the 24th International Conference on
Machine Learning 2007, (ACM New York), pp. 561-568.

[17] Y. Ma, Y. Hec and Y. Tian, Online Robust Lagrangian Support Vector Machine
against adversarial attack, Procedia Computer Science 139, 173-181 (2018).

[18] J. Nocedal and S.J. Wright, Numerical Optimization (Springer, 1999) pp. 58-61,
120-121.

{19] A.L. Peressini, F.E. Sullivan and J.J. Uhl Jr., The Mathematics of Nonlinear
Programming (Springcr,1988)

[20] S. Shalev-Shwartz, Y. Singer and N. Srebro, ”Pegasos: Primal estimated sub-
gradient solver for SVM”, in Proceedings of the 24th International Conference
on Machine Learning 2007, (ACM New York), pp. 807-814.

[21] A. Smola and S.V.N. Vishwanathan, Introduction to Machine Learning (Cam-
bridge University Press, 2008).

[22] P. Sprent and N.C. Smeeton, Applied Nonparametrical Statistical Methods, (The
Psychometric Society, 2010) 3rd ed.

[23] G. Wahba, Support Vector Machines, reproducing kernel Hilbert spaces and the
randomized gacv, in Advances in Kernel Methods-Support Vector Learning 1999,
pp. 69-88.

[24] Y. Wu and Y. Liu, Robust truncated hinge loss support vector machines, J. Am.
Stat. Assoc. 102, 974-983 (2007).

[25] G. Xua, Z. Caob, B. Hua and J.C. Principe, Robust support vector machines
based on the rescaled hinge loss function, Pattern Recognition 63, (2017).

List of References 57

[26] A.B. Yacat, Conjugate gradient method for support vector machines with non-
quadratic regularizations, (2018).

